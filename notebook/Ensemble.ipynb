{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "from models.EnsembledNN import EnsembledNN\n",
    "from utils.plot import plot\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from utils.gen_data import gen_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_x, data_y = gen_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ems = EnsembledNN(num_models=15, num_epochs=200, batch_size=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 1)                 0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                128       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "pred (Dense)                 (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 2,241\n",
      "Trainable params: 2,241\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 1960 samples, validate on 490 samples\n",
      "Epoch 1/200\n",
      "1960/1960 [==============================] - 1s 411us/step - loss: 31.2384 - val_loss: 15.6302\n",
      "Epoch 2/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 10.8007 - val_loss: 9.9403\n",
      "Epoch 3/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 7.6482 - val_loss: 6.5361\n",
      "Epoch 4/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 6.2687 - val_loss: 5.7791\n",
      "Epoch 5/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 5.5573 - val_loss: 5.1624\n",
      "Epoch 6/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 4.6799 - val_loss: 4.3328\n",
      "Epoch 7/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 3.8690 - val_loss: 3.5780\n",
      "Epoch 8/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 3.2874 - val_loss: 3.1781\n",
      "Epoch 9/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 2.9097 - val_loss: 3.0209\n",
      "Epoch 10/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 2.6913 - val_loss: 2.5729\n",
      "Epoch 11/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 2.4386 - val_loss: 2.4660\n",
      "Epoch 12/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 2.3238 - val_loss: 2.2959\n",
      "Epoch 13/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 2.2329 - val_loss: 2.2039\n",
      "Epoch 14/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 2.1738 - val_loss: 2.1084\n",
      "Epoch 15/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 2.1594 - val_loss: 2.0672\n",
      "Epoch 16/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 2.1777 - val_loss: 2.0068\n",
      "Epoch 17/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 2.1250 - val_loss: 2.1149\n",
      "Epoch 18/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 2.0345 - val_loss: 1.8721\n",
      "Epoch 19/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.9472 - val_loss: 2.0131\n",
      "Epoch 20/200\n",
      "1960/1960 [==============================] - 0s 18us/step - loss: 1.9611 - val_loss: 1.8928\n",
      "Epoch 21/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 2.0018 - val_loss: 2.0850\n",
      "Epoch 22/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.9669 - val_loss: 1.8269\n",
      "Epoch 23/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9329 - val_loss: 1.8927\n",
      "Epoch 24/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.9276 - val_loss: 1.8742\n",
      "Epoch 25/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.9621 - val_loss: 1.9158\n",
      "Epoch 26/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 2.0245 - val_loss: 1.9152\n",
      "Epoch 27/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 2.0071 - val_loss: 1.8699\n",
      "Epoch 28/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9461 - val_loss: 1.8453\n",
      "Epoch 29/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.9197 - val_loss: 1.7963\n",
      "Epoch 30/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9174 - val_loss: 1.8356\n",
      "Epoch 31/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.9735 - val_loss: 2.0806\n",
      "Epoch 32/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 2.0531 - val_loss: 1.7767\n",
      "Epoch 33/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9590 - val_loss: 1.7691\n",
      "Epoch 34/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.9545 - val_loss: 1.7801\n",
      "Epoch 35/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.9219 - val_loss: 1.8283\n",
      "Epoch 36/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8678 - val_loss: 1.7716\n",
      "Epoch 37/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8964 - val_loss: 1.8642\n",
      "Epoch 38/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8886 - val_loss: 1.8447\n",
      "Epoch 39/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9272 - val_loss: 1.7764\n",
      "Epoch 40/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8785 - val_loss: 1.7948\n",
      "Epoch 41/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8977 - val_loss: 1.7926\n",
      "Epoch 42/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.9745 - val_loss: 1.8011\n",
      "Epoch 43/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9704 - val_loss: 1.8377\n",
      "Epoch 44/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.9494 - val_loss: 1.8408\n",
      "Epoch 45/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 2.0016 - val_loss: 1.9713\n",
      "Epoch 46/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.9605 - val_loss: 1.8743\n",
      "Epoch 47/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8998 - val_loss: 1.7858\n",
      "Epoch 48/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 2.0355 - val_loss: 1.9752\n",
      "Epoch 49/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 2.0727 - val_loss: 2.0104\n",
      "Epoch 50/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 2.1158 - val_loss: 1.9642\n",
      "Epoch 51/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.9727 - val_loss: 1.8443\n",
      "Epoch 52/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.9002 - val_loss: 1.7390\n",
      "Epoch 53/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8888 - val_loss: 1.9262\n",
      "Epoch 54/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9847 - val_loss: 1.7611\n",
      "Epoch 55/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8835 - val_loss: 1.9287\n",
      "Epoch 56/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 2.0015 - val_loss: 1.8719\n",
      "Epoch 57/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9277 - val_loss: 1.8255\n",
      "Epoch 58/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9127 - val_loss: 1.7597\n",
      "Epoch 59/200\n",
      "1960/1960 [==============================] - 0s 15us/step - loss: 1.9673 - val_loss: 1.8437\n",
      "Epoch 60/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9631 - val_loss: 1.8429\n",
      "Epoch 61/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8865 - val_loss: 1.7392\n",
      "Epoch 62/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8731 - val_loss: 1.7295\n",
      "Epoch 63/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8589 - val_loss: 1.8933\n",
      "Epoch 64/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8996 - val_loss: 1.9633\n",
      "Epoch 65/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.9611 - val_loss: 1.7594\n",
      "Epoch 66/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8782 - val_loss: 1.7439\n",
      "Epoch 67/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8745 - val_loss: 1.7503\n",
      "Epoch 68/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8615 - val_loss: 1.7517\n",
      "Epoch 69/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9204 - val_loss: 1.9971\n",
      "Epoch 70/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9520 - val_loss: 1.7395\n",
      "Epoch 71/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9299 - val_loss: 1.7280\n",
      "Epoch 72/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8790 - val_loss: 1.7953\n",
      "Epoch 73/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8633 - val_loss: 1.7598\n",
      "Epoch 74/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8573 - val_loss: 1.7936\n",
      "Epoch 75/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8498 - val_loss: 1.8167\n",
      "Epoch 76/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8932 - val_loss: 1.7663\n",
      "Epoch 77/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.9561 - val_loss: 1.7371\n",
      "Epoch 78/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9236 - val_loss: 1.7735\n",
      "Epoch 79/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9215 - val_loss: 1.9092\n",
      "Epoch 80/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9864 - val_loss: 2.0165\n",
      "Epoch 81/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9414 - val_loss: 1.7378\n",
      "Epoch 82/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8879 - val_loss: 1.7591\n",
      "Epoch 83/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8521 - val_loss: 1.7526\n",
      "Epoch 84/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8699 - val_loss: 1.7414\n",
      "Epoch 85/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8931 - val_loss: 1.8903\n",
      "Epoch 86/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9279 - val_loss: 1.8447\n",
      "Epoch 87/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.9034 - val_loss: 1.8342\n",
      "Epoch 88/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8480 - val_loss: 1.7786\n",
      "Epoch 89/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8728 - val_loss: 1.8486\n",
      "Epoch 90/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8541 - val_loss: 1.7427\n",
      "Epoch 91/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8887 - val_loss: 1.9195\n",
      "Epoch 92/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.9401 - val_loss: 1.7304\n",
      "Epoch 93/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8594 - val_loss: 1.7719\n",
      "Epoch 94/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8826 - val_loss: 1.9123\n",
      "Epoch 95/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.9946 - val_loss: 1.8121\n",
      "Epoch 96/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.9493 - val_loss: 1.8045\n",
      "Epoch 97/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9144 - val_loss: 1.7928\n",
      "Epoch 98/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9028 - val_loss: 1.8860\n",
      "Epoch 99/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.9018 - val_loss: 1.7492\n",
      "Epoch 100/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8504 - val_loss: 1.7400\n",
      "Epoch 101/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8335 - val_loss: 1.7370\n",
      "Epoch 102/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8933 - val_loss: 1.9686\n",
      "Epoch 103/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9391 - val_loss: 1.8101\n",
      "Epoch 104/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8621 - val_loss: 1.8259\n",
      "Epoch 105/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8658 - val_loss: 1.7504\n",
      "Epoch 106/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8374 - val_loss: 1.8048\n",
      "Epoch 107/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8831 - val_loss: 1.8059\n",
      "Epoch 108/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8277 - val_loss: 1.7680\n",
      "Epoch 109/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8963 - val_loss: 1.8535\n",
      "Epoch 110/200\n",
      "1960/1960 [==============================] - 0s 15us/step - loss: 1.9791 - val_loss: 1.8436\n",
      "Epoch 111/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.9011 - val_loss: 1.7898\n",
      "Epoch 112/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8477 - val_loss: 1.7720\n",
      "Epoch 113/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.9050 - val_loss: 1.7499\n",
      "Epoch 114/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8476 - val_loss: 1.7319\n",
      "Epoch 115/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8345 - val_loss: 1.7505\n",
      "Epoch 116/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8415 - val_loss: 1.7505\n",
      "Epoch 117/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.9174 - val_loss: 1.7322\n",
      "Epoch 118/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8431 - val_loss: 1.7542\n",
      "Epoch 119/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9382 - val_loss: 1.8437\n",
      "Epoch 120/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.9511 - val_loss: 1.7558\n",
      "Epoch 121/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8948 - val_loss: 1.7598\n",
      "Epoch 122/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8507 - val_loss: 1.7487\n",
      "Epoch 123/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8844 - val_loss: 1.8268\n",
      "Epoch 124/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8893 - val_loss: 1.7849\n",
      "Epoch 125/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8799 - val_loss: 1.7957\n",
      "Epoch 126/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8809 - val_loss: 1.7383\n",
      "Epoch 127/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8825 - val_loss: 1.7627\n",
      "Epoch 128/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8867 - val_loss: 1.8694\n",
      "Epoch 129/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.9577 - val_loss: 1.7654\n",
      "Epoch 130/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9117 - val_loss: 1.7643\n",
      "Epoch 131/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9457 - val_loss: 1.8307\n",
      "Epoch 132/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 2.0427 - val_loss: 2.0218\n",
      "Epoch 133/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 2.0054 - val_loss: 1.8298\n",
      "Epoch 134/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 2.0398 - val_loss: 1.9345\n",
      "Epoch 135/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9546 - val_loss: 1.8312\n",
      "Epoch 136/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.9057 - val_loss: 1.8593\n",
      "Epoch 137/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 2.0138 - val_loss: 1.7785\n",
      "Epoch 138/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9236 - val_loss: 1.8419\n",
      "Epoch 139/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8777 - val_loss: 1.7228\n",
      "Epoch 140/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8720 - val_loss: 1.7258\n",
      "Epoch 141/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8590 - val_loss: 1.8024\n",
      "Epoch 142/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8719 - val_loss: 1.7474\n",
      "Epoch 143/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8511 - val_loss: 1.7774\n",
      "Epoch 144/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8861 - val_loss: 1.7862\n",
      "Epoch 145/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8603 - val_loss: 1.7836\n",
      "Epoch 146/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8561 - val_loss: 1.7387\n",
      "Epoch 147/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8551 - val_loss: 1.7564\n",
      "Epoch 148/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8447 - val_loss: 1.7719\n",
      "Epoch 149/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8969 - val_loss: 1.8736\n",
      "Epoch 150/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8624 - val_loss: 1.8327\n",
      "Epoch 151/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8669 - val_loss: 1.8723\n",
      "Epoch 152/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8774 - val_loss: 1.8325\n",
      "Epoch 153/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8610 - val_loss: 1.8458\n",
      "Epoch 154/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.9155 - val_loss: 1.8154\n",
      "Epoch 155/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8730 - val_loss: 1.7301\n",
      "Epoch 156/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8821 - val_loss: 1.7690\n",
      "Epoch 157/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8624 - val_loss: 1.7334\n",
      "Epoch 158/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8783 - val_loss: 1.7657\n",
      "Epoch 159/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9965 - val_loss: 1.8875\n",
      "Epoch 160/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8786 - val_loss: 1.7814\n",
      "Epoch 161/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8818 - val_loss: 1.7911\n",
      "Epoch 162/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8693 - val_loss: 1.9469\n",
      "Epoch 163/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9403 - val_loss: 1.7937\n",
      "Epoch 164/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8792 - val_loss: 1.7805\n",
      "Epoch 165/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8824 - val_loss: 1.7337\n",
      "Epoch 166/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9050 - val_loss: 1.7644\n",
      "Epoch 167/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9006 - val_loss: 1.7518\n",
      "Epoch 168/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.9332 - val_loss: 1.8520\n",
      "Epoch 169/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.9154 - val_loss: 1.9159\n",
      "Epoch 170/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9442 - val_loss: 1.7497\n",
      "Epoch 171/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.9284 - val_loss: 1.7638\n",
      "Epoch 172/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8618 - val_loss: 1.8424\n",
      "Epoch 173/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8487 - val_loss: 1.7852\n",
      "Epoch 174/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8566 - val_loss: 1.7237\n",
      "Epoch 175/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8490 - val_loss: 1.7188\n",
      "Epoch 176/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8642 - val_loss: 1.7758\n",
      "Epoch 177/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8904 - val_loss: 1.7206\n",
      "Epoch 178/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.9501 - val_loss: 1.8909\n",
      "Epoch 179/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8680 - val_loss: 1.7446\n",
      "Epoch 180/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8852 - val_loss: 1.8184\n",
      "Epoch 181/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.9286 - val_loss: 1.8952\n",
      "Epoch 182/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8866 - val_loss: 1.7903\n",
      "Epoch 183/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8922 - val_loss: 1.7957\n",
      "Epoch 184/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.9265 - val_loss: 1.8542\n",
      "Epoch 185/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8976 - val_loss: 1.8282\n",
      "Epoch 186/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8462 - val_loss: 1.7240\n",
      "Epoch 187/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8615 - val_loss: 1.8236\n",
      "Epoch 188/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8565 - val_loss: 1.7776\n",
      "Epoch 189/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8706 - val_loss: 1.7542\n",
      "Epoch 190/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8695 - val_loss: 1.7530\n",
      "Epoch 191/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8586 - val_loss: 1.7433\n",
      "Epoch 192/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8566 - val_loss: 1.7469\n",
      "Epoch 193/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9371 - val_loss: 1.9239\n",
      "Epoch 194/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8785 - val_loss: 1.7487\n",
      "Epoch 195/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8875 - val_loss: 1.8307\n",
      "Epoch 196/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8568 - val_loss: 1.9321\n",
      "Epoch 197/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8573 - val_loss: 1.8350\n",
      "Epoch 198/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8876 - val_loss: 1.8253\n",
      "Epoch 199/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.9056 - val_loss: 1.8210\n",
      "Epoch 200/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.9798 - val_loss: 1.9137\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 1)                 0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 64)                128       \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "pred (Dense)                 (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 2,241\n",
      "Trainable params: 2,241\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 1960 samples, validate on 490 samples\n",
      "Epoch 1/200\n",
      "1960/1960 [==============================] - 0s 133us/step - loss: 31.0399 - val_loss: 15.1901\n",
      "Epoch 2/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 11.2930 - val_loss: 8.3031\n",
      "Epoch 3/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 6.9848 - val_loss: 5.4854\n",
      "Epoch 4/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 5.2588 - val_loss: 4.6374\n",
      "Epoch 5/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 3.8965 - val_loss: 3.2983\n",
      "Epoch 6/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 3.1810 - val_loss: 2.5438\n",
      "Epoch 7/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 2.6100 - val_loss: 2.4460\n",
      "Epoch 8/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 2.3349 - val_loss: 2.1285\n",
      "Epoch 9/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 2.0840 - val_loss: 2.0824\n",
      "Epoch 10/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.9477 - val_loss: 2.0443\n",
      "Epoch 11/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8943 - val_loss: 2.0508\n",
      "Epoch 12/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8961 - val_loss: 1.9707\n",
      "Epoch 13/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9344 - val_loss: 2.6559\n",
      "Epoch 14/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 2.0010 - val_loss: 2.0287\n",
      "Epoch 15/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.9507 - val_loss: 2.0133\n",
      "Epoch 16/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.9027 - val_loss: 1.9393\n",
      "Epoch 17/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8518 - val_loss: 2.0402\n",
      "Epoch 18/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8574 - val_loss: 1.9386\n",
      "Epoch 19/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8880 - val_loss: 2.0681\n",
      "Epoch 20/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.9823 - val_loss: 2.1141\n",
      "Epoch 21/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.9212 - val_loss: 1.9785\n",
      "Epoch 22/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9238 - val_loss: 2.0326\n",
      "Epoch 23/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.9247 - val_loss: 2.0198\n",
      "Epoch 24/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8674 - val_loss: 1.9508\n",
      "Epoch 25/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8330 - val_loss: 2.0418\n",
      "Epoch 26/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8455 - val_loss: 1.9247\n",
      "Epoch 27/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8213 - val_loss: 2.0354\n",
      "Epoch 28/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.9032 - val_loss: 1.9872\n",
      "Epoch 29/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8952 - val_loss: 1.9562\n",
      "Epoch 30/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9141 - val_loss: 2.2191\n",
      "Epoch 31/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.9392 - val_loss: 2.0413\n",
      "Epoch 32/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8879 - val_loss: 2.0684\n",
      "Epoch 33/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8463 - val_loss: 1.9268\n",
      "Epoch 34/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.9435 - val_loss: 1.9173\n",
      "Epoch 35/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8640 - val_loss: 2.0903\n",
      "Epoch 36/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8987 - val_loss: 2.0924\n",
      "Epoch 37/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9999 - val_loss: 2.1453\n",
      "Epoch 38/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8721 - val_loss: 2.1037\n",
      "Epoch 39/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.9044 - val_loss: 2.1021\n",
      "Epoch 40/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9575 - val_loss: 1.9422\n",
      "Epoch 41/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9915 - val_loss: 2.2953\n",
      "Epoch 42/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9659 - val_loss: 1.9369\n",
      "Epoch 43/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8839 - val_loss: 1.9025\n",
      "Epoch 44/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8479 - val_loss: 1.9878\n",
      "Epoch 45/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9351 - val_loss: 1.9576\n",
      "Epoch 46/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.9376 - val_loss: 2.1321\n",
      "Epoch 47/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9550 - val_loss: 1.9366\n",
      "Epoch 48/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8873 - val_loss: 2.0952\n",
      "Epoch 49/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8817 - val_loss: 2.0139\n",
      "Epoch 50/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8647 - val_loss: 1.9532\n",
      "Epoch 51/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8344 - val_loss: 1.9334\n",
      "Epoch 52/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8590 - val_loss: 1.8748\n",
      "Epoch 53/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.7904 - val_loss: 1.8659\n",
      "Epoch 54/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8259 - val_loss: 1.9154\n",
      "Epoch 55/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8343 - val_loss: 2.0244\n",
      "Epoch 56/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8611 - val_loss: 2.1324\n",
      "Epoch 57/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8876 - val_loss: 1.9199\n",
      "Epoch 58/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.9039 - val_loss: 1.9082\n",
      "Epoch 59/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8909 - val_loss: 1.9231\n",
      "Epoch 60/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8331 - val_loss: 1.8839\n",
      "Epoch 61/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8357 - val_loss: 2.0241\n",
      "Epoch 62/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9031 - val_loss: 1.9006\n",
      "Epoch 63/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8426 - val_loss: 1.9552\n",
      "Epoch 64/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8474 - val_loss: 1.8645\n",
      "Epoch 65/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8546 - val_loss: 2.0233\n",
      "Epoch 66/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9410 - val_loss: 2.1111\n",
      "Epoch 67/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.9370 - val_loss: 2.4090\n",
      "Epoch 68/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 2.0403 - val_loss: 1.9803\n",
      "Epoch 69/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8654 - val_loss: 1.9860\n",
      "Epoch 70/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8782 - val_loss: 2.0045\n",
      "Epoch 71/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8251 - val_loss: 1.8729\n",
      "Epoch 72/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8416 - val_loss: 1.9241\n",
      "Epoch 73/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8285 - val_loss: 1.9844\n",
      "Epoch 74/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8404 - val_loss: 1.9040\n",
      "Epoch 75/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8984 - val_loss: 2.0156\n",
      "Epoch 76/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8445 - val_loss: 1.9822\n",
      "Epoch 77/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8328 - val_loss: 1.9260\n",
      "Epoch 78/200\n",
      "1960/1960 [==============================] - 0s 18us/step - loss: 1.8196 - val_loss: 1.8948\n",
      "Epoch 79/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8159 - val_loss: 1.9156\n",
      "Epoch 80/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8663 - val_loss: 1.9257\n",
      "Epoch 81/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8713 - val_loss: 1.9069\n",
      "Epoch 82/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8527 - val_loss: 1.8756\n",
      "Epoch 83/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8254 - val_loss: 1.9376\n",
      "Epoch 84/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8460 - val_loss: 1.9363\n",
      "Epoch 85/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.9126 - val_loss: 2.1060\n",
      "Epoch 86/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.9514 - val_loss: 2.0137\n",
      "Epoch 87/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9690 - val_loss: 2.0546\n",
      "Epoch 88/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9260 - val_loss: 1.9679\n",
      "Epoch 89/200\n",
      "1960/1960 [==============================] - 0s 18us/step - loss: 1.8666 - val_loss: 1.8964\n",
      "Epoch 90/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8288 - val_loss: 1.9534\n",
      "Epoch 91/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8059 - val_loss: 1.8684\n",
      "Epoch 92/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8014 - val_loss: 1.9011\n",
      "Epoch 93/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8575 - val_loss: 2.0266\n",
      "Epoch 94/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8602 - val_loss: 1.8718\n",
      "Epoch 95/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8076 - val_loss: 1.9270\n",
      "Epoch 96/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8894 - val_loss: 2.0238\n",
      "Epoch 97/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8379 - val_loss: 1.9379\n",
      "Epoch 98/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8402 - val_loss: 1.9020\n",
      "Epoch 99/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.9993 - val_loss: 2.1165\n",
      "Epoch 100/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9024 - val_loss: 1.9297\n",
      "Epoch 101/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8352 - val_loss: 1.9055\n",
      "Epoch 102/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8325 - val_loss: 1.8769\n",
      "Epoch 103/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8315 - val_loss: 2.1423\n",
      "Epoch 104/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.9109 - val_loss: 2.0417\n",
      "Epoch 105/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8783 - val_loss: 1.9085\n",
      "Epoch 106/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8190 - val_loss: 1.8994\n",
      "Epoch 107/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8092 - val_loss: 1.9442\n",
      "Epoch 108/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8022 - val_loss: 1.9461\n",
      "Epoch 109/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8833 - val_loss: 2.2893\n",
      "Epoch 110/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9566 - val_loss: 2.0975\n",
      "Epoch 111/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.9508 - val_loss: 1.9432\n",
      "Epoch 112/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8611 - val_loss: 1.9447\n",
      "Epoch 113/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8554 - val_loss: 1.9551\n",
      "Epoch 114/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8490 - val_loss: 2.0057\n",
      "Epoch 115/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8306 - val_loss: 1.8987\n",
      "Epoch 116/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8889 - val_loss: 1.9984\n",
      "Epoch 117/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8440 - val_loss: 1.9140\n",
      "Epoch 118/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8490 - val_loss: 2.1527\n",
      "Epoch 119/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.9079 - val_loss: 1.9740\n",
      "Epoch 120/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 2.0112 - val_loss: 2.2562\n",
      "Epoch 121/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9700 - val_loss: 1.9698\n",
      "Epoch 122/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.9295 - val_loss: 1.8733\n",
      "Epoch 123/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8846 - val_loss: 2.1192\n",
      "Epoch 124/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8957 - val_loss: 1.9359\n",
      "Epoch 125/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8599 - val_loss: 1.9821\n",
      "Epoch 126/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8101 - val_loss: 2.0055\n",
      "Epoch 127/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8213 - val_loss: 1.9249\n",
      "Epoch 128/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8602 - val_loss: 1.9221\n",
      "Epoch 129/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8415 - val_loss: 1.9325\n",
      "Epoch 130/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8722 - val_loss: 1.9241\n",
      "Epoch 131/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9148 - val_loss: 1.8997\n",
      "Epoch 132/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8331 - val_loss: 1.9307\n",
      "Epoch 133/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8011 - val_loss: 1.8545\n",
      "Epoch 134/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8037 - val_loss: 1.8822\n",
      "Epoch 135/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8610 - val_loss: 1.8909\n",
      "Epoch 136/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8124 - val_loss: 1.8684\n",
      "Epoch 137/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8056 - val_loss: 1.8642\n",
      "Epoch 138/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8472 - val_loss: 2.0166\n",
      "Epoch 139/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8860 - val_loss: 1.9935\n",
      "Epoch 140/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8423 - val_loss: 1.9684\n",
      "Epoch 141/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8682 - val_loss: 1.9793\n",
      "Epoch 142/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8657 - val_loss: 1.9428\n",
      "Epoch 143/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8555 - val_loss: 1.9329\n",
      "Epoch 144/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8356 - val_loss: 1.9098\n",
      "Epoch 145/200\n",
      "1960/1960 [==============================] - 0s 18us/step - loss: 1.8248 - val_loss: 1.9691\n",
      "Epoch 146/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.9218 - val_loss: 1.9823\n",
      "Epoch 147/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8703 - val_loss: 1.8995\n",
      "Epoch 148/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8699 - val_loss: 1.9662\n",
      "Epoch 149/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8339 - val_loss: 1.8893\n",
      "Epoch 150/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8360 - val_loss: 1.8795\n",
      "Epoch 151/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.7917 - val_loss: 1.8778\n",
      "Epoch 152/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8478 - val_loss: 1.9807\n",
      "Epoch 153/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9205 - val_loss: 1.9318\n",
      "Epoch 154/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8835 - val_loss: 2.0891\n",
      "Epoch 155/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.9790 - val_loss: 2.0261\n",
      "Epoch 156/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.9010 - val_loss: 1.9982\n",
      "Epoch 157/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8794 - val_loss: 1.9301\n",
      "Epoch 158/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8496 - val_loss: 1.8776\n",
      "Epoch 159/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8249 - val_loss: 1.9768\n",
      "Epoch 160/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8409 - val_loss: 2.0320\n",
      "Epoch 161/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8309 - val_loss: 1.9056\n",
      "Epoch 162/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9109 - val_loss: 1.9328\n",
      "Epoch 163/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8348 - val_loss: 1.9799\n",
      "Epoch 164/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8279 - val_loss: 1.9001\n",
      "Epoch 165/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8260 - val_loss: 1.8825\n",
      "Epoch 166/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.7894 - val_loss: 1.8641\n",
      "Epoch 167/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8644 - val_loss: 2.0084\n",
      "Epoch 168/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8088 - val_loss: 2.0334\n",
      "Epoch 169/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8624 - val_loss: 1.8762\n",
      "Epoch 170/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8425 - val_loss: 1.8970\n",
      "Epoch 171/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8139 - val_loss: 1.8873\n",
      "Epoch 172/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8341 - val_loss: 1.9599\n",
      "Epoch 173/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8099 - val_loss: 2.0349\n",
      "Epoch 174/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9051 - val_loss: 1.9974\n",
      "Epoch 175/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8940 - val_loss: 1.9189\n",
      "Epoch 176/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8225 - val_loss: 1.9868\n",
      "Epoch 177/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8414 - val_loss: 1.8867\n",
      "Epoch 178/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8246 - val_loss: 2.1101\n",
      "Epoch 179/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8256 - val_loss: 2.0203\n",
      "Epoch 180/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9325 - val_loss: 1.9011\n",
      "Epoch 181/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8551 - val_loss: 1.9072\n",
      "Epoch 182/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8318 - val_loss: 1.8567\n",
      "Epoch 183/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8547 - val_loss: 1.9723\n",
      "Epoch 184/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8939 - val_loss: 2.0770\n",
      "Epoch 185/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8456 - val_loss: 1.9633\n",
      "Epoch 186/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8229 - val_loss: 1.9397\n",
      "Epoch 187/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8270 - val_loss: 1.8670\n",
      "Epoch 188/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8258 - val_loss: 2.0068\n",
      "Epoch 189/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8579 - val_loss: 1.9455\n",
      "Epoch 190/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8040 - val_loss: 1.9421\n",
      "Epoch 191/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8232 - val_loss: 1.9195\n",
      "Epoch 192/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8269 - val_loss: 1.9469\n",
      "Epoch 193/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8172 - val_loss: 1.8637\n",
      "Epoch 194/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8070 - val_loss: 1.9225\n",
      "Epoch 195/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8323 - val_loss: 1.9679\n",
      "Epoch 196/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.9039 - val_loss: 1.9794\n",
      "Epoch 197/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8585 - val_loss: 1.9438\n",
      "Epoch 198/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8200 - val_loss: 1.9044\n",
      "Epoch 199/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8060 - val_loss: 2.0140\n",
      "Epoch 200/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.9605 - val_loss: 1.9335\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         (None, 1)                 0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 64)                128       \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "pred (Dense)                 (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 2,241\n",
      "Trainable params: 2,241\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 1960 samples, validate on 490 samples\n",
      "Epoch 1/200\n",
      "1960/1960 [==============================] - 0s 145us/step - loss: 33.4231 - val_loss: 12.0886\n",
      "Epoch 2/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 10.6256 - val_loss: 8.9394\n",
      "Epoch 3/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 7.4606 - val_loss: 7.5340\n",
      "Epoch 4/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 6.4074 - val_loss: 5.7915\n",
      "Epoch 5/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 5.8541 - val_loss: 5.3980\n",
      "Epoch 6/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 5.0484 - val_loss: 4.3497\n",
      "Epoch 7/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 4.1492 - val_loss: 3.3593\n",
      "Epoch 8/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 3.3372 - val_loss: 2.5059\n",
      "Epoch 9/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 2.6716 - val_loss: 2.1376\n",
      "Epoch 10/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 2.2944 - val_loss: 1.9765\n",
      "Epoch 11/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 2.0452 - val_loss: 1.9899\n",
      "Epoch 12/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9404 - val_loss: 1.9961\n",
      "Epoch 13/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.9857 - val_loss: 2.2261\n",
      "Epoch 14/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.9594 - val_loss: 2.0095\n",
      "Epoch 15/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.9280 - val_loss: 2.1376\n",
      "Epoch 16/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8800 - val_loss: 2.0712\n",
      "Epoch 17/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8726 - val_loss: 1.9950\n",
      "Epoch 18/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8621 - val_loss: 1.9738\n",
      "Epoch 19/200\n",
      "1960/1960 [==============================] - 0s 18us/step - loss: 1.8439 - val_loss: 1.8956\n",
      "Epoch 20/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8927 - val_loss: 1.9892\n",
      "Epoch 21/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8374 - val_loss: 1.9711\n",
      "Epoch 22/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8685 - val_loss: 2.0505\n",
      "Epoch 23/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8702 - val_loss: 2.0267\n",
      "Epoch 24/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8281 - val_loss: 2.1176\n",
      "Epoch 25/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.9019 - val_loss: 2.2746\n",
      "Epoch 26/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8716 - val_loss: 2.2392\n",
      "Epoch 27/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.9847 - val_loss: 1.9468\n",
      "Epoch 28/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8464 - val_loss: 2.0129\n",
      "Epoch 29/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8206 - val_loss: 2.0723\n",
      "Epoch 30/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8433 - val_loss: 1.9944\n",
      "Epoch 31/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8081 - val_loss: 2.1098\n",
      "Epoch 32/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.7725 - val_loss: 1.9252\n",
      "Epoch 33/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8627 - val_loss: 2.0253\n",
      "Epoch 34/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8855 - val_loss: 2.0757\n",
      "Epoch 35/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8588 - val_loss: 1.9466\n",
      "Epoch 36/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8686 - val_loss: 2.0819\n",
      "Epoch 37/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8681 - val_loss: 2.0631\n",
      "Epoch 38/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8408 - val_loss: 1.9983\n",
      "Epoch 39/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8690 - val_loss: 2.0128\n",
      "Epoch 40/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9272 - val_loss: 1.9393\n",
      "Epoch 41/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 2.0694 - val_loss: 1.9669\n",
      "Epoch 42/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9643 - val_loss: 2.0134\n",
      "Epoch 43/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8068 - val_loss: 2.0589\n",
      "Epoch 44/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8124 - val_loss: 1.9828\n",
      "Epoch 45/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8030 - val_loss: 2.1049\n",
      "Epoch 46/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8275 - val_loss: 2.1505\n",
      "Epoch 47/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8843 - val_loss: 2.0004\n",
      "Epoch 48/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8315 - val_loss: 1.9928\n",
      "Epoch 49/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8715 - val_loss: 1.9678\n",
      "Epoch 50/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8531 - val_loss: 2.0145\n",
      "Epoch 51/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8095 - val_loss: 2.0808\n",
      "Epoch 52/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8454 - val_loss: 1.9713\n",
      "Epoch 53/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8264 - val_loss: 2.0501\n",
      "Epoch 54/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8604 - val_loss: 2.0135\n",
      "Epoch 55/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8693 - val_loss: 2.2099\n",
      "Epoch 56/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9090 - val_loss: 2.0448\n",
      "Epoch 57/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8149 - val_loss: 2.1386\n",
      "Epoch 58/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8414 - val_loss: 2.0016\n",
      "Epoch 59/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8159 - val_loss: 2.0201\n",
      "Epoch 60/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8223 - val_loss: 2.0630\n",
      "Epoch 61/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8305 - val_loss: 2.0509\n",
      "Epoch 62/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.9272 - val_loss: 2.0161\n",
      "Epoch 63/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9774 - val_loss: 2.4173\n",
      "Epoch 64/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 2.0228 - val_loss: 2.0070\n",
      "Epoch 65/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9309 - val_loss: 1.9685\n",
      "Epoch 66/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8616 - val_loss: 1.9918\n",
      "Epoch 67/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8529 - val_loss: 2.0608\n",
      "Epoch 68/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8630 - val_loss: 2.0049\n",
      "Epoch 69/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8042 - val_loss: 2.0038\n",
      "Epoch 70/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8195 - val_loss: 1.9738\n",
      "Epoch 71/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 2.0476 - val_loss: 1.9517\n",
      "Epoch 72/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.9411 - val_loss: 2.0794\n",
      "Epoch 73/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8662 - val_loss: 2.0732\n",
      "Epoch 74/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8329 - val_loss: 2.0253\n",
      "Epoch 75/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8261 - val_loss: 2.0073\n",
      "Epoch 76/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8870 - val_loss: 2.0575\n",
      "Epoch 77/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8308 - val_loss: 1.9492\n",
      "Epoch 78/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8191 - val_loss: 1.9688\n",
      "Epoch 79/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8078 - val_loss: 2.0390\n",
      "Epoch 80/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9328 - val_loss: 2.1644\n",
      "Epoch 81/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.9565 - val_loss: 1.9252\n",
      "Epoch 82/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8787 - val_loss: 2.0547\n",
      "Epoch 83/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8400 - val_loss: 1.9960\n",
      "Epoch 84/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8954 - val_loss: 2.1599\n",
      "Epoch 85/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8589 - val_loss: 1.9820\n",
      "Epoch 86/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.9046 - val_loss: 2.0891\n",
      "Epoch 87/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8337 - val_loss: 2.0353\n",
      "Epoch 88/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8013 - val_loss: 1.9810\n",
      "Epoch 89/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8285 - val_loss: 2.0875\n",
      "Epoch 90/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8057 - val_loss: 2.1699\n",
      "Epoch 91/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8562 - val_loss: 2.0434\n",
      "Epoch 92/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8271 - val_loss: 2.0668\n",
      "Epoch 93/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8426 - val_loss: 2.1109\n",
      "Epoch 94/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8675 - val_loss: 1.9737\n",
      "Epoch 95/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8811 - val_loss: 2.0722\n",
      "Epoch 96/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8586 - val_loss: 2.0170\n",
      "Epoch 97/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8891 - val_loss: 2.0359\n",
      "Epoch 98/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8264 - val_loss: 1.9968\n",
      "Epoch 99/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8301 - val_loss: 2.1301\n",
      "Epoch 100/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8270 - val_loss: 1.9338\n",
      "Epoch 101/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8103 - val_loss: 2.0961\n",
      "Epoch 102/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8447 - val_loss: 2.0393\n",
      "Epoch 103/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8043 - val_loss: 1.9837\n",
      "Epoch 104/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.7583 - val_loss: 2.0545\n",
      "Epoch 105/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8665 - val_loss: 2.0283\n",
      "Epoch 106/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8934 - val_loss: 2.0344\n",
      "Epoch 107/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8559 - val_loss: 2.0184\n",
      "Epoch 108/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8492 - val_loss: 1.9435\n",
      "Epoch 109/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.7788 - val_loss: 1.9986\n",
      "Epoch 110/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8160 - val_loss: 1.9708\n",
      "Epoch 111/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8639 - val_loss: 2.0492\n",
      "Epoch 112/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8336 - val_loss: 1.9762\n",
      "Epoch 113/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8310 - val_loss: 2.0042\n",
      "Epoch 114/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8072 - val_loss: 1.9948\n",
      "Epoch 115/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.7980 - val_loss: 2.0537\n",
      "Epoch 116/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8475 - val_loss: 1.9435\n",
      "Epoch 117/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8791 - val_loss: 2.2155\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 118/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8518 - val_loss: 2.0142\n",
      "Epoch 119/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8190 - val_loss: 2.0144\n",
      "Epoch 120/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8157 - val_loss: 1.9670\n",
      "Epoch 121/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.7950 - val_loss: 1.9698\n",
      "Epoch 122/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.7934 - val_loss: 2.0087\n",
      "Epoch 123/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8653 - val_loss: 2.0094\n",
      "Epoch 124/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8343 - val_loss: 1.9990\n",
      "Epoch 125/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8011 - val_loss: 2.0916\n",
      "Epoch 126/200\n",
      "1960/1960 [==============================] - 0s 18us/step - loss: 1.8320 - val_loss: 2.0050\n",
      "Epoch 127/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8184 - val_loss: 2.1198\n",
      "Epoch 128/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8085 - val_loss: 1.9395\n",
      "Epoch 129/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8747 - val_loss: 2.1807\n",
      "Epoch 130/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.9280 - val_loss: 2.1013\n",
      "Epoch 131/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8618 - val_loss: 2.0142\n",
      "Epoch 132/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8345 - val_loss: 1.9794\n",
      "Epoch 133/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8458 - val_loss: 2.0607\n",
      "Epoch 134/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8981 - val_loss: 2.0883\n",
      "Epoch 135/200\n",
      "1960/1960 [==============================] - 0s 18us/step - loss: 1.8829 - val_loss: 2.0383\n",
      "Epoch 136/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8582 - val_loss: 2.1269\n",
      "Epoch 137/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8318 - val_loss: 2.0123\n",
      "Epoch 138/200\n",
      "1960/1960 [==============================] - 0s 18us/step - loss: 1.7874 - val_loss: 2.1347\n",
      "Epoch 139/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8539 - val_loss: 2.0053\n",
      "Epoch 140/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8226 - val_loss: 2.0586\n",
      "Epoch 141/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8003 - val_loss: 2.0885\n",
      "Epoch 142/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8111 - val_loss: 2.0542\n",
      "Epoch 143/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8178 - val_loss: 2.0804\n",
      "Epoch 144/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8351 - val_loss: 2.0388\n",
      "Epoch 145/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8451 - val_loss: 2.0005\n",
      "Epoch 146/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8810 - val_loss: 2.0381\n",
      "Epoch 147/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8634 - val_loss: 1.9928\n",
      "Epoch 148/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8389 - val_loss: 2.0888\n",
      "Epoch 149/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8706 - val_loss: 2.0263\n",
      "Epoch 150/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.7810 - val_loss: 1.9672\n",
      "Epoch 151/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8254 - val_loss: 2.0428\n",
      "Epoch 152/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8271 - val_loss: 1.9990\n",
      "Epoch 153/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8275 - val_loss: 1.9854\n",
      "Epoch 154/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.7880 - val_loss: 2.0268\n",
      "Epoch 155/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8561 - val_loss: 2.0336\n",
      "Epoch 156/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8830 - val_loss: 2.2811\n",
      "Epoch 157/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9910 - val_loss: 2.0364\n",
      "Epoch 158/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8451 - val_loss: 1.9657\n",
      "Epoch 159/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8347 - val_loss: 2.1420\n",
      "Epoch 160/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.9174 - val_loss: 2.1938\n",
      "Epoch 161/200\n",
      "1960/1960 [==============================] - 0s 18us/step - loss: 1.9110 - val_loss: 1.9982\n",
      "Epoch 162/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8148 - val_loss: 2.0258\n",
      "Epoch 163/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8182 - val_loss: 1.9717\n",
      "Epoch 164/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.7905 - val_loss: 1.9672\n",
      "Epoch 165/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.7969 - val_loss: 2.0603\n",
      "Epoch 166/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.9268 - val_loss: 1.9857\n",
      "Epoch 167/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.9509 - val_loss: 2.3199\n",
      "Epoch 168/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8879 - val_loss: 2.0014\n",
      "Epoch 169/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8748 - val_loss: 2.1193\n",
      "Epoch 170/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8607 - val_loss: 1.9629\n",
      "Epoch 171/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8598 - val_loss: 1.9558\n",
      "Epoch 172/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8297 - val_loss: 2.0535\n",
      "Epoch 173/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8053 - val_loss: 2.1530\n",
      "Epoch 174/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8657 - val_loss: 2.0727\n",
      "Epoch 175/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8331 - val_loss: 2.0377\n",
      "Epoch 176/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8158 - val_loss: 2.0893\n",
      "Epoch 177/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8026 - val_loss: 1.9915\n",
      "Epoch 178/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8034 - val_loss: 2.0616\n",
      "Epoch 179/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8712 - val_loss: 1.9658\n",
      "Epoch 180/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8362 - val_loss: 2.1225\n",
      "Epoch 181/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8455 - val_loss: 1.9936\n",
      "Epoch 182/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8634 - val_loss: 2.0507\n",
      "Epoch 183/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8831 - val_loss: 1.9998\n",
      "Epoch 184/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8393 - val_loss: 2.0402\n",
      "Epoch 185/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9432 - val_loss: 2.0132\n",
      "Epoch 186/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8359 - val_loss: 2.0669\n",
      "Epoch 187/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9040 - val_loss: 1.9987\n",
      "Epoch 188/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8776 - val_loss: 2.0193\n",
      "Epoch 189/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8887 - val_loss: 2.0204\n",
      "Epoch 190/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9576 - val_loss: 2.0343\n",
      "Epoch 191/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8357 - val_loss: 1.9611\n",
      "Epoch 192/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.9854 - val_loss: 2.2253\n",
      "Epoch 193/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 2.0034 - val_loss: 2.2514\n",
      "Epoch 194/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9237 - val_loss: 2.0846\n",
      "Epoch 195/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8720 - val_loss: 2.1127\n",
      "Epoch 196/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8661 - val_loss: 2.0099\n",
      "Epoch 197/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8598 - val_loss: 1.9886\n",
      "Epoch 198/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8629 - val_loss: 1.9945\n",
      "Epoch 199/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8101 - val_loss: 1.9800\n",
      "Epoch 200/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8204 - val_loss: 1.9635\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_4 (InputLayer)         (None, 1)                 0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 64)                128       \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "pred (Dense)                 (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 2,241\n",
      "Trainable params: 2,241\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 1960 samples, validate on 490 samples\n",
      "Epoch 1/200\n",
      "1960/1960 [==============================] - 0s 157us/step - loss: 26.2482 - val_loss: 13.3946\n",
      "Epoch 2/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 8.4097 - val_loss: 8.2374\n",
      "Epoch 3/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 6.5020 - val_loss: 7.3199\n",
      "Epoch 4/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 5.3510 - val_loss: 5.7059\n",
      "Epoch 5/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 4.3376 - val_loss: 4.5067\n",
      "Epoch 6/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 3.3723 - val_loss: 3.3019\n",
      "Epoch 7/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 2.7582 - val_loss: 2.9913\n",
      "Epoch 8/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 2.4802 - val_loss: 2.4121\n",
      "Epoch 9/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 2.1804 - val_loss: 2.1197\n",
      "Epoch 10/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 2.0434 - val_loss: 1.9536\n",
      "Epoch 11/200\n",
      "1960/1960 [==============================] - 0s 20us/step - loss: 1.9862 - val_loss: 1.8031\n",
      "Epoch 12/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9419 - val_loss: 1.9062\n",
      "Epoch 13/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.9471 - val_loss: 1.8547\n",
      "Epoch 14/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 2.0055 - val_loss: 1.9928\n",
      "Epoch 15/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 2.1712 - val_loss: 2.3573\n",
      "Epoch 16/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.9912 - val_loss: 1.8076\n",
      "Epoch 17/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.9887 - val_loss: 1.9465\n",
      "Epoch 18/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 2.1274 - val_loss: 2.2770\n",
      "Epoch 19/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9431 - val_loss: 1.9325\n",
      "Epoch 20/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.9207 - val_loss: 1.7628\n",
      "Epoch 21/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9223 - val_loss: 1.7432\n",
      "Epoch 22/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.9916 - val_loss: 1.9554\n",
      "Epoch 23/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 2.1534 - val_loss: 2.1436\n",
      "Epoch 24/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 2.0251 - val_loss: 1.8502\n",
      "Epoch 25/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 2.0026 - val_loss: 1.7551\n",
      "Epoch 26/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.9064 - val_loss: 1.9558\n",
      "Epoch 27/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8714 - val_loss: 1.8375\n",
      "Epoch 28/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9737 - val_loss: 1.9810\n",
      "Epoch 29/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.9826 - val_loss: 1.8257\n",
      "Epoch 30/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.9179 - val_loss: 1.9532\n",
      "Epoch 31/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9134 - val_loss: 1.8750\n",
      "Epoch 32/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.9356 - val_loss: 1.8638\n",
      "Epoch 33/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.9402 - val_loss: 1.8499\n",
      "Epoch 34/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 2.0102 - val_loss: 2.1040\n",
      "Epoch 35/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.9284 - val_loss: 1.8649\n",
      "Epoch 36/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8903 - val_loss: 1.7380\n",
      "Epoch 37/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9574 - val_loss: 1.7979\n",
      "Epoch 38/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.9290 - val_loss: 1.7779\n",
      "Epoch 39/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 2.1109 - val_loss: 2.0090\n",
      "Epoch 40/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 2.0818 - val_loss: 2.0991\n",
      "Epoch 41/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 2.0755 - val_loss: 1.7362\n",
      "Epoch 42/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9828 - val_loss: 2.1032\n",
      "Epoch 43/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.9255 - val_loss: 1.9426\n",
      "Epoch 44/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9359 - val_loss: 1.7231\n",
      "Epoch 45/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8639 - val_loss: 1.9698\n",
      "Epoch 46/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8479 - val_loss: 1.7180\n",
      "Epoch 47/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8749 - val_loss: 1.7765\n",
      "Epoch 48/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8729 - val_loss: 1.8548\n",
      "Epoch 49/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8895 - val_loss: 1.8376\n",
      "Epoch 50/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 2.0774 - val_loss: 1.9784\n",
      "Epoch 51/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 2.0063 - val_loss: 2.1845\n",
      "Epoch 52/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.9375 - val_loss: 1.8490\n",
      "Epoch 53/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.9482 - val_loss: 1.7922\n",
      "Epoch 54/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8914 - val_loss: 1.8472\n",
      "Epoch 55/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8779 - val_loss: 1.7201\n",
      "Epoch 56/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8469 - val_loss: 1.8106\n",
      "Epoch 57/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8728 - val_loss: 1.7239\n",
      "Epoch 58/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9534 - val_loss: 1.8075\n",
      "Epoch 59/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.9187 - val_loss: 2.1530\n",
      "Epoch 60/200\n",
      "1960/1960 [==============================] - 0s 18us/step - loss: 1.8863 - val_loss: 1.7598\n",
      "Epoch 61/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.9051 - val_loss: 1.9532\n",
      "Epoch 62/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.9016 - val_loss: 1.7391\n",
      "Epoch 63/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.9107 - val_loss: 1.8360\n",
      "Epoch 64/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8801 - val_loss: 1.7605\n",
      "Epoch 65/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9337 - val_loss: 1.7457\n",
      "Epoch 66/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8977 - val_loss: 1.7303\n",
      "Epoch 67/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8456 - val_loss: 1.7121\n",
      "Epoch 68/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8653 - val_loss: 1.9270\n",
      "Epoch 69/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.9791 - val_loss: 2.2122\n",
      "Epoch 70/200\n",
      "1960/1960 [==============================] - 0s 18us/step - loss: 1.9774 - val_loss: 1.7704\n",
      "Epoch 71/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 2.0594 - val_loss: 2.0895\n",
      "Epoch 72/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 2.2210 - val_loss: 1.9545\n",
      "Epoch 73/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9920 - val_loss: 1.8945\n",
      "Epoch 74/200\n",
      "1960/1960 [==============================] - 0s 18us/step - loss: 1.9349 - val_loss: 1.8833\n",
      "Epoch 75/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8445 - val_loss: 1.7461\n",
      "Epoch 76/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8830 - val_loss: 1.7379\n",
      "Epoch 77/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8699 - val_loss: 1.8308\n",
      "Epoch 78/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8874 - val_loss: 1.7180\n",
      "Epoch 79/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9650 - val_loss: 1.8640\n",
      "Epoch 80/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8944 - val_loss: 1.8745\n",
      "Epoch 81/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9434 - val_loss: 1.7995\n",
      "Epoch 82/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8924 - val_loss: 1.7430\n",
      "Epoch 83/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8631 - val_loss: 1.7502\n",
      "Epoch 84/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.9409 - val_loss: 2.0977\n",
      "Epoch 85/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 2.0713 - val_loss: 2.1580\n",
      "Epoch 86/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 2.0058 - val_loss: 1.8705\n",
      "Epoch 87/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.9148 - val_loss: 2.0007\n",
      "Epoch 88/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 2.0026 - val_loss: 1.7696\n",
      "Epoch 89/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9219 - val_loss: 1.7346\n",
      "Epoch 90/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9249 - val_loss: 1.9916\n",
      "Epoch 91/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 2.0129 - val_loss: 1.7451\n",
      "Epoch 92/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9148 - val_loss: 1.9481\n",
      "Epoch 93/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9688 - val_loss: 1.8612\n",
      "Epoch 94/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 2.0202 - val_loss: 2.2021\n",
      "Epoch 95/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 2.0764 - val_loss: 1.9179\n",
      "Epoch 96/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8831 - val_loss: 1.7636\n",
      "Epoch 97/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8440 - val_loss: 1.7361\n",
      "Epoch 98/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8327 - val_loss: 1.7658\n",
      "Epoch 99/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8854 - val_loss: 1.8949\n",
      "Epoch 100/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.9289 - val_loss: 1.9456\n",
      "Epoch 101/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8646 - val_loss: 1.7555\n",
      "Epoch 102/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8851 - val_loss: 1.7426\n",
      "Epoch 103/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8705 - val_loss: 1.7763\n",
      "Epoch 104/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8800 - val_loss: 1.8220\n",
      "Epoch 105/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9448 - val_loss: 1.8296\n",
      "Epoch 106/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9550 - val_loss: 1.7605\n",
      "Epoch 107/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8867 - val_loss: 1.7590\n",
      "Epoch 108/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8514 - val_loss: 1.9858\n",
      "Epoch 109/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9117 - val_loss: 1.7266\n",
      "Epoch 110/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.9090 - val_loss: 1.7150\n",
      "Epoch 111/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9163 - val_loss: 1.9851\n",
      "Epoch 112/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.9002 - val_loss: 1.7314\n",
      "Epoch 113/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8648 - val_loss: 1.9200\n",
      "Epoch 114/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8918 - val_loss: 1.7557\n",
      "Epoch 115/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8748 - val_loss: 1.7496\n",
      "Epoch 116/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8646 - val_loss: 1.8783\n",
      "Epoch 117/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.9065 - val_loss: 1.7422\n",
      "Epoch 118/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8587 - val_loss: 1.9310\n",
      "Epoch 119/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.9341 - val_loss: 1.9970\n",
      "Epoch 120/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9120 - val_loss: 1.9329\n",
      "Epoch 121/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.9209 - val_loss: 1.8012\n",
      "Epoch 122/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9323 - val_loss: 2.1450\n",
      "Epoch 123/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 2.0667 - val_loss: 1.6968\n",
      "Epoch 124/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9904 - val_loss: 1.8441\n",
      "Epoch 125/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.9788 - val_loss: 2.1896\n",
      "Epoch 126/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 2.0186 - val_loss: 1.9877\n",
      "Epoch 127/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.9516 - val_loss: 1.8440\n",
      "Epoch 128/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8431 - val_loss: 1.7167\n",
      "Epoch 129/200\n",
      "1960/1960 [==============================] - 0s 18us/step - loss: 1.8796 - val_loss: 1.7331\n",
      "Epoch 130/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8623 - val_loss: 1.7729\n",
      "Epoch 131/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8840 - val_loss: 1.7262\n",
      "Epoch 132/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8815 - val_loss: 1.7998\n",
      "Epoch 133/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8854 - val_loss: 1.8997\n",
      "Epoch 134/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9190 - val_loss: 1.8134\n",
      "Epoch 135/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8857 - val_loss: 1.9353\n",
      "Epoch 136/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8504 - val_loss: 1.7761\n",
      "Epoch 137/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8713 - val_loss: 1.8776\n",
      "Epoch 138/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9649 - val_loss: 1.8309\n",
      "Epoch 139/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.9512 - val_loss: 1.9515\n",
      "Epoch 140/200\n",
      "1960/1960 [==============================] - 0s 18us/step - loss: 1.9969 - val_loss: 1.7881\n",
      "Epoch 141/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1960/1960 [==============================] - 0s 16us/step - loss: 2.0273 - val_loss: 1.9787\n",
      "Epoch 142/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 2.0298 - val_loss: 2.0058\n",
      "Epoch 143/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 2.1015 - val_loss: 1.7434\n",
      "Epoch 144/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.9139 - val_loss: 1.8089\n",
      "Epoch 145/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8555 - val_loss: 1.7838\n",
      "Epoch 146/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8630 - val_loss: 1.7572\n",
      "Epoch 147/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8903 - val_loss: 1.7349\n",
      "Epoch 148/200\n",
      "1960/1960 [==============================] - 0s 18us/step - loss: 1.8869 - val_loss: 1.8438\n",
      "Epoch 149/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9092 - val_loss: 1.9114\n",
      "Epoch 150/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8473 - val_loss: 1.7238\n",
      "Epoch 151/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8584 - val_loss: 2.1469\n",
      "Epoch 152/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 2.0472 - val_loss: 1.7614\n",
      "Epoch 153/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.9916 - val_loss: 1.7416\n",
      "Epoch 154/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9575 - val_loss: 2.0754\n",
      "Epoch 155/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9094 - val_loss: 1.8059\n",
      "Epoch 156/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.9551 - val_loss: 1.8232\n",
      "Epoch 157/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8607 - val_loss: 1.7064\n",
      "Epoch 158/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8939 - val_loss: 1.8241\n",
      "Epoch 159/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8646 - val_loss: 1.7385\n",
      "Epoch 160/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8671 - val_loss: 1.8033\n",
      "Epoch 161/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8444 - val_loss: 1.7831\n",
      "Epoch 162/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.9350 - val_loss: 1.7522\n",
      "Epoch 163/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8761 - val_loss: 1.7916\n",
      "Epoch 164/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8813 - val_loss: 1.7642\n",
      "Epoch 165/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8450 - val_loss: 1.6852\n",
      "Epoch 166/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8672 - val_loss: 1.7248\n",
      "Epoch 167/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8844 - val_loss: 2.1036\n",
      "Epoch 168/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8858 - val_loss: 1.8106\n",
      "Epoch 169/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.9843 - val_loss: 1.7403\n",
      "Epoch 170/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8827 - val_loss: 2.1148\n",
      "Epoch 171/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8729 - val_loss: 1.6887\n",
      "Epoch 172/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8618 - val_loss: 2.0550\n",
      "Epoch 173/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8875 - val_loss: 1.7149\n",
      "Epoch 174/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8967 - val_loss: 1.7450\n",
      "Epoch 175/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8630 - val_loss: 1.8167\n",
      "Epoch 176/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8585 - val_loss: 1.7131\n",
      "Epoch 177/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8486 - val_loss: 1.9311\n",
      "Epoch 178/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.9217 - val_loss: 2.0462\n",
      "Epoch 179/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9944 - val_loss: 1.9346\n",
      "Epoch 180/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9969 - val_loss: 1.9923\n",
      "Epoch 181/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8633 - val_loss: 1.7172\n",
      "Epoch 182/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8672 - val_loss: 1.8214\n",
      "Epoch 183/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8643 - val_loss: 1.8007\n",
      "Epoch 184/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9249 - val_loss: 1.9872\n",
      "Epoch 185/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8918 - val_loss: 1.8403\n",
      "Epoch 186/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8624 - val_loss: 1.7254\n",
      "Epoch 187/200\n",
      "1960/1960 [==============================] - 0s 18us/step - loss: 1.8876 - val_loss: 1.9246\n",
      "Epoch 188/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8318 - val_loss: 1.7095\n",
      "Epoch 189/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8711 - val_loss: 2.0673\n",
      "Epoch 190/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9692 - val_loss: 1.7525\n",
      "Epoch 191/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.9542 - val_loss: 1.7223\n",
      "Epoch 192/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9248 - val_loss: 2.0116\n",
      "Epoch 193/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.9178 - val_loss: 1.8669\n",
      "Epoch 194/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8974 - val_loss: 1.7601\n",
      "Epoch 195/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8507 - val_loss: 2.1673\n",
      "Epoch 196/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9035 - val_loss: 1.7226\n",
      "Epoch 197/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8351 - val_loss: 1.8351\n",
      "Epoch 198/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8852 - val_loss: 1.8105\n",
      "Epoch 199/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8494 - val_loss: 1.7112\n",
      "Epoch 200/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8547 - val_loss: 1.7627\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_5 (InputLayer)         (None, 1)                 0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 64)                128       \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "pred (Dense)                 (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 2,241\n",
      "Trainable params: 2,241\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 1960 samples, validate on 490 samples\n",
      "Epoch 1/200\n",
      "1960/1960 [==============================] - 0s 177us/step - loss: 30.8520 - val_loss: 17.5567\n",
      "Epoch 2/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 10.6014 - val_loss: 7.7945\n",
      "Epoch 3/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 7.0542 - val_loss: 6.3640\n",
      "Epoch 4/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 5.6574 - val_loss: 5.6118\n",
      "Epoch 5/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 4.4354 - val_loss: 4.3782\n",
      "Epoch 6/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 3.5577 - val_loss: 3.7116\n",
      "Epoch 7/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 2.8679 - val_loss: 2.9389\n",
      "Epoch 8/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 2.4049 - val_loss: 2.5087\n",
      "Epoch 9/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 2.0882 - val_loss: 2.4049\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 2.0643 - val_loss: 2.4513\n",
      "Epoch 11/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9659 - val_loss: 2.2929\n",
      "Epoch 12/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8634 - val_loss: 2.6246\n",
      "Epoch 13/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9607 - val_loss: 2.4290\n",
      "Epoch 14/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8781 - val_loss: 2.3251\n",
      "Epoch 15/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8264 - val_loss: 2.2723\n",
      "Epoch 16/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.7848 - val_loss: 2.4432\n",
      "Epoch 17/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.7785 - val_loss: 2.2137\n",
      "Epoch 18/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.7639 - val_loss: 2.2059\n",
      "Epoch 19/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.7569 - val_loss: 2.4172\n",
      "Epoch 20/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8948 - val_loss: 2.3704\n",
      "Epoch 21/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8405 - val_loss: 2.3593\n",
      "Epoch 22/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8202 - val_loss: 2.3283\n",
      "Epoch 23/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.7628 - val_loss: 2.3469\n",
      "Epoch 24/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.7227 - val_loss: 2.2306\n",
      "Epoch 25/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8255 - val_loss: 2.2672\n",
      "Epoch 26/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8217 - val_loss: 2.2445\n",
      "Epoch 27/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.7996 - val_loss: 2.2933\n",
      "Epoch 28/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8705 - val_loss: 2.4366\n",
      "Epoch 29/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.7876 - val_loss: 2.3345\n",
      "Epoch 30/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.7300 - val_loss: 2.2316\n",
      "Epoch 31/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.7328 - val_loss: 2.3141\n",
      "Epoch 32/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.7321 - val_loss: 2.6472\n",
      "Epoch 33/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8279 - val_loss: 2.2743\n",
      "Epoch 34/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8161 - val_loss: 2.2319\n",
      "Epoch 35/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.7911 - val_loss: 2.2818\n",
      "Epoch 36/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.7542 - val_loss: 2.3400\n",
      "Epoch 37/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.7733 - val_loss: 2.4262\n",
      "Epoch 38/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8143 - val_loss: 2.4448\n",
      "Epoch 39/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.7406 - val_loss: 2.2600\n",
      "Epoch 40/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.7013 - val_loss: 2.2133\n",
      "Epoch 41/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.7603 - val_loss: 3.0768\n",
      "Epoch 42/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8909 - val_loss: 2.2101\n",
      "Epoch 43/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8184 - val_loss: 2.3989\n",
      "Epoch 44/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.9034 - val_loss: 2.3968\n",
      "Epoch 45/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8724 - val_loss: 2.5850\n",
      "Epoch 46/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.7172 - val_loss: 2.2638\n",
      "Epoch 47/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.7286 - val_loss: 2.9684\n",
      "Epoch 48/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.7750 - val_loss: 2.2579\n",
      "Epoch 49/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.7273 - val_loss: 2.2771\n",
      "Epoch 50/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.7558 - val_loss: 2.1986\n",
      "Epoch 51/200\n",
      "1960/1960 [==============================] - 0s 18us/step - loss: 1.7502 - val_loss: 2.9932\n",
      "Epoch 52/200\n",
      "1960/1960 [==============================] - 0s 19us/step - loss: 1.9134 - val_loss: 2.4618\n",
      "Epoch 53/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8971 - val_loss: 2.3516\n",
      "Epoch 54/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8524 - val_loss: 2.2204\n",
      "Epoch 55/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.7648 - val_loss: 2.3169\n",
      "Epoch 56/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.7855 - val_loss: 2.3133\n",
      "Epoch 57/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.7401 - val_loss: 2.4546\n",
      "Epoch 58/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.7615 - val_loss: 2.3967\n",
      "Epoch 59/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.7907 - val_loss: 2.2103\n",
      "Epoch 60/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.7103 - val_loss: 2.2306\n",
      "Epoch 61/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8027 - val_loss: 2.7725\n",
      "Epoch 62/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.7693 - val_loss: 2.2112\n",
      "Epoch 63/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.6921 - val_loss: 2.4120\n",
      "Epoch 64/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8271 - val_loss: 2.2666\n",
      "Epoch 65/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.7650 - val_loss: 2.3386\n",
      "Epoch 66/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8109 - val_loss: 2.3213\n",
      "Epoch 67/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.7779 - val_loss: 2.3440\n",
      "Epoch 68/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8214 - val_loss: 2.3601\n",
      "Epoch 69/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8104 - val_loss: 2.4370\n",
      "Epoch 70/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8245 - val_loss: 2.2523\n",
      "Epoch 71/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.7142 - val_loss: 2.3609\n",
      "Epoch 72/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.7315 - val_loss: 2.3428\n",
      "Epoch 73/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.7180 - val_loss: 2.2519\n",
      "Epoch 74/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.7734 - val_loss: 2.4973\n",
      "Epoch 75/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.7392 - val_loss: 2.2123\n",
      "Epoch 76/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.7666 - val_loss: 2.5517\n",
      "Epoch 77/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.7521 - val_loss: 2.2669\n",
      "Epoch 78/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.7291 - val_loss: 2.2477\n",
      "Epoch 79/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.7294 - val_loss: 2.5856\n",
      "Epoch 80/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.7952 - val_loss: 2.2725\n",
      "Epoch 81/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.7020 - val_loss: 2.3276\n",
      "Epoch 82/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.7230 - val_loss: 2.2073\n",
      "Epoch 83/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.7427 - val_loss: 2.2100\n",
      "Epoch 84/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.7409 - val_loss: 2.1814\n",
      "Epoch 85/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.7295 - val_loss: 2.2884\n",
      "Epoch 86/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.7683 - val_loss: 2.3388\n",
      "Epoch 87/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.7607 - val_loss: 2.3871\n",
      "Epoch 88/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.7678 - val_loss: 2.2424\n",
      "Epoch 89/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.7658 - val_loss: 2.2068\n",
      "Epoch 90/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.7760 - val_loss: 2.5154\n",
      "Epoch 91/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.7960 - val_loss: 2.3794\n",
      "Epoch 92/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.7765 - val_loss: 2.2844\n",
      "Epoch 93/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.7025 - val_loss: 2.5276\n",
      "Epoch 94/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8562 - val_loss: 2.3176\n",
      "Epoch 95/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9329 - val_loss: 2.3605\n",
      "Epoch 96/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.7705 - val_loss: 2.4276\n",
      "Epoch 97/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.7728 - val_loss: 2.5009\n",
      "Epoch 98/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.7261 - val_loss: 2.2981\n",
      "Epoch 99/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8120 - val_loss: 2.2640\n",
      "Epoch 100/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8661 - val_loss: 2.4209\n",
      "Epoch 101/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.7607 - val_loss: 2.2765\n",
      "Epoch 102/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.7727 - val_loss: 2.2376\n",
      "Epoch 103/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.7411 - val_loss: 2.3066\n",
      "Epoch 104/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.7556 - val_loss: 2.3958\n",
      "Epoch 105/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8032 - val_loss: 2.3104\n",
      "Epoch 106/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.7768 - val_loss: 2.3645\n",
      "Epoch 107/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8125 - val_loss: 2.7056\n",
      "Epoch 108/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.7408 - val_loss: 2.2092\n",
      "Epoch 109/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.7135 - val_loss: 2.2711\n",
      "Epoch 110/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.7185 - val_loss: 2.3948\n",
      "Epoch 111/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.7067 - val_loss: 2.2132\n",
      "Epoch 112/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.7022 - val_loss: 2.3146\n",
      "Epoch 113/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.7375 - val_loss: 2.4046\n",
      "Epoch 114/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.7509 - val_loss: 2.3723\n",
      "Epoch 115/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.7905 - val_loss: 2.8433\n",
      "Epoch 116/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8720 - val_loss: 2.2084\n",
      "Epoch 117/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.7320 - val_loss: 2.2768\n",
      "Epoch 118/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.7129 - val_loss: 2.2953\n",
      "Epoch 119/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.7064 - val_loss: 2.3957\n",
      "Epoch 120/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.7313 - val_loss: 2.2408\n",
      "Epoch 121/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.7770 - val_loss: 2.3272\n",
      "Epoch 122/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8658 - val_loss: 2.3542\n",
      "Epoch 123/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8478 - val_loss: 2.4879\n",
      "Epoch 124/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.7971 - val_loss: 2.4874\n",
      "Epoch 125/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8748 - val_loss: 2.2463\n",
      "Epoch 126/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.7596 - val_loss: 2.3079\n",
      "Epoch 127/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.7611 - val_loss: 2.4007\n",
      "Epoch 128/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.7425 - val_loss: 2.3332\n",
      "Epoch 129/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.7448 - val_loss: 2.4040\n",
      "Epoch 130/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.7312 - val_loss: 2.1917\n",
      "Epoch 131/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.7046 - val_loss: 2.4738\n",
      "Epoch 132/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.7533 - val_loss: 2.2675\n",
      "Epoch 133/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.7758 - val_loss: 2.2751\n",
      "Epoch 134/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.7492 - val_loss: 2.4100\n",
      "Epoch 135/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.6870 - val_loss: 2.2497\n",
      "Epoch 136/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.7946 - val_loss: 2.1933\n",
      "Epoch 137/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.7670 - val_loss: 2.8118\n",
      "Epoch 138/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8116 - val_loss: 2.3217\n",
      "Epoch 139/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.9021 - val_loss: 2.3359\n",
      "Epoch 140/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8496 - val_loss: 2.3561\n",
      "Epoch 141/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.7421 - val_loss: 2.2729\n",
      "Epoch 142/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.7623 - val_loss: 2.2546\n",
      "Epoch 143/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.7354 - val_loss: 2.7952\n",
      "Epoch 144/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.7969 - val_loss: 2.2372\n",
      "Epoch 145/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.7107 - val_loss: 2.2500\n",
      "Epoch 146/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.7279 - val_loss: 2.3453\n",
      "Epoch 147/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.7062 - val_loss: 2.2080\n",
      "Epoch 148/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.7137 - val_loss: 2.2134\n",
      "Epoch 149/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.7203 - val_loss: 2.2429\n",
      "Epoch 150/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.7384 - val_loss: 2.3727\n",
      "Epoch 151/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.7294 - val_loss: 2.3486\n",
      "Epoch 152/200\n",
      "1960/1960 [==============================] - 0s 18us/step - loss: 1.7625 - val_loss: 2.2350\n",
      "Epoch 153/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.7476 - val_loss: 2.2190\n",
      "Epoch 154/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.6954 - val_loss: 2.3193\n",
      "Epoch 155/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.7100 - val_loss: 2.3918\n",
      "Epoch 156/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.7277 - val_loss: 2.1925\n",
      "Epoch 157/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.7145 - val_loss: 2.4939\n",
      "Epoch 158/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.7359 - val_loss: 2.3658\n",
      "Epoch 159/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.7066 - val_loss: 2.2055\n",
      "Epoch 160/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.7344 - val_loss: 2.2107\n",
      "Epoch 161/200\n",
      "1960/1960 [==============================] - 0s 18us/step - loss: 1.8514 - val_loss: 2.5247\n",
      "Epoch 162/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.7478 - val_loss: 2.2884\n",
      "Epoch 163/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.7282 - val_loss: 2.4194\n",
      "Epoch 164/200\n",
      "1960/1960 [==============================] - 0s 18us/step - loss: 1.7633 - val_loss: 2.4884\n",
      "Epoch 165/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.7613 - val_loss: 2.2007\n",
      "Epoch 166/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8209 - val_loss: 2.3609\n",
      "Epoch 167/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.7460 - val_loss: 2.3480\n",
      "Epoch 168/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.7108 - val_loss: 2.3732\n",
      "Epoch 169/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.7505 - val_loss: 2.2136\n",
      "Epoch 170/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.7815 - val_loss: 2.2018\n",
      "Epoch 171/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.7845 - val_loss: 2.3211\n",
      "Epoch 172/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.7721 - val_loss: 2.3424\n",
      "Epoch 173/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.7791 - val_loss: 2.2471\n",
      "Epoch 174/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.7353 - val_loss: 2.4064\n",
      "Epoch 175/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.7304 - val_loss: 2.3148\n",
      "Epoch 176/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.7129 - val_loss: 2.2144\n",
      "Epoch 177/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.7167 - val_loss: 2.5819\n",
      "Epoch 178/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.7840 - val_loss: 2.2389\n",
      "Epoch 179/200\n",
      "1960/1960 [==============================] - 0s 18us/step - loss: 1.7265 - val_loss: 2.3193\n",
      "Epoch 180/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.7023 - val_loss: 2.2346\n",
      "Epoch 181/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.7254 - val_loss: 2.3416\n",
      "Epoch 182/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.6943 - val_loss: 2.2445\n",
      "Epoch 183/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.7458 - val_loss: 2.2194\n",
      "Epoch 184/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.7093 - val_loss: 2.2849\n",
      "Epoch 185/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.7145 - val_loss: 2.4636\n",
      "Epoch 186/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.7426 - val_loss: 2.2357\n",
      "Epoch 187/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.7276 - val_loss: 2.3468\n",
      "Epoch 188/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.7211 - val_loss: 2.3148\n",
      "Epoch 189/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.7332 - val_loss: 2.2361\n",
      "Epoch 190/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.7046 - val_loss: 2.3561\n",
      "Epoch 191/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.7278 - val_loss: 2.2189\n",
      "Epoch 192/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.7561 - val_loss: 2.1769\n",
      "Epoch 193/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.7806 - val_loss: 2.3813\n",
      "Epoch 194/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8662 - val_loss: 2.3574\n",
      "Epoch 195/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8191 - val_loss: 2.3904\n",
      "Epoch 196/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8469 - val_loss: 2.6889\n",
      "Epoch 197/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8088 - val_loss: 2.2172\n",
      "Epoch 198/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.7336 - val_loss: 2.3221\n",
      "Epoch 199/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.7059 - val_loss: 2.2946\n",
      "Epoch 200/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.7341 - val_loss: 2.2342\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_6 (InputLayer)         (None, 1)                 0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 64)                128       \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "pred (Dense)                 (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 2,241\n",
      "Trainable params: 2,241\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 1960 samples, validate on 490 samples\n",
      "Epoch 1/200\n",
      "1960/1960 [==============================] - 0s 176us/step - loss: 28.1801 - val_loss: 16.8932\n",
      "Epoch 2/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 9.7693 - val_loss: 8.3563\n",
      "Epoch 3/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 6.8895 - val_loss: 6.7713\n",
      "Epoch 4/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 5.5512 - val_loss: 6.0686\n",
      "Epoch 5/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 4.7479 - val_loss: 4.9221\n",
      "Epoch 6/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 3.8093 - val_loss: 3.8743\n",
      "Epoch 7/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 3.0503 - val_loss: 3.2753\n",
      "Epoch 8/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 2.6350 - val_loss: 2.7551\n",
      "Epoch 9/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 2.3183 - val_loss: 2.2262\n",
      "Epoch 10/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 2.1109 - val_loss: 2.0499\n",
      "Epoch 11/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 2.0833 - val_loss: 1.8857\n",
      "Epoch 12/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 2.0474 - val_loss: 1.8469\n",
      "Epoch 13/200\n",
      "1960/1960 [==============================] - 0s 20us/step - loss: 2.0454 - val_loss: 1.9561\n",
      "Epoch 14/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 2.0311 - val_loss: 1.8866\n",
      "Epoch 15/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 2.2210 - val_loss: 2.1582\n",
      "Epoch 16/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 2.0667 - val_loss: 1.9320\n",
      "Epoch 17/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9658 - val_loss: 1.7946\n",
      "Epoch 18/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.9514 - val_loss: 1.8136\n",
      "Epoch 19/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.9609 - val_loss: 1.7792\n",
      "Epoch 20/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9538 - val_loss: 1.8274\n",
      "Epoch 21/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9252 - val_loss: 1.9558\n",
      "Epoch 22/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 2.0239 - val_loss: 1.9235\n",
      "Epoch 23/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.9238 - val_loss: 1.7160\n",
      "Epoch 24/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8705 - val_loss: 1.7433\n",
      "Epoch 25/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9123 - val_loss: 1.8623\n",
      "Epoch 26/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9006 - val_loss: 1.7138\n",
      "Epoch 27/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8461 - val_loss: 1.6759\n",
      "Epoch 28/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8605 - val_loss: 1.7872\n",
      "Epoch 29/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8872 - val_loss: 1.8623\n",
      "Epoch 30/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9045 - val_loss: 1.7600\n",
      "Epoch 31/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8388 - val_loss: 1.6900\n",
      "Epoch 32/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9050 - val_loss: 1.7559\n",
      "Epoch 33/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8836 - val_loss: 1.7103\n",
      "Epoch 34/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9623 - val_loss: 1.7344\n",
      "Epoch 35/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8907 - val_loss: 1.7730\n",
      "Epoch 36/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8931 - val_loss: 1.7316\n",
      "Epoch 37/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8854 - val_loss: 1.7895\n",
      "Epoch 38/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.9947 - val_loss: 1.7634\n",
      "Epoch 39/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8803 - val_loss: 1.8273\n",
      "Epoch 40/200\n",
      "1960/1960 [==============================] - 0s 18us/step - loss: 1.8700 - val_loss: 1.7507\n",
      "Epoch 41/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8912 - val_loss: 1.7583\n",
      "Epoch 42/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.9326 - val_loss: 1.7125\n",
      "Epoch 43/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9140 - val_loss: 1.7305\n",
      "Epoch 44/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.9061 - val_loss: 1.7103\n",
      "Epoch 45/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8927 - val_loss: 1.6895\n",
      "Epoch 46/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8990 - val_loss: 1.7580\n",
      "Epoch 47/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8988 - val_loss: 1.7255\n",
      "Epoch 48/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8908 - val_loss: 1.8222\n",
      "Epoch 49/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.9751 - val_loss: 1.9326\n",
      "Epoch 50/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9810 - val_loss: 1.8404\n",
      "Epoch 51/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8952 - val_loss: 1.8224\n",
      "Epoch 52/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.9577 - val_loss: 1.9292\n",
      "Epoch 53/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9159 - val_loss: 1.7457\n",
      "Epoch 54/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9140 - val_loss: 1.7550\n",
      "Epoch 55/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9026 - val_loss: 1.7741\n",
      "Epoch 56/200\n",
      "1960/1960 [==============================] - 0s 18us/step - loss: 1.9392 - val_loss: 2.0124\n",
      "Epoch 57/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.9286 - val_loss: 1.8427\n",
      "Epoch 58/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9396 - val_loss: 1.8626\n",
      "Epoch 59/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8710 - val_loss: 1.7428\n",
      "Epoch 60/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8621 - val_loss: 1.7331\n",
      "Epoch 61/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8933 - val_loss: 1.8254\n",
      "Epoch 62/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8215 - val_loss: 1.7807\n",
      "Epoch 63/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 2.0518 - val_loss: 1.8900\n",
      "Epoch 64/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 2.0003 - val_loss: 1.8387\n",
      "Epoch 65/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.9280 - val_loss: 1.7589\n",
      "Epoch 66/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8745 - val_loss: 1.7782\n",
      "Epoch 67/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8908 - val_loss: 1.7560\n",
      "Epoch 68/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8767 - val_loss: 1.6839\n",
      "Epoch 69/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8471 - val_loss: 1.6858\n",
      "Epoch 70/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8980 - val_loss: 1.8920\n",
      "Epoch 71/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 2.0628 - val_loss: 1.8412\n",
      "Epoch 72/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9829 - val_loss: 1.7451\n",
      "Epoch 73/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8867 - val_loss: 1.8900\n",
      "Epoch 74/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9573 - val_loss: 1.7756\n",
      "Epoch 75/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.9150 - val_loss: 1.8079\n",
      "Epoch 76/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.9565 - val_loss: 1.7167\n",
      "Epoch 77/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.9198 - val_loss: 1.7367\n",
      "Epoch 78/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.9415 - val_loss: 2.0484\n",
      "Epoch 79/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.9795 - val_loss: 1.8845\n",
      "Epoch 80/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.9559 - val_loss: 1.7034\n",
      "Epoch 81/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8764 - val_loss: 1.7400\n",
      "Epoch 82/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8858 - val_loss: 1.7152\n",
      "Epoch 83/200\n",
      "1960/1960 [==============================] - 0s 18us/step - loss: 1.8958 - val_loss: 1.7944\n",
      "Epoch 84/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9124 - val_loss: 1.8485\n",
      "Epoch 85/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9446 - val_loss: 1.8678\n",
      "Epoch 86/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8878 - val_loss: 1.7674\n",
      "Epoch 87/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.9379 - val_loss: 1.8043\n",
      "Epoch 88/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9710 - val_loss: 1.7897\n",
      "Epoch 89/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.9231 - val_loss: 1.7517\n",
      "Epoch 90/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9075 - val_loss: 1.6994\n",
      "Epoch 91/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9235 - val_loss: 1.9235\n",
      "Epoch 92/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8736 - val_loss: 1.7078\n",
      "Epoch 93/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8829 - val_loss: 1.7286\n",
      "Epoch 94/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8913 - val_loss: 1.8116\n",
      "Epoch 95/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8910 - val_loss: 1.7332\n",
      "Epoch 96/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8671 - val_loss: 1.7058\n",
      "Epoch 97/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8516 - val_loss: 1.6995\n",
      "Epoch 98/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8542 - val_loss: 1.7068\n",
      "Epoch 99/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8517 - val_loss: 1.8521\n",
      "Epoch 100/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 2.0092 - val_loss: 1.8210\n",
      "Epoch 101/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.9710 - val_loss: 1.6995\n",
      "Epoch 102/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9254 - val_loss: 1.7648\n",
      "Epoch 103/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8954 - val_loss: 1.8111\n",
      "Epoch 104/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.9102 - val_loss: 1.8059\n",
      "Epoch 105/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9079 - val_loss: 1.8875\n",
      "Epoch 106/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 2.0095 - val_loss: 2.0312\n",
      "Epoch 107/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 2.0443 - val_loss: 1.7747\n",
      "Epoch 108/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.9193 - val_loss: 1.7170\n",
      "Epoch 109/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8883 - val_loss: 1.7792\n",
      "Epoch 110/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8882 - val_loss: 1.6927\n",
      "Epoch 111/200\n",
      "1960/1960 [==============================] - 0s 18us/step - loss: 1.8451 - val_loss: 1.6989\n",
      "Epoch 112/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8861 - val_loss: 1.7234\n",
      "Epoch 113/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8811 - val_loss: 1.7557\n",
      "Epoch 114/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9013 - val_loss: 1.7755\n",
      "Epoch 115/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8750 - val_loss: 1.6868\n",
      "Epoch 116/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8993 - val_loss: 1.8354\n",
      "Epoch 117/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9499 - val_loss: 1.8150\n",
      "Epoch 118/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9152 - val_loss: 1.7514\n",
      "Epoch 119/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8798 - val_loss: 1.7862\n",
      "Epoch 120/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.9419 - val_loss: 1.8799\n",
      "Epoch 121/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9485 - val_loss: 1.6800\n",
      "Epoch 122/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8695 - val_loss: 2.0017\n",
      "Epoch 123/200\n",
      "1960/1960 [==============================] - 0s 18us/step - loss: 1.9488 - val_loss: 1.7086\n",
      "Epoch 124/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8723 - val_loss: 2.0306\n",
      "Epoch 125/200\n",
      "1960/1960 [==============================] - 0s 18us/step - loss: 2.0348 - val_loss: 1.8456\n",
      "Epoch 126/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8919 - val_loss: 1.7751\n",
      "Epoch 127/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8678 - val_loss: 1.7677\n",
      "Epoch 128/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.9218 - val_loss: 1.7462\n",
      "Epoch 129/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8823 - val_loss: 1.6981\n",
      "Epoch 130/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8718 - val_loss: 1.8179\n",
      "Epoch 131/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9064 - val_loss: 1.7099\n",
      "Epoch 132/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.9020 - val_loss: 1.8519\n",
      "Epoch 133/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9724 - val_loss: 1.7026\n",
      "Epoch 134/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9375 - val_loss: 1.9353\n",
      "Epoch 135/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9233 - val_loss: 1.8461\n",
      "Epoch 136/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9316 - val_loss: 1.8941\n",
      "Epoch 137/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9937 - val_loss: 1.8287\n",
      "Epoch 138/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9500 - val_loss: 1.9772\n",
      "Epoch 139/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 2.0050 - val_loss: 1.7309\n",
      "Epoch 140/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.9373 - val_loss: 1.9565\n",
      "Epoch 141/200\n",
      "1960/1960 [==============================] - 0s 18us/step - loss: 1.9588 - val_loss: 1.7411\n",
      "Epoch 142/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9096 - val_loss: 1.9035\n",
      "Epoch 143/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 2.0909 - val_loss: 1.7359\n",
      "Epoch 144/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 2.0321 - val_loss: 1.7161\n",
      "Epoch 145/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.9496 - val_loss: 1.8290\n",
      "Epoch 146/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8634 - val_loss: 1.7006\n",
      "Epoch 147/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9109 - val_loss: 1.7515\n",
      "Epoch 148/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9341 - val_loss: 1.7523\n",
      "Epoch 149/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 2.0061 - val_loss: 2.1792\n",
      "Epoch 150/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 2.0640 - val_loss: 1.8675\n",
      "Epoch 151/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.9504 - val_loss: 1.7104\n",
      "Epoch 152/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8621 - val_loss: 1.7277\n",
      "Epoch 153/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8577 - val_loss: 1.7917\n",
      "Epoch 154/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8584 - val_loss: 1.8416\n",
      "Epoch 155/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.9191 - val_loss: 1.7727\n",
      "Epoch 156/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8778 - val_loss: 1.7920\n",
      "Epoch 157/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.9248 - val_loss: 1.7825\n",
      "Epoch 158/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.9540 - val_loss: 1.7136\n",
      "Epoch 159/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8453 - val_loss: 1.7752\n",
      "Epoch 160/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9797 - val_loss: 1.7231\n",
      "Epoch 161/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9261 - val_loss: 1.7128\n",
      "Epoch 162/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8998 - val_loss: 1.8129\n",
      "Epoch 163/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9400 - val_loss: 1.7615\n",
      "Epoch 164/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8578 - val_loss: 1.6708\n",
      "Epoch 165/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8467 - val_loss: 1.7373\n",
      "Epoch 166/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8783 - val_loss: 1.7216\n",
      "Epoch 167/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8853 - val_loss: 1.7210\n",
      "Epoch 168/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8614 - val_loss: 1.7532\n",
      "Epoch 169/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8402 - val_loss: 1.6831\n",
      "Epoch 170/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8800 - val_loss: 1.8053\n",
      "Epoch 171/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8797 - val_loss: 1.7018\n",
      "Epoch 172/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8398 - val_loss: 1.7307\n",
      "Epoch 173/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8626 - val_loss: 1.6903\n",
      "Epoch 174/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9136 - val_loss: 1.8619\n",
      "Epoch 175/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.9731 - val_loss: 1.7804\n",
      "Epoch 176/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9334 - val_loss: 1.7703\n",
      "Epoch 177/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.9597 - val_loss: 1.7771\n",
      "Epoch 178/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8914 - val_loss: 1.8289\n",
      "Epoch 179/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.9647 - val_loss: 1.8514\n",
      "Epoch 180/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9759 - val_loss: 1.7814\n",
      "Epoch 181/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.9101 - val_loss: 1.7005\n",
      "Epoch 182/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8643 - val_loss: 1.7025\n",
      "Epoch 183/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8678 - val_loss: 1.8870\n",
      "Epoch 184/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9969 - val_loss: 1.9796\n",
      "Epoch 185/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 2.0310 - val_loss: 1.7458\n",
      "Epoch 186/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9346 - val_loss: 1.7932\n",
      "Epoch 187/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.9037 - val_loss: 1.7404\n",
      "Epoch 188/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8677 - val_loss: 1.7033\n",
      "Epoch 189/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8652 - val_loss: 1.7860\n",
      "Epoch 190/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8351 - val_loss: 1.7460\n",
      "Epoch 191/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9929 - val_loss: 2.1476\n",
      "Epoch 192/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 2.0787 - val_loss: 1.8324\n",
      "Epoch 193/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9279 - val_loss: 1.6890\n",
      "Epoch 194/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8677 - val_loss: 1.8091\n",
      "Epoch 195/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8725 - val_loss: 1.8032\n",
      "Epoch 196/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8911 - val_loss: 1.7233\n",
      "Epoch 197/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8603 - val_loss: 1.7180\n",
      "Epoch 198/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8902 - val_loss: 1.7390\n",
      "Epoch 199/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8717 - val_loss: 1.7373\n",
      "Epoch 200/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8884 - val_loss: 1.7282\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_7 (InputLayer)         (None, 1)                 0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 64)                128       \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "pred (Dense)                 (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 2,241\n",
      "Trainable params: 2,241\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 1960 samples, validate on 490 samples\n",
      "Epoch 1/200\n",
      "1960/1960 [==============================] - 0s 190us/step - loss: 32.4391 - val_loss: 10.7608\n",
      "Epoch 2/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 10.5458 - val_loss: 9.3930\n",
      "Epoch 3/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 7.4628 - val_loss: 6.8694\n",
      "Epoch 4/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 5.8885 - val_loss: 5.3925\n",
      "Epoch 5/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 4.7395 - val_loss: 4.5224\n",
      "Epoch 6/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 3.6111 - val_loss: 3.6108\n",
      "Epoch 7/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 3.0375 - val_loss: 2.8177\n",
      "Epoch 8/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 2.4854 - val_loss: 2.4974\n",
      "Epoch 9/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 2.1937 - val_loss: 2.1891\n",
      "Epoch 10/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 2.0819 - val_loss: 2.3092\n",
      "Epoch 11/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 2.1482 - val_loss: 1.9512\n",
      "Epoch 12/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9753 - val_loss: 1.9709\n",
      "Epoch 13/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8912 - val_loss: 1.9411\n",
      "Epoch 14/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8769 - val_loss: 1.9568\n",
      "Epoch 15/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9269 - val_loss: 1.9646\n",
      "Epoch 16/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.9843 - val_loss: 2.3158\n",
      "Epoch 17/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9114 - val_loss: 1.9423\n",
      "Epoch 18/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8550 - val_loss: 1.9358\n",
      "Epoch 19/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8585 - val_loss: 2.0104\n",
      "Epoch 20/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8648 - val_loss: 1.9357\n",
      "Epoch 21/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8195 - val_loss: 1.9181\n",
      "Epoch 22/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8852 - val_loss: 1.9281\n",
      "Epoch 23/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8928 - val_loss: 2.1065\n",
      "Epoch 24/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8679 - val_loss: 2.1365\n",
      "Epoch 25/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9204 - val_loss: 2.2283\n",
      "Epoch 26/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9953 - val_loss: 2.0859\n",
      "Epoch 27/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9878 - val_loss: 2.0491\n",
      "Epoch 28/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8377 - val_loss: 2.0662\n",
      "Epoch 29/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9514 - val_loss: 1.9330\n",
      "Epoch 30/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8232 - val_loss: 2.0336\n",
      "Epoch 31/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8909 - val_loss: 1.9852\n",
      "Epoch 32/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9000 - val_loss: 1.9481\n",
      "Epoch 33/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8328 - val_loss: 1.9233\n",
      "Epoch 34/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8733 - val_loss: 2.1254\n",
      "Epoch 35/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8601 - val_loss: 2.0371\n",
      "Epoch 36/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.9411 - val_loss: 1.9936\n",
      "Epoch 37/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9966 - val_loss: 1.9915\n",
      "Epoch 38/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8028 - val_loss: 1.9059\n",
      "Epoch 39/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8359 - val_loss: 1.9609\n",
      "Epoch 40/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8494 - val_loss: 1.9519\n",
      "Epoch 41/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9149 - val_loss: 1.9569\n",
      "Epoch 42/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8742 - val_loss: 2.0018\n",
      "Epoch 43/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8823 - val_loss: 1.9412\n",
      "Epoch 44/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8076 - val_loss: 1.9050\n",
      "Epoch 45/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.7910 - val_loss: 1.9733\n",
      "Epoch 46/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8533 - val_loss: 1.9062\n",
      "Epoch 47/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8037 - val_loss: 1.9213\n",
      "Epoch 48/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8075 - val_loss: 1.8858\n",
      "Epoch 49/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8841 - val_loss: 2.0823\n",
      "Epoch 50/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.9114 - val_loss: 2.1102\n",
      "Epoch 51/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8504 - val_loss: 2.0329\n",
      "Epoch 52/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8206 - val_loss: 1.9271\n",
      "Epoch 53/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8365 - val_loss: 1.9727\n",
      "Epoch 54/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8173 - val_loss: 1.9904\n",
      "Epoch 55/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.7819 - val_loss: 1.9364\n",
      "Epoch 56/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.7963 - val_loss: 1.9032\n",
      "Epoch 57/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8269 - val_loss: 1.9937\n",
      "Epoch 58/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8243 - val_loss: 1.9586\n",
      "Epoch 59/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8679 - val_loss: 1.8999\n",
      "Epoch 60/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8696 - val_loss: 1.9696\n",
      "Epoch 61/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8801 - val_loss: 1.9636\n",
      "Epoch 62/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8738 - val_loss: 1.9979\n",
      "Epoch 63/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8485 - val_loss: 1.9008\n",
      "Epoch 64/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8142 - val_loss: 1.9613\n",
      "Epoch 65/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9245 - val_loss: 2.2030\n",
      "Epoch 66/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8370 - val_loss: 1.9720\n",
      "Epoch 67/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.7959 - val_loss: 2.1564\n",
      "Epoch 68/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8926 - val_loss: 2.0204\n",
      "Epoch 69/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9139 - val_loss: 2.0088\n",
      "Epoch 70/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8313 - val_loss: 1.9187\n",
      "Epoch 71/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8387 - val_loss: 2.0516\n",
      "Epoch 72/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9139 - val_loss: 2.0108\n",
      "Epoch 73/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9000 - val_loss: 1.9368\n",
      "Epoch 74/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9348 - val_loss: 2.0054\n",
      "Epoch 75/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8491 - val_loss: 1.9169\n",
      "Epoch 76/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8168 - val_loss: 2.1039\n",
      "Epoch 77/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8292 - val_loss: 1.9642\n",
      "Epoch 78/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8459 - val_loss: 1.9333\n",
      "Epoch 79/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8230 - val_loss: 1.9861\n",
      "Epoch 80/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8525 - val_loss: 2.2898\n",
      "Epoch 81/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9055 - val_loss: 2.0254\n",
      "Epoch 82/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8491 - val_loss: 2.1127\n",
      "Epoch 83/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8621 - val_loss: 1.9921\n",
      "Epoch 84/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9124 - val_loss: 1.9811\n",
      "Epoch 85/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8518 - val_loss: 1.9561\n",
      "Epoch 86/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.7941 - val_loss: 1.9412\n",
      "Epoch 87/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8590 - val_loss: 2.0584\n",
      "Epoch 88/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8996 - val_loss: 2.0598\n",
      "Epoch 89/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8764 - val_loss: 1.9107\n",
      "Epoch 90/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9274 - val_loss: 2.2650\n",
      "Epoch 91/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.9157 - val_loss: 2.0059\n",
      "Epoch 92/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 2.0864 - val_loss: 2.3188\n",
      "Epoch 93/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.9940 - val_loss: 1.9472\n",
      "Epoch 94/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8531 - val_loss: 1.9914\n",
      "Epoch 95/200\n",
      "1960/1960 [==============================] - 0s 18us/step - loss: 1.9371 - val_loss: 1.9518\n",
      "Epoch 96/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9060 - val_loss: 1.9285\n",
      "Epoch 97/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8864 - val_loss: 2.1273\n",
      "Epoch 98/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9083 - val_loss: 1.9557\n",
      "Epoch 99/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8706 - val_loss: 2.0612\n",
      "Epoch 100/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8595 - val_loss: 1.9175\n",
      "Epoch 101/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8075 - val_loss: 2.0007\n",
      "Epoch 102/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8931 - val_loss: 1.9806\n",
      "Epoch 103/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8854 - val_loss: 1.9817\n",
      "Epoch 104/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8215 - val_loss: 1.9689\n",
      "Epoch 105/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8444 - val_loss: 2.0766\n",
      "Epoch 106/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8147 - val_loss: 2.0199\n",
      "Epoch 107/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8474 - val_loss: 1.9072\n",
      "Epoch 108/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.7972 - val_loss: 1.9616\n",
      "Epoch 109/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8222 - val_loss: 1.9315\n",
      "Epoch 110/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.7753 - val_loss: 1.9158\n",
      "Epoch 111/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.7848 - val_loss: 1.9173\n",
      "Epoch 112/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.7967 - val_loss: 1.9111\n",
      "Epoch 113/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8153 - val_loss: 1.9522\n",
      "Epoch 114/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8138 - val_loss: 1.9101\n",
      "Epoch 115/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.7912 - val_loss: 2.0122\n",
      "Epoch 116/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8149 - val_loss: 1.8916\n",
      "Epoch 117/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8356 - val_loss: 1.9382\n",
      "Epoch 118/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.7915 - val_loss: 1.9096\n",
      "Epoch 119/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8132 - val_loss: 1.9807\n",
      "Epoch 120/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 2.0286 - val_loss: 2.3284\n",
      "Epoch 121/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8780 - val_loss: 2.0989\n",
      "Epoch 122/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8605 - val_loss: 2.0945\n",
      "Epoch 123/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8811 - val_loss: 1.9354\n",
      "Epoch 124/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8431 - val_loss: 1.8894\n",
      "Epoch 125/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.7944 - val_loss: 1.8892\n",
      "Epoch 126/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.7960 - val_loss: 1.9109\n",
      "Epoch 127/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.7974 - val_loss: 1.9636\n",
      "Epoch 128/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.7959 - val_loss: 1.9251\n",
      "Epoch 129/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8021 - val_loss: 1.9038\n",
      "Epoch 130/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8154 - val_loss: 1.9433\n",
      "Epoch 131/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8332 - val_loss: 1.9356\n",
      "Epoch 132/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8583 - val_loss: 2.2232\n",
      "Epoch 133/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.9289 - val_loss: 2.0476\n",
      "Epoch 134/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8211 - val_loss: 1.9562\n",
      "Epoch 135/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8359 - val_loss: 2.0076\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 136/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8256 - val_loss: 1.9343\n",
      "Epoch 137/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8972 - val_loss: 2.1207\n",
      "Epoch 138/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8937 - val_loss: 2.0858\n",
      "Epoch 139/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.9505 - val_loss: 2.0614\n",
      "Epoch 140/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 2.0234 - val_loss: 2.1689\n",
      "Epoch 141/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8784 - val_loss: 1.9198\n",
      "Epoch 142/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8419 - val_loss: 1.8826\n",
      "Epoch 143/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8281 - val_loss: 2.0897\n",
      "Epoch 144/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9513 - val_loss: 1.9235\n",
      "Epoch 145/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.7965 - val_loss: 1.9296\n",
      "Epoch 146/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8275 - val_loss: 1.9480\n",
      "Epoch 147/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.7997 - val_loss: 1.9449\n",
      "Epoch 148/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8251 - val_loss: 2.0394\n",
      "Epoch 149/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.7843 - val_loss: 1.9166\n",
      "Epoch 150/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8301 - val_loss: 2.0002\n",
      "Epoch 151/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8175 - val_loss: 2.0043\n",
      "Epoch 152/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8562 - val_loss: 2.0493\n",
      "Epoch 153/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9110 - val_loss: 2.0875\n",
      "Epoch 154/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9854 - val_loss: 2.0157\n",
      "Epoch 155/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.9353 - val_loss: 1.8863\n",
      "Epoch 156/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8434 - val_loss: 1.9398\n",
      "Epoch 157/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.7942 - val_loss: 1.9040\n",
      "Epoch 158/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8328 - val_loss: 2.0448\n",
      "Epoch 159/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8306 - val_loss: 1.9690\n",
      "Epoch 160/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9104 - val_loss: 2.0252\n",
      "Epoch 161/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 2.0638 - val_loss: 2.0655\n",
      "Epoch 162/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9186 - val_loss: 2.0238\n",
      "Epoch 163/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9012 - val_loss: 2.0801\n",
      "Epoch 164/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8782 - val_loss: 2.1546\n",
      "Epoch 165/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9112 - val_loss: 1.8909\n",
      "Epoch 166/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8426 - val_loss: 1.8967\n",
      "Epoch 167/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9364 - val_loss: 1.9441\n",
      "Epoch 168/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8705 - val_loss: 1.9017\n",
      "Epoch 169/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.7880 - val_loss: 1.9803\n",
      "Epoch 170/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8052 - val_loss: 1.9330\n",
      "Epoch 171/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8185 - val_loss: 1.9330\n",
      "Epoch 172/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9083 - val_loss: 2.1822\n",
      "Epoch 173/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 2.1315 - val_loss: 2.5703\n",
      "Epoch 174/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9354 - val_loss: 2.0036\n",
      "Epoch 175/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9758 - val_loss: 2.1488\n",
      "Epoch 176/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8541 - val_loss: 1.9774\n",
      "Epoch 177/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8755 - val_loss: 2.0645\n",
      "Epoch 178/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9267 - val_loss: 1.9376\n",
      "Epoch 179/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9063 - val_loss: 1.9513\n",
      "Epoch 180/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.7954 - val_loss: 1.9548\n",
      "Epoch 181/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8859 - val_loss: 1.9621\n",
      "Epoch 182/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8145 - val_loss: 1.9291\n",
      "Epoch 183/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8223 - val_loss: 2.0073\n",
      "Epoch 184/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8390 - val_loss: 1.9534\n",
      "Epoch 185/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8185 - val_loss: 1.9154\n",
      "Epoch 186/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8213 - val_loss: 2.0630\n",
      "Epoch 187/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8364 - val_loss: 1.9756\n",
      "Epoch 188/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8199 - val_loss: 2.0279\n",
      "Epoch 189/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8409 - val_loss: 1.9089\n",
      "Epoch 190/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8418 - val_loss: 1.9197\n",
      "Epoch 191/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8552 - val_loss: 2.0077\n",
      "Epoch 192/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8402 - val_loss: 1.9680\n",
      "Epoch 193/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8256 - val_loss: 1.9208\n",
      "Epoch 194/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8420 - val_loss: 2.1740\n",
      "Epoch 195/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.9157 - val_loss: 2.0914\n",
      "Epoch 196/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8947 - val_loss: 1.9116\n",
      "Epoch 197/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8906 - val_loss: 1.9606\n",
      "Epoch 198/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8926 - val_loss: 1.9374\n",
      "Epoch 199/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8368 - val_loss: 2.0033\n",
      "Epoch 200/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8266 - val_loss: 1.9965\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_8 (InputLayer)         (None, 1)                 0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 64)                128       \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "pred (Dense)                 (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 2,241\n",
      "Trainable params: 2,241\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 1960 samples, validate on 490 samples\n",
      "Epoch 1/200\n",
      "1960/1960 [==============================] - 0s 203us/step - loss: 42.8863 - val_loss: 18.1967\n",
      "Epoch 2/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 11.9579 - val_loss: 9.6529\n",
      "Epoch 3/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 8.0351 - val_loss: 7.8485\n",
      "Epoch 4/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 6.5809 - val_loss: 6.5977\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 5.6781 - val_loss: 5.4841\n",
      "Epoch 6/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 4.8896 - val_loss: 4.5356\n",
      "Epoch 7/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 3.6672 - val_loss: 3.4985\n",
      "Epoch 8/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 2.8878 - val_loss: 2.8642\n",
      "Epoch 9/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 2.4070 - val_loss: 2.3854\n",
      "Epoch 10/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 2.1238 - val_loss: 2.1966\n",
      "Epoch 11/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 2.0193 - val_loss: 2.1033\n",
      "Epoch 12/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.9380 - val_loss: 2.2511\n",
      "Epoch 13/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.9179 - val_loss: 2.0276\n",
      "Epoch 14/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 2.0342 - val_loss: 1.9672\n",
      "Epoch 15/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9185 - val_loss: 1.9654\n",
      "Epoch 16/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8901 - val_loss: 2.0018\n",
      "Epoch 17/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8717 - val_loss: 2.0194\n",
      "Epoch 18/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8520 - val_loss: 2.0077\n",
      "Epoch 19/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8804 - val_loss: 2.2020\n",
      "Epoch 20/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9824 - val_loss: 2.0090\n",
      "Epoch 21/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9515 - val_loss: 1.9515\n",
      "Epoch 22/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8295 - val_loss: 1.9165\n",
      "Epoch 23/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8075 - val_loss: 2.0838\n",
      "Epoch 24/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.7924 - val_loss: 1.9396\n",
      "Epoch 25/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8534 - val_loss: 1.9374\n",
      "Epoch 26/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8052 - val_loss: 1.9763\n",
      "Epoch 27/200\n",
      "1960/1960 [==============================] - 0s 18us/step - loss: 1.8693 - val_loss: 2.1586\n",
      "Epoch 28/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8670 - val_loss: 1.9824\n",
      "Epoch 29/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8365 - val_loss: 1.9017\n",
      "Epoch 30/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8356 - val_loss: 1.9153\n",
      "Epoch 31/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8168 - val_loss: 1.9852\n",
      "Epoch 32/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8635 - val_loss: 1.9205\n",
      "Epoch 33/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8858 - val_loss: 1.9822\n",
      "Epoch 34/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8181 - val_loss: 1.9566\n",
      "Epoch 35/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.7974 - val_loss: 1.9256\n",
      "Epoch 36/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8161 - val_loss: 1.9312\n",
      "Epoch 37/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8376 - val_loss: 1.9395\n",
      "Epoch 38/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.7973 - val_loss: 1.9478\n",
      "Epoch 39/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8117 - val_loss: 1.9130\n",
      "Epoch 40/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8024 - val_loss: 1.9294\n",
      "Epoch 41/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8427 - val_loss: 2.0113\n",
      "Epoch 42/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8184 - val_loss: 1.9687\n",
      "Epoch 43/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8573 - val_loss: 1.9370\n",
      "Epoch 44/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8366 - val_loss: 2.0785\n",
      "Epoch 45/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9676 - val_loss: 2.0571\n",
      "Epoch 46/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8629 - val_loss: 1.9453\n",
      "Epoch 47/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8646 - val_loss: 2.4366\n",
      "Epoch 48/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.9773 - val_loss: 2.0725\n",
      "Epoch 49/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.7959 - val_loss: 1.9704\n",
      "Epoch 50/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8738 - val_loss: 1.9811\n",
      "Epoch 51/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8508 - val_loss: 2.1853\n",
      "Epoch 52/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8550 - val_loss: 1.8895\n",
      "Epoch 53/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.7640 - val_loss: 2.1149\n",
      "Epoch 54/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8081 - val_loss: 2.0186\n",
      "Epoch 55/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8923 - val_loss: 2.0847\n",
      "Epoch 56/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8177 - val_loss: 2.0411\n",
      "Epoch 57/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8411 - val_loss: 1.9254\n",
      "Epoch 58/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8975 - val_loss: 1.9087\n",
      "Epoch 59/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8200 - val_loss: 1.9425\n",
      "Epoch 60/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8347 - val_loss: 1.9090\n",
      "Epoch 61/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8286 - val_loss: 2.1321\n",
      "Epoch 62/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.7900 - val_loss: 1.9421\n",
      "Epoch 63/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.7985 - val_loss: 1.9070\n",
      "Epoch 64/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.7963 - val_loss: 1.9577\n",
      "Epoch 65/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.7869 - val_loss: 1.9273\n",
      "Epoch 66/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8932 - val_loss: 1.9458\n",
      "Epoch 67/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8466 - val_loss: 2.1947\n",
      "Epoch 68/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8197 - val_loss: 2.0717\n",
      "Epoch 69/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8089 - val_loss: 1.9678\n",
      "Epoch 70/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8686 - val_loss: 2.0204\n",
      "Epoch 71/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8243 - val_loss: 1.9358\n",
      "Epoch 72/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8364 - val_loss: 1.9156\n",
      "Epoch 73/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8637 - val_loss: 2.1535\n",
      "Epoch 74/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9714 - val_loss: 2.2540\n",
      "Epoch 75/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8915 - val_loss: 1.9430\n",
      "Epoch 76/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8033 - val_loss: 1.9745\n",
      "Epoch 77/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.7962 - val_loss: 2.0183\n",
      "Epoch 78/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.7896 - val_loss: 1.9301\n",
      "Epoch 79/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8088 - val_loss: 2.0298\n",
      "Epoch 80/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8299 - val_loss: 1.9369\n",
      "Epoch 81/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8380 - val_loss: 1.9401\n",
      "Epoch 82/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8185 - val_loss: 1.9710\n",
      "Epoch 83/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8445 - val_loss: 2.0052\n",
      "Epoch 84/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8192 - val_loss: 1.9985\n",
      "Epoch 85/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8161 - val_loss: 1.9090\n",
      "Epoch 86/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8485 - val_loss: 1.8927\n",
      "Epoch 87/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8008 - val_loss: 2.0208\n",
      "Epoch 88/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.7978 - val_loss: 1.9861\n",
      "Epoch 89/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.7964 - val_loss: 1.9456\n",
      "Epoch 90/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.7999 - val_loss: 1.9254\n",
      "Epoch 91/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.7850 - val_loss: 2.1212\n",
      "Epoch 92/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8984 - val_loss: 2.0482\n",
      "Epoch 93/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8540 - val_loss: 2.0666\n",
      "Epoch 94/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8408 - val_loss: 1.9504\n",
      "Epoch 95/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8209 - val_loss: 2.0391\n",
      "Epoch 96/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9367 - val_loss: 2.0133\n",
      "Epoch 97/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8179 - val_loss: 2.0537\n",
      "Epoch 98/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8447 - val_loss: 1.9621\n",
      "Epoch 99/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8609 - val_loss: 2.0446\n",
      "Epoch 100/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8014 - val_loss: 1.9134\n",
      "Epoch 101/200\n",
      "1960/1960 [==============================] - 0s 18us/step - loss: 1.8391 - val_loss: 1.9444\n",
      "Epoch 102/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8376 - val_loss: 2.2160\n",
      "Epoch 103/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8111 - val_loss: 1.9159\n",
      "Epoch 104/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8339 - val_loss: 2.0679\n",
      "Epoch 105/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9896 - val_loss: 2.0753\n",
      "Epoch 106/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.9064 - val_loss: 2.1402\n",
      "Epoch 107/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8800 - val_loss: 2.0281\n",
      "Epoch 108/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8223 - val_loss: 2.0345\n",
      "Epoch 109/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8254 - val_loss: 1.9170\n",
      "Epoch 110/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8450 - val_loss: 1.9730\n",
      "Epoch 111/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8149 - val_loss: 1.9383\n",
      "Epoch 112/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.7812 - val_loss: 2.0059\n",
      "Epoch 113/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.7799 - val_loss: 1.9415\n",
      "Epoch 114/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.7949 - val_loss: 2.0079\n",
      "Epoch 115/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8177 - val_loss: 2.1764\n",
      "Epoch 116/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8705 - val_loss: 1.9513\n",
      "Epoch 117/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8708 - val_loss: 1.9474\n",
      "Epoch 118/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8032 - val_loss: 1.9409\n",
      "Epoch 119/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.7994 - val_loss: 2.0851\n",
      "Epoch 120/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8130 - val_loss: 1.9688\n",
      "Epoch 121/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8690 - val_loss: 1.9484\n",
      "Epoch 122/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8398 - val_loss: 2.3261\n",
      "Epoch 123/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8540 - val_loss: 1.8997\n",
      "Epoch 124/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8044 - val_loss: 1.9832\n",
      "Epoch 125/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.9154 - val_loss: 2.0042\n",
      "Epoch 126/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9440 - val_loss: 1.9797\n",
      "Epoch 127/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8366 - val_loss: 2.0796\n",
      "Epoch 128/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8970 - val_loss: 2.1050\n",
      "Epoch 129/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8238 - val_loss: 1.9196\n",
      "Epoch 130/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8239 - val_loss: 1.9589\n",
      "Epoch 131/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.7992 - val_loss: 1.9633\n",
      "Epoch 132/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8318 - val_loss: 1.9724\n",
      "Epoch 133/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8144 - val_loss: 1.9407\n",
      "Epoch 134/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8374 - val_loss: 1.9654\n",
      "Epoch 135/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.7885 - val_loss: 1.9284\n",
      "Epoch 136/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.7782 - val_loss: 2.0802\n",
      "Epoch 137/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8461 - val_loss: 2.0014\n",
      "Epoch 138/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8344 - val_loss: 1.9140\n",
      "Epoch 139/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8136 - val_loss: 1.9595\n",
      "Epoch 140/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8537 - val_loss: 1.9996\n",
      "Epoch 141/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8386 - val_loss: 1.9381\n",
      "Epoch 142/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8084 - val_loss: 2.0028\n",
      "Epoch 143/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8591 - val_loss: 1.9578\n",
      "Epoch 144/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8991 - val_loss: 1.9855\n",
      "Epoch 145/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8174 - val_loss: 2.0028\n",
      "Epoch 146/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.7803 - val_loss: 1.9722\n",
      "Epoch 147/200\n",
      "1960/1960 [==============================] - 0s 18us/step - loss: 1.8488 - val_loss: 2.0912\n",
      "Epoch 148/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.7844 - val_loss: 1.9142\n",
      "Epoch 149/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.7814 - val_loss: 1.9744\n",
      "Epoch 150/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8946 - val_loss: 1.9612\n",
      "Epoch 151/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8756 - val_loss: 1.9810\n",
      "Epoch 152/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9265 - val_loss: 2.1801\n",
      "Epoch 153/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8203 - val_loss: 1.9245\n",
      "Epoch 154/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8432 - val_loss: 2.0512\n",
      "Epoch 155/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.9556 - val_loss: 2.0039\n",
      "Epoch 156/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8607 - val_loss: 1.9999\n",
      "Epoch 157/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8745 - val_loss: 2.1772\n",
      "Epoch 158/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8461 - val_loss: 1.9488\n",
      "Epoch 159/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8411 - val_loss: 1.9736\n",
      "Epoch 160/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8360 - val_loss: 2.1595\n",
      "Epoch 161/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8236 - val_loss: 1.9615\n",
      "Epoch 162/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.7946 - val_loss: 1.9574\n",
      "Epoch 163/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8146 - val_loss: 2.1387\n",
      "Epoch 164/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8240 - val_loss: 1.9547\n",
      "Epoch 165/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8397 - val_loss: 2.0645\n",
      "Epoch 166/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.7927 - val_loss: 1.8969\n",
      "Epoch 167/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8288 - val_loss: 1.9845\n",
      "Epoch 168/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8249 - val_loss: 2.0726\n",
      "Epoch 169/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8093 - val_loss: 1.9282\n",
      "Epoch 170/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8202 - val_loss: 1.9857\n",
      "Epoch 171/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.7841 - val_loss: 1.9169\n",
      "Epoch 172/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.7707 - val_loss: 2.0174\n",
      "Epoch 173/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.7896 - val_loss: 1.9284\n",
      "Epoch 174/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8101 - val_loss: 2.0005\n",
      "Epoch 175/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.7889 - val_loss: 1.9573\n",
      "Epoch 176/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.7996 - val_loss: 2.0180\n",
      "Epoch 177/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.7994 - val_loss: 1.9483\n",
      "Epoch 178/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.7985 - val_loss: 1.9087\n",
      "Epoch 179/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8712 - val_loss: 1.9507\n",
      "Epoch 180/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9063 - val_loss: 2.0051\n",
      "Epoch 181/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8608 - val_loss: 2.1030\n",
      "Epoch 182/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8151 - val_loss: 1.9381\n",
      "Epoch 183/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8189 - val_loss: 1.9142\n",
      "Epoch 184/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8176 - val_loss: 1.9741\n",
      "Epoch 185/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8057 - val_loss: 1.9208\n",
      "Epoch 186/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8345 - val_loss: 1.9351\n",
      "Epoch 187/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.7909 - val_loss: 1.9801\n",
      "Epoch 188/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8227 - val_loss: 2.0540\n",
      "Epoch 189/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8237 - val_loss: 1.9533\n",
      "Epoch 190/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8276 - val_loss: 2.0579\n",
      "Epoch 191/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8118 - val_loss: 2.0080\n",
      "Epoch 192/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.7983 - val_loss: 1.9298\n",
      "Epoch 193/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8357 - val_loss: 1.9788\n",
      "Epoch 194/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8392 - val_loss: 2.1472\n",
      "Epoch 195/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8365 - val_loss: 1.9605\n",
      "Epoch 196/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8472 - val_loss: 1.9236\n",
      "Epoch 197/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8783 - val_loss: 2.0260\n",
      "Epoch 198/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.7996 - val_loss: 2.0211\n",
      "Epoch 199/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8165 - val_loss: 1.9258\n",
      "Epoch 200/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.7771 - val_loss: 1.9750\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_9 (InputLayer)         (None, 1)                 0         \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 64)                128       \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "pred (Dense)                 (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 2,241\n",
      "Trainable params: 2,241\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 1960 samples, validate on 490 samples\n",
      "Epoch 1/200\n",
      "1960/1960 [==============================] - 0s 211us/step - loss: 26.3482 - val_loss: 13.1712\n",
      "Epoch 2/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 9.1749 - val_loss: 7.8102\n",
      "Epoch 3/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 6.9176 - val_loss: 6.7162\n",
      "Epoch 4/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 5.4288 - val_loss: 5.2792\n",
      "Epoch 5/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 4.4344 - val_loss: 4.1422\n",
      "Epoch 6/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 3.4711 - val_loss: 3.1509\n",
      "Epoch 7/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 2.8093 - val_loss: 2.6534\n",
      "Epoch 8/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 2.3197 - val_loss: 2.1940\n",
      "Epoch 9/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 2.0687 - val_loss: 2.2657\n",
      "Epoch 10/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9871 - val_loss: 2.0443\n",
      "Epoch 11/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9679 - val_loss: 2.0327\n",
      "Epoch 12/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 2.1101 - val_loss: 2.3525\n",
      "Epoch 13/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 2.1282 - val_loss: 2.0121\n",
      "Epoch 14/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9442 - val_loss: 1.9570\n",
      "Epoch 15/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 2.0665 - val_loss: 2.2660\n",
      "Epoch 16/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 2.0028 - val_loss: 1.9343\n",
      "Epoch 17/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.9137 - val_loss: 1.8917\n",
      "Epoch 18/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9344 - val_loss: 2.1266\n",
      "Epoch 19/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 2.1018 - val_loss: 2.1160\n",
      "Epoch 20/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9709 - val_loss: 2.0517\n",
      "Epoch 21/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.9108 - val_loss: 1.8866\n",
      "Epoch 22/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8770 - val_loss: 2.0810\n",
      "Epoch 23/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9690 - val_loss: 2.0166\n",
      "Epoch 24/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 2.0398 - val_loss: 1.9341\n",
      "Epoch 25/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9937 - val_loss: 1.8655\n",
      "Epoch 26/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8626 - val_loss: 1.8813\n",
      "Epoch 27/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.7955 - val_loss: 1.8432\n",
      "Epoch 28/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8163 - val_loss: 1.8433\n",
      "Epoch 29/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8618 - val_loss: 1.8949\n",
      "Epoch 30/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9062 - val_loss: 1.9304\n",
      "Epoch 31/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9302 - val_loss: 1.8570\n",
      "Epoch 32/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8002 - val_loss: 1.9458\n",
      "Epoch 33/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.9169 - val_loss: 2.0372\n",
      "Epoch 34/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8444 - val_loss: 1.8566\n",
      "Epoch 35/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8856 - val_loss: 1.9480\n",
      "Epoch 36/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9111 - val_loss: 1.9332\n",
      "Epoch 37/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8721 - val_loss: 1.9634\n",
      "Epoch 38/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8846 - val_loss: 1.9905\n",
      "Epoch 39/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.7984 - val_loss: 1.8266\n",
      "Epoch 40/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8196 - val_loss: 1.9481\n",
      "Epoch 41/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8195 - val_loss: 1.8777\n",
      "Epoch 42/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8439 - val_loss: 1.8831\n",
      "Epoch 43/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8565 - val_loss: 1.8318\n",
      "Epoch 44/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8287 - val_loss: 1.8926\n",
      "Epoch 45/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8041 - val_loss: 1.9322\n",
      "Epoch 46/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8840 - val_loss: 1.8912\n",
      "Epoch 47/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8543 - val_loss: 1.9777\n",
      "Epoch 48/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8581 - val_loss: 1.9119\n",
      "Epoch 49/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8170 - val_loss: 1.8425\n",
      "Epoch 50/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8400 - val_loss: 1.9151\n",
      "Epoch 51/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9046 - val_loss: 1.8930\n",
      "Epoch 52/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8838 - val_loss: 2.0034\n",
      "Epoch 53/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.9365 - val_loss: 2.0792\n",
      "Epoch 54/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.9374 - val_loss: 1.8905\n",
      "Epoch 55/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8897 - val_loss: 1.8842\n",
      "Epoch 56/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 2.1148 - val_loss: 1.9161\n",
      "Epoch 57/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9593 - val_loss: 2.2966\n",
      "Epoch 58/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8794 - val_loss: 1.8687\n",
      "Epoch 59/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8607 - val_loss: 1.9248\n",
      "Epoch 60/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.9319 - val_loss: 2.0128\n",
      "Epoch 61/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8771 - val_loss: 2.1113\n",
      "Epoch 62/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9233 - val_loss: 1.9554\n",
      "Epoch 63/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8805 - val_loss: 2.0395\n",
      "Epoch 64/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9016 - val_loss: 2.0684\n",
      "Epoch 65/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8958 - val_loss: 1.9277\n",
      "Epoch 66/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9631 - val_loss: 1.9286\n",
      "Epoch 67/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8980 - val_loss: 2.0853\n",
      "Epoch 68/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8717 - val_loss: 1.9791\n",
      "Epoch 69/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9150 - val_loss: 2.0311\n",
      "Epoch 70/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.7998 - val_loss: 1.9833\n",
      "Epoch 71/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 2.0373 - val_loss: 2.0615\n",
      "Epoch 72/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 2.1128 - val_loss: 2.0829\n",
      "Epoch 73/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8767 - val_loss: 1.9143\n",
      "Epoch 74/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8143 - val_loss: 1.8751\n",
      "Epoch 75/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8172 - val_loss: 1.8565\n",
      "Epoch 76/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8487 - val_loss: 1.9445\n",
      "Epoch 77/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8778 - val_loss: 1.9488\n",
      "Epoch 78/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9083 - val_loss: 2.0766\n",
      "Epoch 79/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9364 - val_loss: 2.0211\n",
      "Epoch 80/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.9342 - val_loss: 1.8987\n",
      "Epoch 81/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.9295 - val_loss: 1.9346\n",
      "Epoch 82/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9117 - val_loss: 1.9659\n",
      "Epoch 83/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8341 - val_loss: 1.8979\n",
      "Epoch 84/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8284 - val_loss: 1.8801\n",
      "Epoch 85/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8334 - val_loss: 2.0723\n",
      "Epoch 86/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8453 - val_loss: 1.9239\n",
      "Epoch 87/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8795 - val_loss: 1.8649\n",
      "Epoch 88/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9953 - val_loss: 1.9613\n",
      "Epoch 89/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9293 - val_loss: 1.9654\n",
      "Epoch 90/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8440 - val_loss: 1.9724\n",
      "Epoch 91/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8923 - val_loss: 1.8669\n",
      "Epoch 92/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8884 - val_loss: 1.8619\n",
      "Epoch 93/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9113 - val_loss: 1.9193\n",
      "Epoch 94/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9183 - val_loss: 1.9141\n",
      "Epoch 95/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.9918 - val_loss: 2.0087\n",
      "Epoch 96/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9233 - val_loss: 1.8637\n",
      "Epoch 97/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8854 - val_loss: 1.8445\n",
      "Epoch 98/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8871 - val_loss: 1.8812\n",
      "Epoch 99/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8775 - val_loss: 1.9104\n",
      "Epoch 100/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8136 - val_loss: 1.8650\n",
      "Epoch 101/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8567 - val_loss: 1.8565\n",
      "Epoch 102/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8311 - val_loss: 1.8509\n",
      "Epoch 103/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8491 - val_loss: 1.9017\n",
      "Epoch 104/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8196 - val_loss: 1.8688\n",
      "Epoch 105/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.7844 - val_loss: 1.8671\n",
      "Epoch 106/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8302 - val_loss: 2.0431\n",
      "Epoch 107/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9458 - val_loss: 1.8929\n",
      "Epoch 108/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9020 - val_loss: 1.8966\n",
      "Epoch 109/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8830 - val_loss: 1.9743\n",
      "Epoch 110/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9501 - val_loss: 1.9272\n",
      "Epoch 111/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8798 - val_loss: 2.3100\n",
      "Epoch 112/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9983 - val_loss: 2.0235\n",
      "Epoch 113/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8935 - val_loss: 1.9983\n",
      "Epoch 114/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9196 - val_loss: 1.9137\n",
      "Epoch 115/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8694 - val_loss: 1.9312\n",
      "Epoch 116/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8192 - val_loss: 1.8850\n",
      "Epoch 117/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8467 - val_loss: 1.8982\n",
      "Epoch 118/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8014 - val_loss: 1.8647\n",
      "Epoch 119/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.7976 - val_loss: 1.9648\n",
      "Epoch 120/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8384 - val_loss: 1.8534\n",
      "Epoch 121/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8346 - val_loss: 1.8570\n",
      "Epoch 122/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.9101 - val_loss: 1.9272\n",
      "Epoch 123/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9194 - val_loss: 2.0808\n",
      "Epoch 124/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9225 - val_loss: 1.8895\n",
      "Epoch 125/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8776 - val_loss: 1.8844\n",
      "Epoch 126/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8803 - val_loss: 1.9580\n",
      "Epoch 127/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8954 - val_loss: 1.8931\n",
      "Epoch 128/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8512 - val_loss: 1.8778\n",
      "Epoch 129/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9531 - val_loss: 1.8653\n",
      "Epoch 130/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9763 - val_loss: 1.8900\n",
      "Epoch 131/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8692 - val_loss: 1.8822\n",
      "Epoch 132/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8570 - val_loss: 1.9378\n",
      "Epoch 133/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8206 - val_loss: 1.8904\n",
      "Epoch 134/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8249 - val_loss: 2.0430\n",
      "Epoch 135/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8708 - val_loss: 1.9961\n",
      "Epoch 136/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8747 - val_loss: 1.8540\n",
      "Epoch 137/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8384 - val_loss: 1.9007\n",
      "Epoch 138/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8742 - val_loss: 1.9170\n",
      "Epoch 139/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9214 - val_loss: 1.9565\n",
      "Epoch 140/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9107 - val_loss: 1.8557\n",
      "Epoch 141/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8669 - val_loss: 2.1785\n",
      "Epoch 142/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.9763 - val_loss: 2.0146\n",
      "Epoch 143/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.9687 - val_loss: 1.9405\n",
      "Epoch 144/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8904 - val_loss: 1.9717\n",
      "Epoch 145/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9126 - val_loss: 1.9706\n",
      "Epoch 146/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8719 - val_loss: 1.9725\n",
      "Epoch 147/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.9168 - val_loss: 2.0938\n",
      "Epoch 148/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8986 - val_loss: 1.9569\n",
      "Epoch 149/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8657 - val_loss: 1.9143\n",
      "Epoch 150/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8152 - val_loss: 1.8845\n",
      "Epoch 151/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8186 - val_loss: 1.8613\n",
      "Epoch 152/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8328 - val_loss: 1.9088\n",
      "Epoch 153/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8204 - val_loss: 1.9090\n",
      "Epoch 154/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8086 - val_loss: 1.8534\n",
      "Epoch 155/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.7994 - val_loss: 1.9000\n",
      "Epoch 156/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.7993 - val_loss: 1.9084\n",
      "Epoch 157/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8244 - val_loss: 1.9383\n",
      "Epoch 158/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8722 - val_loss: 2.1884\n",
      "Epoch 159/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 2.0080 - val_loss: 1.8846\n",
      "Epoch 160/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8456 - val_loss: 1.8887\n",
      "Epoch 161/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8972 - val_loss: 1.8730\n",
      "Epoch 162/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8444 - val_loss: 1.8933\n",
      "Epoch 163/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8559 - val_loss: 1.8630\n",
      "Epoch 164/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8434 - val_loss: 1.8849\n",
      "Epoch 165/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8509 - val_loss: 1.9284\n",
      "Epoch 166/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.7949 - val_loss: 1.8707\n",
      "Epoch 167/200\n",
      "1960/1960 [==============================] - 0s 18us/step - loss: 1.8070 - val_loss: 1.9235\n",
      "Epoch 168/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8131 - val_loss: 1.8599\n",
      "Epoch 169/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8075 - val_loss: 1.9080\n",
      "Epoch 170/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8807 - val_loss: 1.9967\n",
      "Epoch 171/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8446 - val_loss: 1.9450\n",
      "Epoch 172/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8230 - val_loss: 1.8755\n",
      "Epoch 173/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8406 - val_loss: 1.8834\n",
      "Epoch 174/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8197 - val_loss: 2.0374\n",
      "Epoch 175/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8727 - val_loss: 1.8849\n",
      "Epoch 176/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8867 - val_loss: 1.9166\n",
      "Epoch 177/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9246 - val_loss: 2.2335\n",
      "Epoch 178/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 2.0015 - val_loss: 1.8600\n",
      "Epoch 179/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9006 - val_loss: 1.9510\n",
      "Epoch 180/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8503 - val_loss: 1.9460\n",
      "Epoch 181/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8150 - val_loss: 1.9060\n",
      "Epoch 182/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8219 - val_loss: 2.0784\n",
      "Epoch 183/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9031 - val_loss: 1.9481\n",
      "Epoch 184/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9127 - val_loss: 1.9874\n",
      "Epoch 185/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8878 - val_loss: 2.0554\n",
      "Epoch 186/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8458 - val_loss: 1.8660\n",
      "Epoch 187/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8288 - val_loss: 1.8636\n",
      "Epoch 188/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9688 - val_loss: 2.0786\n",
      "Epoch 189/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9312 - val_loss: 2.0788\n",
      "Epoch 190/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8324 - val_loss: 1.8963\n",
      "Epoch 191/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8377 - val_loss: 1.9198\n",
      "Epoch 192/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8518 - val_loss: 1.9118\n",
      "Epoch 193/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8569 - val_loss: 1.8652\n",
      "Epoch 194/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8083 - val_loss: 1.9133\n",
      "Epoch 195/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8677 - val_loss: 1.9904\n",
      "Epoch 196/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9071 - val_loss: 2.0464\n",
      "Epoch 197/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8729 - val_loss: 2.0059\n",
      "Epoch 198/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9374 - val_loss: 1.8923\n",
      "Epoch 199/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8287 - val_loss: 2.0273\n",
      "Epoch 200/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8707 - val_loss: 1.9305\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_10 (InputLayer)        (None, 1)                 0         \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 64)                128       \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "pred (Dense)                 (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 2,241\n",
      "Trainable params: 2,241\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 1960 samples, validate on 490 samples\n",
      "Epoch 1/200\n",
      "1960/1960 [==============================] - 0s 224us/step - loss: 33.4026 - val_loss: 9.7248\n",
      "Epoch 2/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 9.8291 - val_loss: 8.7588\n",
      "Epoch 3/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 6.7244 - val_loss: 7.8239\n",
      "Epoch 4/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 5.7707 - val_loss: 6.1264\n",
      "Epoch 5/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 5.0317 - val_loss: 5.3761\n",
      "Epoch 6/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 4.0940 - val_loss: 4.0294\n",
      "Epoch 7/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 3.3459 - val_loss: 3.6960\n",
      "Epoch 8/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 2.8887 - val_loss: 2.6579\n",
      "Epoch 9/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 2.6150 - val_loss: 2.3896\n",
      "Epoch 10/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 2.3723 - val_loss: 2.3107\n",
      "Epoch 11/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 2.2027 - val_loss: 2.0197\n",
      "Epoch 12/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 2.1565 - val_loss: 2.0029\n",
      "Epoch 13/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 2.1128 - val_loss: 2.4477\n",
      "Epoch 14/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 2.2800 - val_loss: 2.1828\n",
      "Epoch 15/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 2.0919 - val_loss: 2.0991\n",
      "Epoch 16/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9566 - val_loss: 1.8830\n",
      "Epoch 17/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9209 - val_loss: 1.9363\n",
      "Epoch 18/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9500 - val_loss: 1.8721\n",
      "Epoch 19/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9222 - val_loss: 2.1352\n",
      "Epoch 20/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 2.0498 - val_loss: 2.0652\n",
      "Epoch 21/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9418 - val_loss: 1.9088\n",
      "Epoch 22/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8764 - val_loss: 1.9886\n",
      "Epoch 23/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8633 - val_loss: 1.8905\n",
      "Epoch 24/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9351 - val_loss: 2.1348\n",
      "Epoch 25/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9640 - val_loss: 2.0485\n",
      "Epoch 26/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8997 - val_loss: 2.0196\n",
      "Epoch 27/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9824 - val_loss: 1.9272\n",
      "Epoch 28/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8820 - val_loss: 1.9058\n",
      "Epoch 29/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9160 - val_loss: 2.3180\n",
      "Epoch 30/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9345 - val_loss: 1.9672\n",
      "Epoch 31/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8529 - val_loss: 1.9181\n",
      "Epoch 32/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8126 - val_loss: 1.9774\n",
      "Epoch 33/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8471 - val_loss: 2.0510\n",
      "Epoch 34/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8588 - val_loss: 2.0196\n",
      "Epoch 35/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8712 - val_loss: 1.9873\n",
      "Epoch 36/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8423 - val_loss: 2.0288\n",
      "Epoch 37/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8549 - val_loss: 1.9787\n",
      "Epoch 38/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8308 - val_loss: 1.9817\n",
      "Epoch 39/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8574 - val_loss: 1.9141\n",
      "Epoch 40/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8054 - val_loss: 1.8979\n",
      "Epoch 41/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8433 - val_loss: 2.0215\n",
      "Epoch 42/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9005 - val_loss: 2.0160\n",
      "Epoch 43/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8837 - val_loss: 1.9318\n",
      "Epoch 44/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8394 - val_loss: 1.9095\n",
      "Epoch 45/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8359 - val_loss: 1.9317\n",
      "Epoch 46/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8292 - val_loss: 1.9446\n",
      "Epoch 47/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8221 - val_loss: 1.9084\n",
      "Epoch 48/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8946 - val_loss: 2.0051\n",
      "Epoch 49/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8020 - val_loss: 2.0155\n",
      "Epoch 50/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8806 - val_loss: 2.1228\n",
      "Epoch 51/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9326 - val_loss: 2.0338\n",
      "Epoch 52/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8578 - val_loss: 1.9417\n",
      "Epoch 53/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9366 - val_loss: 1.9683\n",
      "Epoch 54/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8478 - val_loss: 1.9365\n",
      "Epoch 55/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8054 - val_loss: 1.9831\n",
      "Epoch 56/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8000 - val_loss: 1.9218\n",
      "Epoch 57/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8507 - val_loss: 1.9749\n",
      "Epoch 58/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8652 - val_loss: 2.0283\n",
      "Epoch 59/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8515 - val_loss: 2.1254\n",
      "Epoch 60/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8635 - val_loss: 1.9029\n",
      "Epoch 61/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8314 - val_loss: 1.9512\n",
      "Epoch 62/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8095 - val_loss: 1.9267\n",
      "Epoch 63/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8064 - val_loss: 1.9336\n",
      "Epoch 64/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8333 - val_loss: 1.9898\n",
      "Epoch 65/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8279 - val_loss: 2.0244\n",
      "Epoch 66/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9035 - val_loss: 1.9550\n",
      "Epoch 67/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8308 - val_loss: 2.0202\n",
      "Epoch 68/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8310 - val_loss: 1.9763\n",
      "Epoch 69/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8149 - val_loss: 1.9991\n",
      "Epoch 70/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8428 - val_loss: 1.9345\n",
      "Epoch 71/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8575 - val_loss: 2.0959\n",
      "Epoch 72/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8973 - val_loss: 1.9317\n",
      "Epoch 73/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8036 - val_loss: 1.9043\n",
      "Epoch 74/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.7803 - val_loss: 1.9045\n",
      "Epoch 75/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8384 - val_loss: 1.9476\n",
      "Epoch 76/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.9337 - val_loss: 2.0042\n",
      "Epoch 77/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8683 - val_loss: 1.8920\n",
      "Epoch 78/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.7925 - val_loss: 1.8972\n",
      "Epoch 79/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8087 - val_loss: 1.8938\n",
      "Epoch 80/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8255 - val_loss: 2.3654\n",
      "Epoch 81/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9498 - val_loss: 1.9749\n",
      "Epoch 82/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8692 - val_loss: 1.8896\n",
      "Epoch 83/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8111 - val_loss: 1.9048\n",
      "Epoch 84/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8142 - val_loss: 2.1324\n",
      "Epoch 85/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9005 - val_loss: 1.9009\n",
      "Epoch 86/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8304 - val_loss: 1.9123\n",
      "Epoch 87/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8150 - val_loss: 1.9349\n",
      "Epoch 88/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8027 - val_loss: 2.0276\n",
      "Epoch 89/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8724 - val_loss: 2.0899\n",
      "Epoch 90/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8694 - val_loss: 1.9333\n",
      "Epoch 91/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8396 - val_loss: 1.9787\n",
      "Epoch 92/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8489 - val_loss: 1.9878\n",
      "Epoch 93/200\n",
      "1960/1960 [==============================] - 0s 18us/step - loss: 1.8464 - val_loss: 1.9065\n",
      "Epoch 94/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8270 - val_loss: 1.9341\n",
      "Epoch 95/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.7979 - val_loss: 1.9559\n",
      "Epoch 96/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8358 - val_loss: 1.9931\n",
      "Epoch 97/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8303 - val_loss: 1.9045\n",
      "Epoch 98/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8006 - val_loss: 1.9489\n",
      "Epoch 99/200\n",
      "1960/1960 [==============================] - 0s 18us/step - loss: 1.7962 - val_loss: 1.9047\n",
      "Epoch 100/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.7805 - val_loss: 1.9632\n",
      "Epoch 101/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8611 - val_loss: 1.9491\n",
      "Epoch 102/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8714 - val_loss: 1.9578\n",
      "Epoch 103/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8603 - val_loss: 2.0377\n",
      "Epoch 104/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.9623 - val_loss: 2.0898\n",
      "Epoch 105/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9953 - val_loss: 2.0124\n",
      "Epoch 106/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.9228 - val_loss: 1.8929\n",
      "Epoch 107/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9070 - val_loss: 2.0713\n",
      "Epoch 108/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8725 - val_loss: 1.9677\n",
      "Epoch 109/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8279 - val_loss: 2.0172\n",
      "Epoch 110/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8394 - val_loss: 1.9031\n",
      "Epoch 111/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8200 - val_loss: 1.9745\n",
      "Epoch 112/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8404 - val_loss: 2.0008\n",
      "Epoch 113/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8731 - val_loss: 2.0033\n",
      "Epoch 114/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9001 - val_loss: 2.0498\n",
      "Epoch 115/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8419 - val_loss: 1.9038\n",
      "Epoch 116/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8085 - val_loss: 1.9281\n",
      "Epoch 117/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8039 - val_loss: 1.9569\n",
      "Epoch 118/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8502 - val_loss: 2.0095\n",
      "Epoch 119/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8061 - val_loss: 1.8899\n",
      "Epoch 120/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8430 - val_loss: 1.9163\n",
      "Epoch 121/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9672 - val_loss: 2.1950\n",
      "Epoch 122/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9628 - val_loss: 1.9970\n",
      "Epoch 123/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8305 - val_loss: 1.9133\n",
      "Epoch 124/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8178 - val_loss: 1.9707\n",
      "Epoch 125/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8382 - val_loss: 2.0024\n",
      "Epoch 126/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9325 - val_loss: 2.0178\n",
      "Epoch 127/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.9462 - val_loss: 1.9926\n",
      "Epoch 128/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8329 - val_loss: 1.9353\n",
      "Epoch 129/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8296 - val_loss: 1.9530\n",
      "Epoch 130/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.7768 - val_loss: 1.9026\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 131/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.7986 - val_loss: 1.9485\n",
      "Epoch 132/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8669 - val_loss: 1.9223\n",
      "Epoch 133/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8636 - val_loss: 2.0580\n",
      "Epoch 134/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9220 - val_loss: 2.0078\n",
      "Epoch 135/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.7972 - val_loss: 1.9522\n",
      "Epoch 136/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.7891 - val_loss: 1.9719\n",
      "Epoch 137/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8283 - val_loss: 1.9883\n",
      "Epoch 138/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8628 - val_loss: 2.0842\n",
      "Epoch 139/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8507 - val_loss: 1.9463\n",
      "Epoch 140/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8708 - val_loss: 2.0332\n",
      "Epoch 141/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8975 - val_loss: 1.9997\n",
      "Epoch 142/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8482 - val_loss: 1.9753\n",
      "Epoch 143/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9044 - val_loss: 2.2037\n",
      "Epoch 144/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9528 - val_loss: 1.9053\n",
      "Epoch 145/200\n",
      "1960/1960 [==============================] - 0s 18us/step - loss: 1.8050 - val_loss: 1.9834\n",
      "Epoch 146/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8291 - val_loss: 1.9176\n",
      "Epoch 147/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8542 - val_loss: 1.9765\n",
      "Epoch 148/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8105 - val_loss: 2.0350\n",
      "Epoch 149/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9128 - val_loss: 2.0688\n",
      "Epoch 150/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8433 - val_loss: 1.9909\n",
      "Epoch 151/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8656 - val_loss: 1.9349\n",
      "Epoch 152/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.7922 - val_loss: 1.9994\n",
      "Epoch 153/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8787 - val_loss: 2.0097\n",
      "Epoch 154/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8938 - val_loss: 2.1887\n",
      "Epoch 155/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9243 - val_loss: 2.0417\n",
      "Epoch 156/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8361 - val_loss: 1.9179\n",
      "Epoch 157/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.7978 - val_loss: 1.9532\n",
      "Epoch 158/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8429 - val_loss: 2.0048\n",
      "Epoch 159/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9176 - val_loss: 2.0430\n",
      "Epoch 160/200\n",
      "1960/1960 [==============================] - 0s 18us/step - loss: 1.8244 - val_loss: 1.9404\n",
      "Epoch 161/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8173 - val_loss: 1.9353\n",
      "Epoch 162/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8087 - val_loss: 2.0378\n",
      "Epoch 163/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8301 - val_loss: 1.9031\n",
      "Epoch 164/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.7873 - val_loss: 1.9533\n",
      "Epoch 165/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8238 - val_loss: 1.9868\n",
      "Epoch 166/200\n",
      "1960/1960 [==============================] - 0s 19us/step - loss: 1.8728 - val_loss: 1.9266\n",
      "Epoch 167/200\n",
      "1960/1960 [==============================] - 0s 18us/step - loss: 1.7929 - val_loss: 1.9700\n",
      "Epoch 168/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8780 - val_loss: 1.9590\n",
      "Epoch 169/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8942 - val_loss: 2.0671\n",
      "Epoch 170/200\n",
      "1960/1960 [==============================] - 0s 20us/step - loss: 1.8404 - val_loss: 1.9329\n",
      "Epoch 171/200\n",
      "1960/1960 [==============================] - 0s 20us/step - loss: 1.8371 - val_loss: 2.1571\n",
      "Epoch 172/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8736 - val_loss: 1.9838\n",
      "Epoch 173/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8216 - val_loss: 1.9335\n",
      "Epoch 174/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8363 - val_loss: 1.9936\n",
      "Epoch 175/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8760 - val_loss: 1.9106\n",
      "Epoch 176/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.7923 - val_loss: 2.0032\n",
      "Epoch 177/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8526 - val_loss: 1.9358\n",
      "Epoch 178/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8020 - val_loss: 1.9252\n",
      "Epoch 179/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.7844 - val_loss: 1.9624\n",
      "Epoch 180/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.7892 - val_loss: 1.9063\n",
      "Epoch 181/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8124 - val_loss: 2.0289\n",
      "Epoch 182/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8136 - val_loss: 1.9647\n",
      "Epoch 183/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8537 - val_loss: 1.9639\n",
      "Epoch 184/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8415 - val_loss: 1.9439\n",
      "Epoch 185/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8700 - val_loss: 1.9533\n",
      "Epoch 186/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8806 - val_loss: 2.0712\n",
      "Epoch 187/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9362 - val_loss: 2.0421\n",
      "Epoch 188/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8767 - val_loss: 2.0625\n",
      "Epoch 189/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8229 - val_loss: 1.9137\n",
      "Epoch 190/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.7872 - val_loss: 2.0177\n",
      "Epoch 191/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8681 - val_loss: 2.0411\n",
      "Epoch 192/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9478 - val_loss: 1.9388\n",
      "Epoch 193/200\n",
      "1960/1960 [==============================] - 0s 18us/step - loss: 1.8972 - val_loss: 1.9779\n",
      "Epoch 194/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8483 - val_loss: 1.9695\n",
      "Epoch 195/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8460 - val_loss: 1.9255\n",
      "Epoch 196/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8128 - val_loss: 1.9265\n",
      "Epoch 197/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.7914 - val_loss: 1.9221\n",
      "Epoch 198/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8485 - val_loss: 1.9724\n",
      "Epoch 199/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8536 - val_loss: 1.9189\n",
      "Epoch 200/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.7994 - val_loss: 1.9279\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_11 (InputLayer)        (None, 1)                 0         \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 64)                128       \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "pred (Dense)                 (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 2,241\n",
      "Trainable params: 2,241\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1960 samples, validate on 490 samples\n",
      "Epoch 1/200\n",
      "1960/1960 [==============================] - 0s 233us/step - loss: 37.1520 - val_loss: 8.5623\n",
      "Epoch 2/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 9.7447 - val_loss: 9.2756\n",
      "Epoch 3/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 7.0915 - val_loss: 7.8820\n",
      "Epoch 4/200\n",
      "1960/1960 [==============================] - 0s 18us/step - loss: 5.9930 - val_loss: 6.7705\n",
      "Epoch 5/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 5.0861 - val_loss: 5.7521\n",
      "Epoch 6/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 4.2086 - val_loss: 4.6275\n",
      "Epoch 7/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 3.4683 - val_loss: 3.6605\n",
      "Epoch 8/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 2.8723 - val_loss: 3.2002\n",
      "Epoch 9/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 2.4599 - val_loss: 2.6193\n",
      "Epoch 10/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 2.1375 - val_loss: 2.2526\n",
      "Epoch 11/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9943 - val_loss: 2.1129\n",
      "Epoch 12/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9541 - val_loss: 2.0674\n",
      "Epoch 13/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9278 - val_loss: 2.1133\n",
      "Epoch 14/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9788 - val_loss: 2.3972\n",
      "Epoch 15/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 2.3759 - val_loss: 2.4237\n",
      "Epoch 16/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 2.0318 - val_loss: 2.0418\n",
      "Epoch 17/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.9055 - val_loss: 1.9902\n",
      "Epoch 18/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8518 - val_loss: 1.9748\n",
      "Epoch 19/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8710 - val_loss: 1.9906\n",
      "Epoch 20/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9089 - val_loss: 1.9751\n",
      "Epoch 21/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9571 - val_loss: 2.1866\n",
      "Epoch 22/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9635 - val_loss: 2.0197\n",
      "Epoch 23/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8787 - val_loss: 2.0281\n",
      "Epoch 24/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8877 - val_loss: 1.9775\n",
      "Epoch 25/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8763 - val_loss: 1.9568\n",
      "Epoch 26/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8508 - val_loss: 1.9746\n",
      "Epoch 27/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8254 - val_loss: 1.9826\n",
      "Epoch 28/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8890 - val_loss: 2.1455\n",
      "Epoch 29/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9198 - val_loss: 2.0470\n",
      "Epoch 30/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 2.0932 - val_loss: 2.0910\n",
      "Epoch 31/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 2.1081 - val_loss: 2.6328\n",
      "Epoch 32/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 2.1673 - val_loss: 2.2149\n",
      "Epoch 33/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 2.0089 - val_loss: 2.1022\n",
      "Epoch 34/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8728 - val_loss: 1.9245\n",
      "Epoch 35/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8372 - val_loss: 2.0393\n",
      "Epoch 36/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8325 - val_loss: 1.9515\n",
      "Epoch 37/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8183 - val_loss: 2.0016\n",
      "Epoch 38/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8459 - val_loss: 1.9887\n",
      "Epoch 39/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8709 - val_loss: 1.9633\n",
      "Epoch 40/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9385 - val_loss: 1.9333\n",
      "Epoch 41/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9168 - val_loss: 1.9254\n",
      "Epoch 42/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9644 - val_loss: 2.0307\n",
      "Epoch 43/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8937 - val_loss: 1.9767\n",
      "Epoch 44/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8757 - val_loss: 2.1646\n",
      "Epoch 45/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8861 - val_loss: 1.9070\n",
      "Epoch 46/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8592 - val_loss: 2.0707\n",
      "Epoch 47/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9330 - val_loss: 2.2293\n",
      "Epoch 48/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9866 - val_loss: 1.9334\n",
      "Epoch 49/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9721 - val_loss: 2.0002\n",
      "Epoch 50/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8376 - val_loss: 1.9836\n",
      "Epoch 51/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8341 - val_loss: 1.9873\n",
      "Epoch 52/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8978 - val_loss: 1.9485\n",
      "Epoch 53/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8484 - val_loss: 1.9513\n",
      "Epoch 54/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8557 - val_loss: 1.9449\n",
      "Epoch 55/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8413 - val_loss: 2.0359\n",
      "Epoch 56/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8195 - val_loss: 1.8900\n",
      "Epoch 57/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8108 - val_loss: 1.9556\n",
      "Epoch 58/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8280 - val_loss: 1.9025\n",
      "Epoch 59/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8187 - val_loss: 1.9834\n",
      "Epoch 60/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.7743 - val_loss: 1.8971\n",
      "Epoch 61/200\n",
      "1960/1960 [==============================] - 0s 18us/step - loss: 1.8315 - val_loss: 1.9523\n",
      "Epoch 62/200\n",
      "1960/1960 [==============================] - 0s 18us/step - loss: 1.8306 - val_loss: 2.2535\n",
      "Epoch 63/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9888 - val_loss: 2.0523\n",
      "Epoch 64/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8825 - val_loss: 2.0242\n",
      "Epoch 65/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8783 - val_loss: 2.0081\n",
      "Epoch 66/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8773 - val_loss: 1.9909\n",
      "Epoch 67/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8437 - val_loss: 1.9393\n",
      "Epoch 68/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8078 - val_loss: 1.9402\n",
      "Epoch 69/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.7967 - val_loss: 1.9531\n",
      "Epoch 70/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8038 - val_loss: 1.9272\n",
      "Epoch 71/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8463 - val_loss: 1.8932\n",
      "Epoch 72/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8536 - val_loss: 1.9587\n",
      "Epoch 73/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8188 - val_loss: 2.0571\n",
      "Epoch 74/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9179 - val_loss: 2.0894\n",
      "Epoch 75/200\n",
      "1960/1960 [==============================] - 0s 18us/step - loss: 1.9202 - val_loss: 2.0699\n",
      "Epoch 76/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8846 - val_loss: 1.9817\n",
      "Epoch 77/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8174 - val_loss: 1.9055\n",
      "Epoch 78/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9426 - val_loss: 1.9077\n",
      "Epoch 79/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8682 - val_loss: 1.9776\n",
      "Epoch 80/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8553 - val_loss: 2.0254\n",
      "Epoch 81/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8647 - val_loss: 1.9476\n",
      "Epoch 82/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8550 - val_loss: 1.9093\n",
      "Epoch 83/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.7803 - val_loss: 1.9796\n",
      "Epoch 84/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8604 - val_loss: 1.9785\n",
      "Epoch 85/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8767 - val_loss: 1.9472\n",
      "Epoch 86/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8612 - val_loss: 1.8984\n",
      "Epoch 87/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8434 - val_loss: 2.0453\n",
      "Epoch 88/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8652 - val_loss: 1.9078\n",
      "Epoch 89/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8548 - val_loss: 2.1066\n",
      "Epoch 90/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8290 - val_loss: 1.9445\n",
      "Epoch 91/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9390 - val_loss: 2.0248\n",
      "Epoch 92/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8675 - val_loss: 1.9919\n",
      "Epoch 93/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8458 - val_loss: 1.9861\n",
      "Epoch 94/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8784 - val_loss: 1.9539\n",
      "Epoch 95/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8074 - val_loss: 1.8920\n",
      "Epoch 96/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8645 - val_loss: 1.9607\n",
      "Epoch 97/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8868 - val_loss: 2.2007\n",
      "Epoch 98/200\n",
      "1960/1960 [==============================] - 0s 18us/step - loss: 1.8729 - val_loss: 2.0680\n",
      "Epoch 99/200\n",
      "1960/1960 [==============================] - 0s 18us/step - loss: 1.8553 - val_loss: 1.9395\n",
      "Epoch 100/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8181 - val_loss: 1.9160\n",
      "Epoch 101/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8051 - val_loss: 1.9802\n",
      "Epoch 102/200\n",
      "1960/1960 [==============================] - 0s 18us/step - loss: 1.8088 - val_loss: 2.0345\n",
      "Epoch 103/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8699 - val_loss: 2.0331\n",
      "Epoch 104/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9066 - val_loss: 1.9756\n",
      "Epoch 105/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8829 - val_loss: 1.9764\n",
      "Epoch 106/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.9540 - val_loss: 2.0585\n",
      "Epoch 107/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8723 - val_loss: 1.9815\n",
      "Epoch 108/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8734 - val_loss: 2.0237\n",
      "Epoch 109/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8552 - val_loss: 1.9067\n",
      "Epoch 110/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8487 - val_loss: 2.0113\n",
      "Epoch 111/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8347 - val_loss: 1.9566\n",
      "Epoch 112/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8685 - val_loss: 2.1486\n",
      "Epoch 113/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8643 - val_loss: 2.0064\n",
      "Epoch 114/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8415 - val_loss: 1.9804\n",
      "Epoch 115/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8172 - val_loss: 1.9361\n",
      "Epoch 116/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8547 - val_loss: 1.9142\n",
      "Epoch 117/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8159 - val_loss: 1.9648\n",
      "Epoch 118/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8152 - val_loss: 1.9350\n",
      "Epoch 119/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.7883 - val_loss: 1.9542\n",
      "Epoch 120/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8639 - val_loss: 2.0393\n",
      "Epoch 121/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8789 - val_loss: 2.0228\n",
      "Epoch 122/200\n",
      "1960/1960 [==============================] - 0s 18us/step - loss: 1.8619 - val_loss: 1.9625\n",
      "Epoch 123/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8530 - val_loss: 1.9009\n",
      "Epoch 124/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9471 - val_loss: 2.0226\n",
      "Epoch 125/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8488 - val_loss: 1.9541\n",
      "Epoch 126/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8612 - val_loss: 2.0874\n",
      "Epoch 127/200\n",
      "1960/1960 [==============================] - 0s 18us/step - loss: 1.8755 - val_loss: 1.9744\n",
      "Epoch 128/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8196 - val_loss: 2.0093\n",
      "Epoch 129/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8490 - val_loss: 1.8964\n",
      "Epoch 130/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8054 - val_loss: 1.9456\n",
      "Epoch 131/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8049 - val_loss: 1.9366\n",
      "Epoch 132/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8107 - val_loss: 1.9335\n",
      "Epoch 133/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.7839 - val_loss: 1.8886\n",
      "Epoch 134/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.7836 - val_loss: 2.0088\n",
      "Epoch 135/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8460 - val_loss: 1.9612\n",
      "Epoch 136/200\n",
      "1960/1960 [==============================] - 0s 18us/step - loss: 1.8430 - val_loss: 1.8842\n",
      "Epoch 137/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8352 - val_loss: 2.0062\n",
      "Epoch 138/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8542 - val_loss: 1.9300\n",
      "Epoch 139/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8280 - val_loss: 1.9282\n",
      "Epoch 140/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8414 - val_loss: 1.9215\n",
      "Epoch 141/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8548 - val_loss: 1.9648\n",
      "Epoch 142/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8502 - val_loss: 1.9151\n",
      "Epoch 143/200\n",
      "1960/1960 [==============================] - 0s 18us/step - loss: 1.8068 - val_loss: 1.9473\n",
      "Epoch 144/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.7955 - val_loss: 1.9280\n",
      "Epoch 145/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8047 - val_loss: 1.9124\n",
      "Epoch 146/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8019 - val_loss: 2.0194\n",
      "Epoch 147/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8032 - val_loss: 1.8926\n",
      "Epoch 148/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8041 - val_loss: 1.9366\n",
      "Epoch 149/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8497 - val_loss: 2.0501\n",
      "Epoch 150/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8344 - val_loss: 1.9134\n",
      "Epoch 151/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.7935 - val_loss: 1.9366\n",
      "Epoch 152/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8010 - val_loss: 1.8978\n",
      "Epoch 153/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.7928 - val_loss: 2.0400\n",
      "Epoch 154/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8276 - val_loss: 1.9460\n",
      "Epoch 155/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8744 - val_loss: 1.9861\n",
      "Epoch 156/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8318 - val_loss: 2.0113\n",
      "Epoch 157/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8261 - val_loss: 1.9264\n",
      "Epoch 158/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8446 - val_loss: 1.9629\n",
      "Epoch 159/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8391 - val_loss: 2.0212\n",
      "Epoch 160/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8348 - val_loss: 1.9442\n",
      "Epoch 161/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8307 - val_loss: 1.8987\n",
      "Epoch 162/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8195 - val_loss: 1.9402\n",
      "Epoch 163/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8118 - val_loss: 1.9985\n",
      "Epoch 164/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8581 - val_loss: 2.0122\n",
      "Epoch 165/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8223 - val_loss: 2.0260\n",
      "Epoch 166/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8742 - val_loss: 1.9822\n",
      "Epoch 167/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8500 - val_loss: 2.0156\n",
      "Epoch 168/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9018 - val_loss: 1.9718\n",
      "Epoch 169/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8071 - val_loss: 2.1357\n",
      "Epoch 170/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8327 - val_loss: 1.9629\n",
      "Epoch 171/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8554 - val_loss: 1.9683\n",
      "Epoch 172/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8235 - val_loss: 2.0174\n",
      "Epoch 173/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8399 - val_loss: 1.9190\n",
      "Epoch 174/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8975 - val_loss: 1.9852\n",
      "Epoch 175/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.9209 - val_loss: 2.0718\n",
      "Epoch 176/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9464 - val_loss: 2.0742\n",
      "Epoch 177/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8723 - val_loss: 1.9459\n",
      "Epoch 178/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8436 - val_loss: 1.9167\n",
      "Epoch 179/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8397 - val_loss: 1.9612\n",
      "Epoch 180/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.7951 - val_loss: 2.1024\n",
      "Epoch 181/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8929 - val_loss: 2.0535\n",
      "Epoch 182/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8510 - val_loss: 2.0013\n",
      "Epoch 183/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8067 - val_loss: 1.9534\n",
      "Epoch 184/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8789 - val_loss: 2.0006\n",
      "Epoch 185/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8080 - val_loss: 1.9117\n",
      "Epoch 186/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.7830 - val_loss: 1.9087\n",
      "Epoch 187/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8287 - val_loss: 1.9990\n",
      "Epoch 188/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9029 - val_loss: 2.0284\n",
      "Epoch 189/200\n",
      "1960/1960 [==============================] - 0s 18us/step - loss: 1.8335 - val_loss: 2.0231\n",
      "Epoch 190/200\n",
      "1960/1960 [==============================] - 0s 18us/step - loss: 1.8726 - val_loss: 2.0555\n",
      "Epoch 191/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8576 - val_loss: 1.9688\n",
      "Epoch 192/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8370 - val_loss: 1.9924\n",
      "Epoch 193/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8362 - val_loss: 1.9534\n",
      "Epoch 194/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8029 - val_loss: 1.9136\n",
      "Epoch 195/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.7856 - val_loss: 1.9196\n",
      "Epoch 196/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8146 - val_loss: 1.8969\n",
      "Epoch 197/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8324 - val_loss: 2.0327\n",
      "Epoch 198/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8258 - val_loss: 1.9521\n",
      "Epoch 199/200\n",
      "1960/1960 [==============================] - 0s 18us/step - loss: 1.8291 - val_loss: 1.9943\n",
      "Epoch 200/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8196 - val_loss: 1.9230\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_12 (InputLayer)        (None, 1)                 0         \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 64)                128       \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "pred (Dense)                 (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 2,241\n",
      "Trainable params: 2,241\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 1960 samples, validate on 490 samples\n",
      "Epoch 1/200\n",
      "1960/1960 [==============================] - 1s 256us/step - loss: 36.8582 - val_loss: 6.1326\n",
      "Epoch 2/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 10.4667 - val_loss: 7.7479\n",
      "Epoch 3/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 8.0145 - val_loss: 6.2934\n",
      "Epoch 4/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 6.6891 - val_loss: 4.5647\n",
      "Epoch 5/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 5.6697 - val_loss: 4.0978\n",
      "Epoch 6/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 4.8976 - val_loss: 3.4402\n",
      "Epoch 7/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 4.0603 - val_loss: 2.8195\n",
      "Epoch 8/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 3.4144 - val_loss: 2.5420\n",
      "Epoch 9/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 3.0836 - val_loss: 2.1574\n",
      "Epoch 10/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 2.7916 - val_loss: 2.0485\n",
      "Epoch 11/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 2.5293 - val_loss: 1.8463\n",
      "Epoch 12/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 2.4244 - val_loss: 1.9620\n",
      "Epoch 13/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 2.3234 - val_loss: 1.8351\n",
      "Epoch 14/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 2.2532 - val_loss: 1.8725\n",
      "Epoch 15/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 2.1364 - val_loss: 1.6037\n",
      "Epoch 16/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 2.1992 - val_loss: 1.7119\n",
      "Epoch 17/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 2.1506 - val_loss: 1.6123\n",
      "Epoch 18/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 2.1091 - val_loss: 1.7524\n",
      "Epoch 19/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 2.0750 - val_loss: 1.5659\n",
      "Epoch 20/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9793 - val_loss: 1.5171\n",
      "Epoch 21/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9616 - val_loss: 1.5255\n",
      "Epoch 22/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9395 - val_loss: 1.7651\n",
      "Epoch 23/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 2.0453 - val_loss: 1.5747\n",
      "Epoch 24/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9473 - val_loss: 1.5582\n",
      "Epoch 25/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 2.0584 - val_loss: 1.5361\n",
      "Epoch 26/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 2.0302 - val_loss: 1.5320\n",
      "Epoch 27/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9562 - val_loss: 1.5318\n",
      "Epoch 28/200\n",
      "1960/1960 [==============================] - 0s 18us/step - loss: 1.9543 - val_loss: 1.5836\n",
      "Epoch 29/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9592 - val_loss: 1.6725\n",
      "Epoch 30/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 2.0209 - val_loss: 1.5411\n",
      "Epoch 31/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9779 - val_loss: 1.7278\n",
      "Epoch 32/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 2.0632 - val_loss: 1.8119\n",
      "Epoch 33/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 2.0208 - val_loss: 1.5733\n",
      "Epoch 34/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9595 - val_loss: 1.5328\n",
      "Epoch 35/200\n",
      "1960/1960 [==============================] - 0s 18us/step - loss: 1.9840 - val_loss: 1.5540\n",
      "Epoch 36/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9631 - val_loss: 1.5620\n",
      "Epoch 37/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9582 - val_loss: 1.6890\n",
      "Epoch 38/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9830 - val_loss: 1.5331\n",
      "Epoch 39/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9302 - val_loss: 1.6226\n",
      "Epoch 40/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9900 - val_loss: 1.5059\n",
      "Epoch 41/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9324 - val_loss: 1.5043\n",
      "Epoch 42/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9113 - val_loss: 1.4957\n",
      "Epoch 43/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9607 - val_loss: 1.5662\n",
      "Epoch 44/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9678 - val_loss: 1.6185\n",
      "Epoch 45/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 2.0187 - val_loss: 1.5360\n",
      "Epoch 46/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9594 - val_loss: 1.5015\n",
      "Epoch 47/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9180 - val_loss: 1.5840\n",
      "Epoch 48/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9248 - val_loss: 1.4981\n",
      "Epoch 49/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9210 - val_loss: 1.4932\n",
      "Epoch 50/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9492 - val_loss: 1.5228\n",
      "Epoch 51/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9169 - val_loss: 1.5172\n",
      "Epoch 52/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9100 - val_loss: 1.6066\n",
      "Epoch 53/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 2.1501 - val_loss: 1.8207\n",
      "Epoch 54/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 2.0388 - val_loss: 1.6975\n",
      "Epoch 55/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 2.0701 - val_loss: 1.6500\n",
      "Epoch 56/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 2.0574 - val_loss: 1.5287\n",
      "Epoch 57/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 2.0134 - val_loss: 1.7420\n",
      "Epoch 58/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 2.0386 - val_loss: 1.6514\n",
      "Epoch 59/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.9767 - val_loss: 1.5519\n",
      "Epoch 60/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9844 - val_loss: 1.5650\n",
      "Epoch 61/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9793 - val_loss: 1.7082\n",
      "Epoch 62/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 2.0470 - val_loss: 1.6846\n",
      "Epoch 63/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 2.0803 - val_loss: 1.6897\n",
      "Epoch 64/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 2.0715 - val_loss: 1.7452\n",
      "Epoch 65/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 2.0819 - val_loss: 1.6647\n",
      "Epoch 66/200\n",
      "1960/1960 [==============================] - 0s 18us/step - loss: 2.0022 - val_loss: 1.5396\n",
      "Epoch 67/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9007 - val_loss: 1.5681\n",
      "Epoch 68/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9586 - val_loss: 1.6182\n",
      "Epoch 69/200\n",
      "1960/1960 [==============================] - 0s 18us/step - loss: 1.9319 - val_loss: 1.4970\n",
      "Epoch 70/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9113 - val_loss: 1.5090\n",
      "Epoch 71/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9675 - val_loss: 1.6073\n",
      "Epoch 72/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9665 - val_loss: 1.5774\n",
      "Epoch 73/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9751 - val_loss: 1.7314\n",
      "Epoch 74/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9829 - val_loss: 1.5595\n",
      "Epoch 75/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9989 - val_loss: 1.5567\n",
      "Epoch 76/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9000 - val_loss: 1.5162\n",
      "Epoch 77/200\n",
      "1960/1960 [==============================] - 0s 18us/step - loss: 1.9184 - val_loss: 1.5493\n",
      "Epoch 78/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9102 - val_loss: 1.5083\n",
      "Epoch 79/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9706 - val_loss: 1.5304\n",
      "Epoch 80/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9260 - val_loss: 1.5253\n",
      "Epoch 81/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9070 - val_loss: 1.5334\n",
      "Epoch 82/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9689 - val_loss: 1.6455\n",
      "Epoch 83/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 2.1135 - val_loss: 1.6958\n",
      "Epoch 84/200\n",
      "1960/1960 [==============================] - 0s 18us/step - loss: 1.9958 - val_loss: 1.5334\n",
      "Epoch 85/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9362 - val_loss: 1.5071\n",
      "Epoch 86/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8939 - val_loss: 1.5200\n",
      "Epoch 87/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8932 - val_loss: 1.5347\n",
      "Epoch 88/200\n",
      "1960/1960 [==============================] - 0s 18us/step - loss: 1.9020 - val_loss: 1.5539\n",
      "Epoch 89/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9017 - val_loss: 1.5211\n",
      "Epoch 90/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9037 - val_loss: 1.4919\n",
      "Epoch 91/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8859 - val_loss: 1.5086\n",
      "Epoch 92/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9416 - val_loss: 1.5381\n",
      "Epoch 93/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9229 - val_loss: 1.5405\n",
      "Epoch 94/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9515 - val_loss: 1.7263\n",
      "Epoch 95/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9858 - val_loss: 1.6072\n",
      "Epoch 96/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 2.0300 - val_loss: 1.5639\n",
      "Epoch 97/200\n",
      "1960/1960 [==============================] - 0s 18us/step - loss: 1.9383 - val_loss: 1.5361\n",
      "Epoch 98/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9393 - val_loss: 1.5017\n",
      "Epoch 99/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9381 - val_loss: 1.5137\n",
      "Epoch 100/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 2.0179 - val_loss: 1.5836\n",
      "Epoch 101/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9399 - val_loss: 1.7532\n",
      "Epoch 102/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1960/1960 [==============================] - 0s 17us/step - loss: 2.0882 - val_loss: 1.6468\n",
      "Epoch 103/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 2.0813 - val_loss: 1.5205\n",
      "Epoch 104/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9126 - val_loss: 1.6643\n",
      "Epoch 105/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9185 - val_loss: 1.5951\n",
      "Epoch 106/200\n",
      "1960/1960 [==============================] - 0s 18us/step - loss: 1.9488 - val_loss: 1.5441\n",
      "Epoch 107/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9109 - val_loss: 1.5095\n",
      "Epoch 108/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9612 - val_loss: 1.5697\n",
      "Epoch 109/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9592 - val_loss: 1.5263\n",
      "Epoch 110/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8939 - val_loss: 1.5326\n",
      "Epoch 111/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9159 - val_loss: 1.5716\n",
      "Epoch 112/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9414 - val_loss: 1.4905\n",
      "Epoch 113/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9406 - val_loss: 1.5619\n",
      "Epoch 114/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 2.0503 - val_loss: 1.5703\n",
      "Epoch 115/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 2.0960 - val_loss: 1.7666\n",
      "Epoch 116/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 2.0512 - val_loss: 1.5889\n",
      "Epoch 117/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9296 - val_loss: 1.5402\n",
      "Epoch 118/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9106 - val_loss: 1.5198\n",
      "Epoch 119/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9374 - val_loss: 1.5826\n",
      "Epoch 120/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 2.0747 - val_loss: 1.6436\n",
      "Epoch 121/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9997 - val_loss: 1.5962\n",
      "Epoch 122/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9760 - val_loss: 1.6185\n",
      "Epoch 123/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 2.0386 - val_loss: 1.5294\n",
      "Epoch 124/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9273 - val_loss: 1.5425\n",
      "Epoch 125/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9276 - val_loss: 1.4959\n",
      "Epoch 126/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9157 - val_loss: 1.5051\n",
      "Epoch 127/200\n",
      "1960/1960 [==============================] - 0s 18us/step - loss: 1.8908 - val_loss: 1.5527\n",
      "Epoch 128/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9668 - val_loss: 1.5491\n",
      "Epoch 129/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9412 - val_loss: 1.6405\n",
      "Epoch 130/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9733 - val_loss: 1.5750\n",
      "Epoch 131/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9195 - val_loss: 1.5483\n",
      "Epoch 132/200\n",
      "1960/1960 [==============================] - 0s 18us/step - loss: 1.9537 - val_loss: 1.7009\n",
      "Epoch 133/200\n",
      "1960/1960 [==============================] - 0s 18us/step - loss: 1.9636 - val_loss: 1.6006\n",
      "Epoch 134/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9714 - val_loss: 1.5530\n",
      "Epoch 135/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8959 - val_loss: 1.5375\n",
      "Epoch 136/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9062 - val_loss: 1.5692\n",
      "Epoch 137/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9508 - val_loss: 1.5253\n",
      "Epoch 138/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9479 - val_loss: 1.5091\n",
      "Epoch 139/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8977 - val_loss: 1.4994\n",
      "Epoch 140/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9125 - val_loss: 1.5104\n",
      "Epoch 141/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9226 - val_loss: 1.6111\n",
      "Epoch 142/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9454 - val_loss: 1.5529\n",
      "Epoch 143/200\n",
      "1960/1960 [==============================] - 0s 18us/step - loss: 1.9419 - val_loss: 1.7004\n",
      "Epoch 144/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9673 - val_loss: 1.5384\n",
      "Epoch 145/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9117 - val_loss: 1.4890\n",
      "Epoch 146/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9019 - val_loss: 1.4954\n",
      "Epoch 147/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9276 - val_loss: 1.5275\n",
      "Epoch 148/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9584 - val_loss: 1.7092\n",
      "Epoch 149/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9518 - val_loss: 1.5127\n",
      "Epoch 150/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9155 - val_loss: 1.5303\n",
      "Epoch 151/200\n",
      "1960/1960 [==============================] - 0s 18us/step - loss: 1.9297 - val_loss: 1.5521\n",
      "Epoch 152/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9404 - val_loss: 1.4918\n",
      "Epoch 153/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9327 - val_loss: 1.5268\n",
      "Epoch 154/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9700 - val_loss: 1.6224\n",
      "Epoch 155/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9273 - val_loss: 1.6756\n",
      "Epoch 156/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9945 - val_loss: 1.5263\n",
      "Epoch 157/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9065 - val_loss: 1.5350\n",
      "Epoch 158/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9640 - val_loss: 1.6445\n",
      "Epoch 159/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 2.1047 - val_loss: 1.6243\n",
      "Epoch 160/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9509 - val_loss: 1.5951\n",
      "Epoch 161/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9792 - val_loss: 1.5851\n",
      "Epoch 162/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 2.0646 - val_loss: 1.5952\n",
      "Epoch 163/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9213 - val_loss: 1.5720\n",
      "Epoch 164/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9879 - val_loss: 1.5020\n",
      "Epoch 165/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9586 - val_loss: 1.7312\n",
      "Epoch 166/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 2.0140 - val_loss: 1.5206\n",
      "Epoch 167/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9656 - val_loss: 1.5722\n",
      "Epoch 168/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9644 - val_loss: 1.5358\n",
      "Epoch 169/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9107 - val_loss: 1.5681\n",
      "Epoch 170/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9348 - val_loss: 1.4945\n",
      "Epoch 171/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9394 - val_loss: 1.5545\n",
      "Epoch 172/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 2.0145 - val_loss: 1.5935\n",
      "Epoch 173/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9784 - val_loss: 1.5350\n",
      "Epoch 174/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.9783 - val_loss: 1.5528\n",
      "Epoch 175/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9578 - val_loss: 1.5279\n",
      "Epoch 176/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9113 - val_loss: 1.5384\n",
      "Epoch 177/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9096 - val_loss: 1.5511\n",
      "Epoch 178/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9831 - val_loss: 1.5468\n",
      "Epoch 179/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9779 - val_loss: 1.5566\n",
      "Epoch 180/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9457 - val_loss: 1.5307\n",
      "Epoch 181/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9238 - val_loss: 1.4860\n",
      "Epoch 182/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9130 - val_loss: 1.5240\n",
      "Epoch 183/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9154 - val_loss: 1.5128\n",
      "Epoch 184/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9322 - val_loss: 1.5915\n",
      "Epoch 185/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9543 - val_loss: 1.5454\n",
      "Epoch 186/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9022 - val_loss: 1.5588\n",
      "Epoch 187/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8947 - val_loss: 1.5641\n",
      "Epoch 188/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 2.0018 - val_loss: 1.5999\n",
      "Epoch 189/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9257 - val_loss: 1.5672\n",
      "Epoch 190/200\n",
      "1960/1960 [==============================] - 0s 18us/step - loss: 1.8984 - val_loss: 1.5539\n",
      "Epoch 191/200\n",
      "1960/1960 [==============================] - 0s 18us/step - loss: 1.9471 - val_loss: 1.5017\n",
      "Epoch 192/200\n",
      "1960/1960 [==============================] - 0s 18us/step - loss: 1.9045 - val_loss: 1.5187\n",
      "Epoch 193/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9141 - val_loss: 1.5484\n",
      "Epoch 194/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9022 - val_loss: 1.5212\n",
      "Epoch 195/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9194 - val_loss: 1.5322\n",
      "Epoch 196/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9037 - val_loss: 1.5677\n",
      "Epoch 197/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9094 - val_loss: 1.5166\n",
      "Epoch 198/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9132 - val_loss: 1.5726\n",
      "Epoch 199/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9147 - val_loss: 1.5191\n",
      "Epoch 200/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8889 - val_loss: 1.5054\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_13 (InputLayer)        (None, 1)                 0         \n",
      "_________________________________________________________________\n",
      "dense_25 (Dense)             (None, 64)                128       \n",
      "_________________________________________________________________\n",
      "dense_26 (Dense)             (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "pred (Dense)                 (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 2,241\n",
      "Trainable params: 2,241\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 1960 samples, validate on 490 samples\n",
      "Epoch 1/200\n",
      "1960/1960 [==============================] - 1s 261us/step - loss: 40.9296 - val_loss: 11.5834\n",
      "Epoch 2/200\n",
      "1960/1960 [==============================] - 0s 18us/step - loss: 10.8649 - val_loss: 6.7877\n",
      "Epoch 3/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 8.1663 - val_loss: 6.4712\n",
      "Epoch 4/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 6.6201 - val_loss: 5.8330\n",
      "Epoch 5/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 5.7528 - val_loss: 5.0939\n",
      "Epoch 6/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 4.9867 - val_loss: 4.4036\n",
      "Epoch 7/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 4.4542 - val_loss: 4.4968\n",
      "Epoch 8/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 3.9246 - val_loss: 4.0282\n",
      "Epoch 9/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 3.4696 - val_loss: 3.6136\n",
      "Epoch 10/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 3.1961 - val_loss: 2.8700\n",
      "Epoch 11/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 2.7525 - val_loss: 2.6268\n",
      "Epoch 12/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 2.5384 - val_loss: 2.6823\n",
      "Epoch 13/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 2.5017 - val_loss: 2.4300\n",
      "Epoch 14/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 2.1945 - val_loss: 2.6535\n",
      "Epoch 15/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 2.1358 - val_loss: 2.1783\n",
      "Epoch 16/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 2.0330 - val_loss: 2.4319\n",
      "Epoch 17/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 2.1273 - val_loss: 2.0570\n",
      "Epoch 18/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9566 - val_loss: 2.3358\n",
      "Epoch 19/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 2.0455 - val_loss: 2.2674\n",
      "Epoch 20/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 2.0406 - val_loss: 2.1808\n",
      "Epoch 21/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 2.1170 - val_loss: 2.5472\n",
      "Epoch 22/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 2.0018 - val_loss: 2.6185\n",
      "Epoch 23/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9864 - val_loss: 2.4791\n",
      "Epoch 24/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 2.1118 - val_loss: 2.4774\n",
      "Epoch 25/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9519 - val_loss: 2.1571\n",
      "Epoch 26/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8631 - val_loss: 2.0733\n",
      "Epoch 27/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9171 - val_loss: 2.0745\n",
      "Epoch 28/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8698 - val_loss: 2.1521\n",
      "Epoch 29/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9442 - val_loss: 2.0173\n",
      "Epoch 30/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 2.0078 - val_loss: 2.1065\n",
      "Epoch 31/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9838 - val_loss: 2.0533\n",
      "Epoch 32/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8376 - val_loss: 2.0703\n",
      "Epoch 33/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8127 - val_loss: 2.2300\n",
      "Epoch 34/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8925 - val_loss: 2.2367\n",
      "Epoch 35/200\n",
      "1960/1960 [==============================] - 0s 18us/step - loss: 1.9261 - val_loss: 2.2028\n",
      "Epoch 36/200\n",
      "1960/1960 [==============================] - 0s 18us/step - loss: 1.9048 - val_loss: 2.3845\n",
      "Epoch 37/200\n",
      "1960/1960 [==============================] - 0s 18us/step - loss: 1.9782 - val_loss: 2.2675\n",
      "Epoch 38/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9862 - val_loss: 2.0583\n",
      "Epoch 39/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9168 - val_loss: 2.3801\n",
      "Epoch 40/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9647 - val_loss: 2.1384\n",
      "Epoch 41/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8322 - val_loss: 2.0468\n",
      "Epoch 42/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8208 - val_loss: 2.0410\n",
      "Epoch 43/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8133 - val_loss: 2.2075\n",
      "Epoch 44/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8154 - val_loss: 2.1142\n",
      "Epoch 45/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8335 - val_loss: 2.1833\n",
      "Epoch 46/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8692 - val_loss: 2.2132\n",
      "Epoch 47/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8650 - val_loss: 2.0045\n",
      "Epoch 48/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8724 - val_loss: 2.4219\n",
      "Epoch 49/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9078 - val_loss: 2.1177\n",
      "Epoch 50/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8459 - val_loss: 2.0838\n",
      "Epoch 51/200\n",
      "1960/1960 [==============================] - 0s 18us/step - loss: 1.7948 - val_loss: 1.9900\n",
      "Epoch 52/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8475 - val_loss: 2.0119\n",
      "Epoch 53/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9030 - val_loss: 1.9851\n",
      "Epoch 54/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8735 - val_loss: 2.1586\n",
      "Epoch 55/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8483 - val_loss: 2.2909\n",
      "Epoch 56/200\n",
      "1960/1960 [==============================] - 0s 18us/step - loss: 1.8409 - val_loss: 2.0714\n",
      "Epoch 57/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8357 - val_loss: 2.1160\n",
      "Epoch 58/200\n",
      "1960/1960 [==============================] - 0s 18us/step - loss: 1.8740 - val_loss: 2.0164\n",
      "Epoch 59/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9122 - val_loss: 2.0976\n",
      "Epoch 60/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9458 - val_loss: 2.0215\n",
      "Epoch 61/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9222 - val_loss: 2.0826\n",
      "Epoch 62/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8320 - val_loss: 2.0391\n",
      "Epoch 63/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9520 - val_loss: 2.0379\n",
      "Epoch 64/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8203 - val_loss: 2.0490\n",
      "Epoch 65/200\n",
      "1960/1960 [==============================] - 0s 18us/step - loss: 1.8876 - val_loss: 2.3473\n",
      "Epoch 66/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9418 - val_loss: 2.1307\n",
      "Epoch 67/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8035 - val_loss: 2.1001\n",
      "Epoch 68/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8144 - val_loss: 2.0271\n",
      "Epoch 69/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8118 - val_loss: 2.0333\n",
      "Epoch 70/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8339 - val_loss: 1.9820\n",
      "Epoch 71/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8482 - val_loss: 2.1293\n",
      "Epoch 72/200\n",
      "1960/1960 [==============================] - 0s 18us/step - loss: 1.8636 - val_loss: 2.1635\n",
      "Epoch 73/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9271 - val_loss: 2.0012\n",
      "Epoch 74/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9342 - val_loss: 1.9989\n",
      "Epoch 75/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8474 - val_loss: 1.9995\n",
      "Epoch 76/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8698 - val_loss: 1.9525\n",
      "Epoch 77/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9755 - val_loss: 2.1272\n",
      "Epoch 78/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9306 - val_loss: 2.9863\n",
      "Epoch 79/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 2.1466 - val_loss: 2.9287\n",
      "Epoch 80/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 2.1356 - val_loss: 2.2912\n",
      "Epoch 81/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9505 - val_loss: 2.3839\n",
      "Epoch 82/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9492 - val_loss: 2.3498\n",
      "Epoch 83/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9893 - val_loss: 2.0849\n",
      "Epoch 84/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9075 - val_loss: 2.0414\n",
      "Epoch 85/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8771 - val_loss: 2.0096\n",
      "Epoch 86/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8975 - val_loss: 2.0414\n",
      "Epoch 87/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8327 - val_loss: 2.0543\n",
      "Epoch 88/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8662 - val_loss: 2.0970\n",
      "Epoch 89/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8814 - val_loss: 2.2944\n",
      "Epoch 90/200\n",
      "1960/1960 [==============================] - 0s 18us/step - loss: 1.9749 - val_loss: 2.2013\n",
      "Epoch 91/200\n",
      "1960/1960 [==============================] - 0s 18us/step - loss: 1.9489 - val_loss: 2.0246\n",
      "Epoch 92/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8241 - val_loss: 2.0120\n",
      "Epoch 93/200\n",
      "1960/1960 [==============================] - 0s 18us/step - loss: 1.7901 - val_loss: 2.1032\n",
      "Epoch 94/200\n",
      "1960/1960 [==============================] - 0s 18us/step - loss: 1.8139 - val_loss: 2.0194\n",
      "Epoch 95/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8456 - val_loss: 1.9692\n",
      "Epoch 96/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8680 - val_loss: 2.1772\n",
      "Epoch 97/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9082 - val_loss: 2.1337\n",
      "Epoch 98/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9439 - val_loss: 1.9635\n",
      "Epoch 99/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8949 - val_loss: 1.9586\n",
      "Epoch 100/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9305 - val_loss: 2.0874\n",
      "Epoch 101/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8720 - val_loss: 2.3231\n",
      "Epoch 102/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8647 - val_loss: 2.2746\n",
      "Epoch 103/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9478 - val_loss: 1.9802\n",
      "Epoch 104/200\n",
      "1960/1960 [==============================] - 0s 18us/step - loss: 1.8616 - val_loss: 1.9864\n",
      "Epoch 105/200\n",
      "1960/1960 [==============================] - 0s 18us/step - loss: 1.8623 - val_loss: 2.0857\n",
      "Epoch 106/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9039 - val_loss: 1.9836\n",
      "Epoch 107/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8587 - val_loss: 2.1949\n",
      "Epoch 108/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8927 - val_loss: 2.2503\n",
      "Epoch 109/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8894 - val_loss: 2.1135\n",
      "Epoch 110/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8586 - val_loss: 2.0498\n",
      "Epoch 111/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8993 - val_loss: 2.0857\n",
      "Epoch 112/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8223 - val_loss: 2.0300\n",
      "Epoch 113/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8939 - val_loss: 2.1589\n",
      "Epoch 114/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8687 - val_loss: 2.2618\n",
      "Epoch 115/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8640 - val_loss: 2.0210\n",
      "Epoch 116/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8270 - val_loss: 2.0646\n",
      "Epoch 117/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8693 - val_loss: 2.1640\n",
      "Epoch 118/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8387 - val_loss: 2.1528\n",
      "Epoch 119/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8647 - val_loss: 2.1441\n",
      "Epoch 120/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8882 - val_loss: 2.1354\n",
      "Epoch 121/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8463 - val_loss: 2.0045\n",
      "Epoch 122/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8774 - val_loss: 2.0615\n",
      "Epoch 123/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8565 - val_loss: 2.2994\n",
      "Epoch 124/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8547 - val_loss: 2.2448\n",
      "Epoch 125/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8935 - val_loss: 1.9685\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 126/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8070 - val_loss: 2.0166\n",
      "Epoch 127/200\n",
      "1960/1960 [==============================] - 0s 18us/step - loss: 1.8198 - val_loss: 2.0193\n",
      "Epoch 128/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9023 - val_loss: 2.0661\n",
      "Epoch 129/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8414 - val_loss: 2.2294\n",
      "Epoch 130/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8611 - val_loss: 2.1670\n",
      "Epoch 131/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8851 - val_loss: 2.0246\n",
      "Epoch 132/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9078 - val_loss: 1.9974\n",
      "Epoch 133/200\n",
      "1960/1960 [==============================] - 0s 18us/step - loss: 1.8220 - val_loss: 2.3029\n",
      "Epoch 134/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8157 - val_loss: 1.9727\n",
      "Epoch 135/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8235 - val_loss: 2.1266\n",
      "Epoch 136/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8434 - val_loss: 2.0255\n",
      "Epoch 137/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8199 - val_loss: 2.0957\n",
      "Epoch 138/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8249 - val_loss: 2.0463\n",
      "Epoch 139/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8596 - val_loss: 2.0625\n",
      "Epoch 140/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8243 - val_loss: 2.0531\n",
      "Epoch 141/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8226 - val_loss: 2.1307\n",
      "Epoch 142/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8617 - val_loss: 2.0527\n",
      "Epoch 143/200\n",
      "1960/1960 [==============================] - 0s 18us/step - loss: 2.0154 - val_loss: 2.1741\n",
      "Epoch 144/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9653 - val_loss: 2.2526\n",
      "Epoch 145/200\n",
      "1960/1960 [==============================] - 0s 18us/step - loss: 1.8862 - val_loss: 2.2059\n",
      "Epoch 146/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9221 - val_loss: 2.0248\n",
      "Epoch 147/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8443 - val_loss: 2.2060\n",
      "Epoch 148/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9491 - val_loss: 2.0457\n",
      "Epoch 149/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8730 - val_loss: 2.0393\n",
      "Epoch 150/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8318 - val_loss: 2.1442\n",
      "Epoch 151/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8389 - val_loss: 2.0078\n",
      "Epoch 152/200\n",
      "1960/1960 [==============================] - 0s 18us/step - loss: 1.8443 - val_loss: 2.0220\n",
      "Epoch 153/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8179 - val_loss: 1.9635\n",
      "Epoch 154/200\n",
      "1960/1960 [==============================] - 0s 18us/step - loss: 1.8292 - val_loss: 1.9902\n",
      "Epoch 155/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8139 - val_loss: 2.0302\n",
      "Epoch 156/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8753 - val_loss: 2.2202\n",
      "Epoch 157/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9236 - val_loss: 1.9729\n",
      "Epoch 158/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8499 - val_loss: 2.0946\n",
      "Epoch 159/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9202 - val_loss: 2.2379\n",
      "Epoch 160/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9004 - val_loss: 2.0639\n",
      "Epoch 161/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.7975 - val_loss: 2.0919\n",
      "Epoch 162/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8619 - val_loss: 2.0517\n",
      "Epoch 163/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8540 - val_loss: 2.0155\n",
      "Epoch 164/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8173 - val_loss: 2.1668\n",
      "Epoch 165/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.7940 - val_loss: 2.0344\n",
      "Epoch 166/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8429 - val_loss: 2.0405\n",
      "Epoch 167/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8386 - val_loss: 2.0039\n",
      "Epoch 168/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8173 - val_loss: 2.0814\n",
      "Epoch 169/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8325 - val_loss: 2.0533\n",
      "Epoch 170/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8322 - val_loss: 2.0128\n",
      "Epoch 171/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8184 - val_loss: 2.0919\n",
      "Epoch 172/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8314 - val_loss: 2.1615\n",
      "Epoch 173/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9341 - val_loss: 2.0859\n",
      "Epoch 174/200\n",
      "1960/1960 [==============================] - 0s 18us/step - loss: 1.8689 - val_loss: 2.0525\n",
      "Epoch 175/200\n",
      "1960/1960 [==============================] - 0s 18us/step - loss: 1.8520 - val_loss: 2.0002\n",
      "Epoch 176/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8398 - val_loss: 2.1853\n",
      "Epoch 177/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9055 - val_loss: 2.2825\n",
      "Epoch 178/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9169 - val_loss: 2.2826\n",
      "Epoch 179/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9169 - val_loss: 1.9942\n",
      "Epoch 180/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8350 - val_loss: 2.0180\n",
      "Epoch 181/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8149 - val_loss: 2.0729\n",
      "Epoch 182/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8558 - val_loss: 2.2228\n",
      "Epoch 183/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9139 - val_loss: 2.0987\n",
      "Epoch 184/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8535 - val_loss: 2.0600\n",
      "Epoch 185/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8529 - val_loss: 2.0845\n",
      "Epoch 186/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8616 - val_loss: 2.2461\n",
      "Epoch 187/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9082 - val_loss: 1.9525\n",
      "Epoch 188/200\n",
      "1960/1960 [==============================] - 0s 18us/step - loss: 1.8535 - val_loss: 2.1250\n",
      "Epoch 189/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8966 - val_loss: 2.0658\n",
      "Epoch 190/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8311 - val_loss: 1.9842\n",
      "Epoch 191/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8161 - val_loss: 2.1501\n",
      "Epoch 192/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8165 - val_loss: 1.9990\n",
      "Epoch 193/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.7980 - val_loss: 2.0174\n",
      "Epoch 194/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8429 - val_loss: 1.9799\n",
      "Epoch 195/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8424 - val_loss: 2.4241\n",
      "Epoch 196/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9969 - val_loss: 2.1098\n",
      "Epoch 197/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9106 - val_loss: 2.2109\n",
      "Epoch 198/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9078 - val_loss: 2.0609\n",
      "Epoch 199/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8440 - val_loss: 2.2065\n",
      "Epoch 200/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8266 - val_loss: 2.0951\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_14 (InputLayer)        (None, 1)                 0         \n",
      "_________________________________________________________________\n",
      "dense_27 (Dense)             (None, 64)                128       \n",
      "_________________________________________________________________\n",
      "dense_28 (Dense)             (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "pred (Dense)                 (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 2,241\n",
      "Trainable params: 2,241\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1960 samples, validate on 490 samples\n",
      "Epoch 1/200\n",
      "1960/1960 [==============================] - 1s 269us/step - loss: 28.3770 - val_loss: 16.2081\n",
      "Epoch 2/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 10.5070 - val_loss: 7.4463\n",
      "Epoch 3/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 6.9013 - val_loss: 6.4666\n",
      "Epoch 4/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 5.4961 - val_loss: 5.0624\n",
      "Epoch 5/200\n",
      "1960/1960 [==============================] - 0s 18us/step - loss: 4.4369 - val_loss: 4.2704\n",
      "Epoch 6/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 3.5727 - val_loss: 3.2985\n",
      "Epoch 7/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 2.8552 - val_loss: 2.9050\n",
      "Epoch 8/200\n",
      "1960/1960 [==============================] - 0s 18us/step - loss: 2.5444 - val_loss: 2.5754\n",
      "Epoch 9/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 2.4610 - val_loss: 2.6164\n",
      "Epoch 10/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 2.2556 - val_loss: 2.3369\n",
      "Epoch 11/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 2.1363 - val_loss: 2.0244\n",
      "Epoch 12/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 2.0486 - val_loss: 1.8991\n",
      "Epoch 13/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 2.0068 - val_loss: 1.8661\n",
      "Epoch 14/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9697 - val_loss: 1.9133\n",
      "Epoch 15/200\n",
      "1960/1960 [==============================] - 0s 18us/step - loss: 1.9462 - val_loss: 1.9575\n",
      "Epoch 16/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8833 - val_loss: 1.9494\n",
      "Epoch 17/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8618 - val_loss: 1.8154\n",
      "Epoch 18/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8974 - val_loss: 1.8111\n",
      "Epoch 19/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9181 - val_loss: 1.9471\n",
      "Epoch 20/200\n",
      "1960/1960 [==============================] - 0s 18us/step - loss: 1.8728 - val_loss: 1.9027\n",
      "Epoch 21/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8957 - val_loss: 1.8538\n",
      "Epoch 22/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8779 - val_loss: 1.7903\n",
      "Epoch 23/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9077 - val_loss: 1.8346\n",
      "Epoch 24/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9818 - val_loss: 1.8333\n",
      "Epoch 25/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 2.0838 - val_loss: 2.1977\n",
      "Epoch 26/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9591 - val_loss: 1.9087\n",
      "Epoch 27/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8472 - val_loss: 1.7939\n",
      "Epoch 28/200\n",
      "1960/1960 [==============================] - 0s 18us/step - loss: 1.9034 - val_loss: 1.9009\n",
      "Epoch 29/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8817 - val_loss: 1.8191\n",
      "Epoch 30/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8310 - val_loss: 1.8333\n",
      "Epoch 31/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8209 - val_loss: 1.8024\n",
      "Epoch 32/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8679 - val_loss: 1.7865\n",
      "Epoch 33/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8726 - val_loss: 1.7618\n",
      "Epoch 34/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8802 - val_loss: 1.7927\n",
      "Epoch 35/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8536 - val_loss: 2.2002\n",
      "Epoch 36/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 2.2201 - val_loss: 2.0485\n",
      "Epoch 37/200\n",
      "1960/1960 [==============================] - 0s 18us/step - loss: 2.0619 - val_loss: 1.9798\n",
      "Epoch 38/200\n",
      "1960/1960 [==============================] - 0s 18us/step - loss: 1.9310 - val_loss: 1.7770\n",
      "Epoch 39/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9005 - val_loss: 1.8433\n",
      "Epoch 40/200\n",
      "1960/1960 [==============================] - 0s 18us/step - loss: 1.9033 - val_loss: 1.8937\n",
      "Epoch 41/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9199 - val_loss: 1.8381\n",
      "Epoch 42/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9149 - val_loss: 1.8113\n",
      "Epoch 43/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8832 - val_loss: 1.8815\n",
      "Epoch 44/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8913 - val_loss: 1.8168\n",
      "Epoch 45/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9283 - val_loss: 1.7619\n",
      "Epoch 46/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9048 - val_loss: 1.8531\n",
      "Epoch 47/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8752 - val_loss: 1.9098\n",
      "Epoch 48/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9181 - val_loss: 1.9649\n",
      "Epoch 49/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9263 - val_loss: 1.8629\n",
      "Epoch 50/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8707 - val_loss: 1.7671\n",
      "Epoch 51/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9802 - val_loss: 1.9771\n",
      "Epoch 52/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9691 - val_loss: 2.1077\n",
      "Epoch 53/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9028 - val_loss: 1.8581\n",
      "Epoch 54/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9179 - val_loss: 1.7882\n",
      "Epoch 55/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8663 - val_loss: 2.0103\n",
      "Epoch 56/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8749 - val_loss: 1.7581\n",
      "Epoch 57/200\n",
      "1960/1960 [==============================] - 0s 18us/step - loss: 1.8539 - val_loss: 1.9167\n",
      "Epoch 58/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9477 - val_loss: 2.0681\n",
      "Epoch 59/200\n",
      "1960/1960 [==============================] - 0s 18us/step - loss: 1.8974 - val_loss: 1.8740\n",
      "Epoch 60/200\n",
      "1960/1960 [==============================] - 0s 18us/step - loss: 1.9035 - val_loss: 1.8280\n",
      "Epoch 61/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8510 - val_loss: 1.8238\n",
      "Epoch 62/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9079 - val_loss: 1.7740\n",
      "Epoch 63/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9093 - val_loss: 1.8491\n",
      "Epoch 64/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9990 - val_loss: 1.8087\n",
      "Epoch 65/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8847 - val_loss: 2.0683\n",
      "Epoch 66/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9832 - val_loss: 1.9261\n",
      "Epoch 67/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8793 - val_loss: 1.7853\n",
      "Epoch 68/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9016 - val_loss: 1.9200\n",
      "Epoch 69/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9401 - val_loss: 1.8985\n",
      "Epoch 70/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8394 - val_loss: 1.8275\n",
      "Epoch 71/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9025 - val_loss: 1.8394\n",
      "Epoch 72/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8243 - val_loss: 1.8151\n",
      "Epoch 73/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8667 - val_loss: 1.8185\n",
      "Epoch 74/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8652 - val_loss: 1.9136\n",
      "Epoch 75/200\n",
      "1960/1960 [==============================] - 0s 18us/step - loss: 1.9322 - val_loss: 2.1201\n",
      "Epoch 76/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9042 - val_loss: 1.8471\n",
      "Epoch 77/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8480 - val_loss: 1.8327\n",
      "Epoch 78/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8439 - val_loss: 1.7662\n",
      "Epoch 79/200\n",
      "1960/1960 [==============================] - 0s 18us/step - loss: 1.8710 - val_loss: 1.9061\n",
      "Epoch 80/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8247 - val_loss: 1.8060\n",
      "Epoch 81/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9360 - val_loss: 1.9519\n",
      "Epoch 82/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8579 - val_loss: 1.8659\n",
      "Epoch 83/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9196 - val_loss: 1.9547\n",
      "Epoch 84/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9184 - val_loss: 1.8658\n",
      "Epoch 85/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9061 - val_loss: 1.8000\n",
      "Epoch 86/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8316 - val_loss: 1.7704\n",
      "Epoch 87/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8553 - val_loss: 1.7548\n",
      "Epoch 88/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8057 - val_loss: 1.8358\n",
      "Epoch 89/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8350 - val_loss: 1.8118\n",
      "Epoch 90/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8524 - val_loss: 1.8538\n",
      "Epoch 91/200\n",
      "1960/1960 [==============================] - 0s 18us/step - loss: 1.8444 - val_loss: 2.0324\n",
      "Epoch 92/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9704 - val_loss: 1.9066\n",
      "Epoch 93/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9372 - val_loss: 2.1365\n",
      "Epoch 94/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9547 - val_loss: 1.8358\n",
      "Epoch 95/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9210 - val_loss: 1.8557\n",
      "Epoch 96/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9527 - val_loss: 1.8178\n",
      "Epoch 97/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8514 - val_loss: 2.0373\n",
      "Epoch 98/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8815 - val_loss: 1.7897\n",
      "Epoch 99/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8469 - val_loss: 1.8692\n",
      "Epoch 100/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8507 - val_loss: 1.8073\n",
      "Epoch 101/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8728 - val_loss: 1.7750\n",
      "Epoch 102/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8879 - val_loss: 1.8493\n",
      "Epoch 103/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8713 - val_loss: 2.0040\n",
      "Epoch 104/200\n",
      "1960/1960 [==============================] - 0s 18us/step - loss: 1.8655 - val_loss: 1.8448\n",
      "Epoch 105/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9186 - val_loss: 1.7887\n",
      "Epoch 106/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8331 - val_loss: 1.8974\n",
      "Epoch 107/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8474 - val_loss: 1.7528\n",
      "Epoch 108/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8497 - val_loss: 1.7922\n",
      "Epoch 109/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8352 - val_loss: 1.8329\n",
      "Epoch 110/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8632 - val_loss: 1.7446\n",
      "Epoch 111/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8216 - val_loss: 1.8230\n",
      "Epoch 112/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8318 - val_loss: 1.8005\n",
      "Epoch 113/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8817 - val_loss: 2.0858\n",
      "Epoch 114/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8681 - val_loss: 1.7964\n",
      "Epoch 115/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8462 - val_loss: 2.0766\n",
      "Epoch 116/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 2.0038 - val_loss: 1.8007\n",
      "Epoch 117/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8971 - val_loss: 1.8805\n",
      "Epoch 118/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9137 - val_loss: 1.9718\n",
      "Epoch 119/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8475 - val_loss: 1.7928\n",
      "Epoch 120/200\n",
      "1960/1960 [==============================] - 0s 18us/step - loss: 1.8985 - val_loss: 1.7674\n",
      "Epoch 121/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8555 - val_loss: 1.7798\n",
      "Epoch 122/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8460 - val_loss: 1.9041\n",
      "Epoch 123/200\n",
      "1960/1960 [==============================] - 0s 18us/step - loss: 1.9621 - val_loss: 2.0467\n",
      "Epoch 124/200\n",
      "1960/1960 [==============================] - 0s 18us/step - loss: 2.0208 - val_loss: 1.7894\n",
      "Epoch 125/200\n",
      "1960/1960 [==============================] - 0s 18us/step - loss: 1.9038 - val_loss: 1.8408\n",
      "Epoch 126/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8674 - val_loss: 1.7366\n",
      "Epoch 127/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9217 - val_loss: 1.7364\n",
      "Epoch 128/200\n",
      "1960/1960 [==============================] - 0s 18us/step - loss: 1.8387 - val_loss: 1.7631\n",
      "Epoch 129/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8496 - val_loss: 1.7741\n",
      "Epoch 130/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9202 - val_loss: 1.9785\n",
      "Epoch 131/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9848 - val_loss: 2.1489\n",
      "Epoch 132/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9608 - val_loss: 2.0015\n",
      "Epoch 133/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8765 - val_loss: 1.9151\n",
      "Epoch 134/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8986 - val_loss: 1.8443\n",
      "Epoch 135/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9273 - val_loss: 1.8365\n",
      "Epoch 136/200\n",
      "1960/1960 [==============================] - 0s 18us/step - loss: 1.8767 - val_loss: 1.9996\n",
      "Epoch 137/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8611 - val_loss: 1.7897\n",
      "Epoch 138/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8435 - val_loss: 1.9400\n",
      "Epoch 139/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8384 - val_loss: 1.7671\n",
      "Epoch 140/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9287 - val_loss: 1.9804\n",
      "Epoch 141/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8468 - val_loss: 1.8963\n",
      "Epoch 142/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8692 - val_loss: 1.8390\n",
      "Epoch 143/200\n",
      "1960/1960 [==============================] - 0s 18us/step - loss: 1.8828 - val_loss: 1.8222\n",
      "Epoch 144/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8580 - val_loss: 1.7424\n",
      "Epoch 145/200\n",
      "1960/1960 [==============================] - 0s 18us/step - loss: 1.8304 - val_loss: 1.7561\n",
      "Epoch 146/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8398 - val_loss: 1.8310\n",
      "Epoch 147/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9210 - val_loss: 1.8705\n",
      "Epoch 148/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8708 - val_loss: 1.7420\n",
      "Epoch 149/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8660 - val_loss: 1.8018\n",
      "Epoch 150/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9343 - val_loss: 1.9972\n",
      "Epoch 151/200\n",
      "1960/1960 [==============================] - 0s 16us/step - loss: 1.8412 - val_loss: 1.7690\n",
      "Epoch 152/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8759 - val_loss: 1.9090\n",
      "Epoch 153/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8600 - val_loss: 1.9844\n",
      "Epoch 154/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9310 - val_loss: 1.7657\n",
      "Epoch 155/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8371 - val_loss: 1.7731\n",
      "Epoch 156/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8455 - val_loss: 1.8078\n",
      "Epoch 157/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9052 - val_loss: 1.7238\n",
      "Epoch 158/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8528 - val_loss: 1.8967\n",
      "Epoch 159/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8640 - val_loss: 1.9304\n",
      "Epoch 160/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9228 - val_loss: 1.8025\n",
      "Epoch 161/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9549 - val_loss: 1.7713\n",
      "Epoch 162/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8272 - val_loss: 1.8425\n",
      "Epoch 163/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8284 - val_loss: 1.8440\n",
      "Epoch 164/200\n",
      "1960/1960 [==============================] - 0s 18us/step - loss: 1.8239 - val_loss: 1.8692\n",
      "Epoch 165/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8794 - val_loss: 1.7853\n",
      "Epoch 166/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8940 - val_loss: 1.7749\n",
      "Epoch 167/200\n",
      "1960/1960 [==============================] - 0s 18us/step - loss: 1.8734 - val_loss: 1.7938\n",
      "Epoch 168/200\n",
      "1960/1960 [==============================] - 0s 18us/step - loss: 1.8703 - val_loss: 1.8356\n",
      "Epoch 169/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8538 - val_loss: 1.7862\n",
      "Epoch 170/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8821 - val_loss: 1.9525\n",
      "Epoch 171/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8678 - val_loss: 1.8146\n",
      "Epoch 172/200\n",
      "1960/1960 [==============================] - 0s 18us/step - loss: 1.9508 - val_loss: 1.8026\n",
      "Epoch 173/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9291 - val_loss: 1.7801\n",
      "Epoch 174/200\n",
      "1960/1960 [==============================] - 0s 18us/step - loss: 1.8640 - val_loss: 1.7593\n",
      "Epoch 175/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8325 - val_loss: 1.7883\n",
      "Epoch 176/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8587 - val_loss: 1.8282\n",
      "Epoch 177/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8416 - val_loss: 1.8908\n",
      "Epoch 178/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8355 - val_loss: 1.7813\n",
      "Epoch 179/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8831 - val_loss: 1.7691\n",
      "Epoch 180/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8684 - val_loss: 2.0950\n",
      "Epoch 181/200\n",
      "1960/1960 [==============================] - 0s 18us/step - loss: 1.9208 - val_loss: 1.9378\n",
      "Epoch 182/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8621 - val_loss: 1.8681\n",
      "Epoch 183/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8558 - val_loss: 1.8107\n",
      "Epoch 184/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8404 - val_loss: 1.7953\n",
      "Epoch 185/200\n",
      "1960/1960 [==============================] - 0s 18us/step - loss: 1.8358 - val_loss: 1.7434\n",
      "Epoch 186/200\n",
      "1960/1960 [==============================] - 0s 18us/step - loss: 1.8520 - val_loss: 1.8847\n",
      "Epoch 187/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8583 - val_loss: 1.9654\n",
      "Epoch 188/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9401 - val_loss: 2.0195\n",
      "Epoch 189/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9249 - val_loss: 1.7890\n",
      "Epoch 190/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9471 - val_loss: 1.8779\n",
      "Epoch 191/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8694 - val_loss: 1.8732\n",
      "Epoch 192/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8206 - val_loss: 1.7641\n",
      "Epoch 193/200\n",
      "1960/1960 [==============================] - 0s 18us/step - loss: 1.8484 - val_loss: 1.7811\n",
      "Epoch 194/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8571 - val_loss: 1.9536\n",
      "Epoch 195/200\n",
      "1960/1960 [==============================] - 0s 18us/step - loss: 1.8663 - val_loss: 1.8873\n",
      "Epoch 196/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8526 - val_loss: 1.8412\n",
      "Epoch 197/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8105 - val_loss: 1.8299\n",
      "Epoch 198/200\n",
      "1960/1960 [==============================] - 0s 18us/step - loss: 1.9221 - val_loss: 1.7702\n",
      "Epoch 199/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8427 - val_loss: 1.7485\n",
      "Epoch 200/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8853 - val_loss: 1.7972\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_15 (InputLayer)        (None, 1)                 0         \n",
      "_________________________________________________________________\n",
      "dense_29 (Dense)             (None, 64)                128       \n",
      "_________________________________________________________________\n",
      "dense_30 (Dense)             (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "pred (Dense)                 (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 2,241\n",
      "Trainable params: 2,241\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 1960 samples, validate on 490 samples\n",
      "Epoch 1/200\n",
      "1960/1960 [==============================] - 1s 280us/step - loss: 31.7772 - val_loss: 12.5869\n",
      "Epoch 2/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 10.2276 - val_loss: 7.6267\n",
      "Epoch 3/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 7.3024 - val_loss: 5.8652\n",
      "Epoch 4/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 5.6941 - val_loss: 4.5545\n",
      "Epoch 5/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 4.4115 - val_loss: 3.7172\n",
      "Epoch 6/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 3.6753 - val_loss: 2.8797\n",
      "Epoch 7/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 3.0813 - val_loss: 2.5634\n",
      "Epoch 8/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 2.5459 - val_loss: 2.1374\n",
      "Epoch 9/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 2.2498 - val_loss: 2.0231\n",
      "Epoch 10/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 2.1297 - val_loss: 1.9377\n",
      "Epoch 11/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 2.0680 - val_loss: 1.9353\n",
      "Epoch 12/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 2.1223 - val_loss: 2.2157\n",
      "Epoch 13/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 2.0623 - val_loss: 1.9279\n",
      "Epoch 14/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9498 - val_loss: 1.8384\n",
      "Epoch 15/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9015 - val_loss: 2.0367\n",
      "Epoch 16/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9494 - val_loss: 1.9622\n",
      "Epoch 17/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 2.0196 - val_loss: 1.9193\n",
      "Epoch 18/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9122 - val_loss: 1.8362\n",
      "Epoch 19/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8585 - val_loss: 1.7744\n",
      "Epoch 20/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8472 - val_loss: 1.8657\n",
      "Epoch 21/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8429 - val_loss: 1.7849\n",
      "Epoch 22/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9057 - val_loss: 2.0841\n",
      "Epoch 23/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9729 - val_loss: 1.8413\n",
      "Epoch 24/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9380 - val_loss: 1.9235\n",
      "Epoch 25/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9410 - val_loss: 1.8385\n",
      "Epoch 26/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8819 - val_loss: 1.8331\n",
      "Epoch 27/200\n",
      "1960/1960 [==============================] - 0s 18us/step - loss: 1.9096 - val_loss: 2.0147\n",
      "Epoch 28/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9218 - val_loss: 1.8249\n",
      "Epoch 29/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8718 - val_loss: 1.8982\n",
      "Epoch 30/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 2.1382 - val_loss: 1.8873\n",
      "Epoch 31/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9134 - val_loss: 1.8285\n",
      "Epoch 32/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9007 - val_loss: 1.8557\n",
      "Epoch 33/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9062 - val_loss: 1.8152\n",
      "Epoch 34/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9142 - val_loss: 1.8039\n",
      "Epoch 35/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8597 - val_loss: 1.7865\n",
      "Epoch 36/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8355 - val_loss: 1.8036\n",
      "Epoch 37/200\n",
      "1960/1960 [==============================] - 0s 18us/step - loss: 1.8653 - val_loss: 1.8042\n",
      "Epoch 38/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8857 - val_loss: 1.8481\n",
      "Epoch 39/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8510 - val_loss: 1.8315\n",
      "Epoch 40/200\n",
      "1960/1960 [==============================] - 0s 18us/step - loss: 1.8205 - val_loss: 1.8369\n",
      "Epoch 41/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9408 - val_loss: 1.7719\n",
      "Epoch 42/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9329 - val_loss: 2.1756\n",
      "Epoch 43/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9686 - val_loss: 1.8324\n",
      "Epoch 44/200\n",
      "1960/1960 [==============================] - 0s 18us/step - loss: 1.9866 - val_loss: 2.1537\n",
      "Epoch 45/200\n",
      "1960/1960 [==============================] - 0s 18us/step - loss: 2.0282 - val_loss: 1.9294\n",
      "Epoch 46/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9310 - val_loss: 1.9367\n",
      "Epoch 47/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8753 - val_loss: 1.9708\n",
      "Epoch 48/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9231 - val_loss: 1.7939\n",
      "Epoch 49/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8433 - val_loss: 1.8947\n",
      "Epoch 50/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8469 - val_loss: 1.8555\n",
      "Epoch 51/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8463 - val_loss: 1.9257\n",
      "Epoch 52/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9001 - val_loss: 1.9727\n",
      "Epoch 53/200\n",
      "1960/1960 [==============================] - 0s 18us/step - loss: 1.8832 - val_loss: 1.7718\n",
      "Epoch 54/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8389 - val_loss: 1.8248\n",
      "Epoch 55/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9123 - val_loss: 2.1830\n",
      "Epoch 56/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9497 - val_loss: 1.8401\n",
      "Epoch 57/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 2.0407 - val_loss: 1.9080\n",
      "Epoch 58/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9985 - val_loss: 1.8552\n",
      "Epoch 59/200\n",
      "1960/1960 [==============================] - 0s 18us/step - loss: 1.9882 - val_loss: 1.9728\n",
      "Epoch 60/200\n",
      "1960/1960 [==============================] - 0s 18us/step - loss: 1.8348 - val_loss: 1.7902\n",
      "Epoch 61/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8211 - val_loss: 1.8458\n",
      "Epoch 62/200\n",
      "1960/1960 [==============================] - 0s 18us/step - loss: 1.9062 - val_loss: 1.8013\n",
      "Epoch 63/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9143 - val_loss: 1.7879\n",
      "Epoch 64/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9963 - val_loss: 1.9311\n",
      "Epoch 65/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9330 - val_loss: 1.7942\n",
      "Epoch 66/200\n",
      "1960/1960 [==============================] - 0s 18us/step - loss: 1.8960 - val_loss: 2.0251\n",
      "Epoch 67/200\n",
      "1960/1960 [==============================] - 0s 18us/step - loss: 1.9098 - val_loss: 1.8337\n",
      "Epoch 68/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8890 - val_loss: 1.8038\n",
      "Epoch 69/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9082 - val_loss: 1.8226\n",
      "Epoch 70/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8613 - val_loss: 1.8798\n",
      "Epoch 71/200\n",
      "1960/1960 [==============================] - 0s 18us/step - loss: 1.8834 - val_loss: 1.8384\n",
      "Epoch 72/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8549 - val_loss: 1.7827\n",
      "Epoch 73/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9156 - val_loss: 1.8274\n",
      "Epoch 74/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8517 - val_loss: 1.9619\n",
      "Epoch 75/200\n",
      "1960/1960 [==============================] - 0s 18us/step - loss: 2.0168 - val_loss: 1.9539\n",
      "Epoch 76/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9600 - val_loss: 1.9241\n",
      "Epoch 77/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9696 - val_loss: 1.8834\n",
      "Epoch 78/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9702 - val_loss: 1.7962\n",
      "Epoch 79/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8225 - val_loss: 1.7863\n",
      "Epoch 80/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8771 - val_loss: 1.8466\n",
      "Epoch 81/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8516 - val_loss: 1.7943\n",
      "Epoch 82/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9068 - val_loss: 1.9694\n",
      "Epoch 83/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9245 - val_loss: 1.7777\n",
      "Epoch 84/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8724 - val_loss: 1.8337\n",
      "Epoch 85/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8487 - val_loss: 1.7888\n",
      "Epoch 86/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8456 - val_loss: 1.7726\n",
      "Epoch 87/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8398 - val_loss: 1.9337\n",
      "Epoch 88/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9508 - val_loss: 1.8734\n",
      "Epoch 89/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8893 - val_loss: 2.0086\n",
      "Epoch 90/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9363 - val_loss: 1.8636\n",
      "Epoch 91/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9077 - val_loss: 2.1144\n",
      "Epoch 92/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 2.0004 - val_loss: 1.8776\n",
      "Epoch 93/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8691 - val_loss: 1.8378\n",
      "Epoch 94/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9072 - val_loss: 2.0822\n",
      "Epoch 95/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9665 - val_loss: 1.8853\n",
      "Epoch 96/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8455 - val_loss: 1.8061\n",
      "Epoch 97/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8507 - val_loss: 1.8576\n",
      "Epoch 98/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8756 - val_loss: 1.7698\n",
      "Epoch 99/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8885 - val_loss: 1.8084\n",
      "Epoch 100/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8353 - val_loss: 1.8098\n",
      "Epoch 101/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8416 - val_loss: 1.8001\n",
      "Epoch 102/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8295 - val_loss: 1.7901\n",
      "Epoch 103/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8297 - val_loss: 1.7975\n",
      "Epoch 104/200\n",
      "1960/1960 [==============================] - 0s 18us/step - loss: 1.8397 - val_loss: 1.8668\n",
      "Epoch 105/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8873 - val_loss: 1.8893\n",
      "Epoch 106/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9355 - val_loss: 1.7890\n",
      "Epoch 107/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8704 - val_loss: 1.8342\n",
      "Epoch 108/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8626 - val_loss: 1.8374\n",
      "Epoch 109/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8519 - val_loss: 1.7876\n",
      "Epoch 110/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8281 - val_loss: 2.0769\n",
      "Epoch 111/200\n",
      "1960/1960 [==============================] - 0s 18us/step - loss: 1.8705 - val_loss: 1.8016\n",
      "Epoch 112/200\n",
      "1960/1960 [==============================] - 0s 18us/step - loss: 1.8865 - val_loss: 1.8436\n",
      "Epoch 113/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8701 - val_loss: 1.7674\n",
      "Epoch 114/200\n",
      "1960/1960 [==============================] - 0s 20us/step - loss: 1.9059 - val_loss: 1.9665\n",
      "Epoch 115/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9049 - val_loss: 1.8560\n",
      "Epoch 116/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8564 - val_loss: 1.8522\n",
      "Epoch 117/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 2.0131 - val_loss: 1.7820\n",
      "Epoch 118/200\n",
      "1960/1960 [==============================] - 0s 18us/step - loss: 1.8685 - val_loss: 1.8444\n",
      "Epoch 119/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8574 - val_loss: 1.7905\n",
      "Epoch 120/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8492 - val_loss: 1.8785\n",
      "Epoch 121/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8889 - val_loss: 1.8602\n",
      "Epoch 122/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8548 - val_loss: 1.7648\n",
      "Epoch 123/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8712 - val_loss: 1.7775\n",
      "Epoch 124/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8196 - val_loss: 1.8121\n",
      "Epoch 125/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8613 - val_loss: 1.8064\n",
      "Epoch 126/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8450 - val_loss: 1.8089\n",
      "Epoch 127/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9488 - val_loss: 1.8185\n",
      "Epoch 128/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8751 - val_loss: 1.8616\n",
      "Epoch 129/200\n",
      "1960/1960 [==============================] - 0s 18us/step - loss: 1.8610 - val_loss: 1.7908\n",
      "Epoch 130/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8048 - val_loss: 1.7792\n",
      "Epoch 131/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9421 - val_loss: 1.8900\n",
      "Epoch 132/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8855 - val_loss: 1.8093\n",
      "Epoch 133/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8507 - val_loss: 1.8194\n",
      "Epoch 134/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8605 - val_loss: 1.8931\n",
      "Epoch 135/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9501 - val_loss: 1.7703\n",
      "Epoch 136/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9124 - val_loss: 1.9117\n",
      "Epoch 137/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8720 - val_loss: 1.7917\n",
      "Epoch 138/200\n",
      "1960/1960 [==============================] - 0s 18us/step - loss: 1.8801 - val_loss: 1.8180\n",
      "Epoch 139/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8861 - val_loss: 2.1474\n",
      "Epoch 140/200\n",
      "1960/1960 [==============================] - 0s 18us/step - loss: 2.1576 - val_loss: 1.8597\n",
      "Epoch 141/200\n",
      "1960/1960 [==============================] - 0s 18us/step - loss: 1.9808 - val_loss: 1.9094\n",
      "Epoch 142/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8980 - val_loss: 1.9271\n",
      "Epoch 143/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9636 - val_loss: 1.8121\n",
      "Epoch 144/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9311 - val_loss: 1.8265\n",
      "Epoch 145/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8744 - val_loss: 1.8262\n",
      "Epoch 146/200\n",
      "1960/1960 [==============================] - 0s 18us/step - loss: 1.8399 - val_loss: 1.7796\n",
      "Epoch 147/200\n",
      "1960/1960 [==============================] - 0s 18us/step - loss: 1.8321 - val_loss: 1.8237\n",
      "Epoch 148/200\n",
      "1960/1960 [==============================] - 0s 18us/step - loss: 1.8426 - val_loss: 1.7923\n",
      "Epoch 149/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8237 - val_loss: 1.7991\n",
      "Epoch 150/200\n",
      "1960/1960 [==============================] - 0s 18us/step - loss: 1.8304 - val_loss: 1.8032\n",
      "Epoch 151/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8197 - val_loss: 1.8230\n",
      "Epoch 152/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8766 - val_loss: 1.8390\n",
      "Epoch 153/200\n",
      "1960/1960 [==============================] - 0s 18us/step - loss: 1.8413 - val_loss: 1.8010\n",
      "Epoch 154/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8369 - val_loss: 1.7854\n",
      "Epoch 155/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8200 - val_loss: 1.7872\n",
      "Epoch 156/200\n",
      "1960/1960 [==============================] - 0s 18us/step - loss: 1.8443 - val_loss: 1.7906\n",
      "Epoch 157/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8878 - val_loss: 1.7960\n",
      "Epoch 158/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8544 - val_loss: 1.8175\n",
      "Epoch 159/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8287 - val_loss: 1.8044\n",
      "Epoch 160/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8643 - val_loss: 1.8680\n",
      "Epoch 161/200\n",
      "1960/1960 [==============================] - 0s 18us/step - loss: 1.8699 - val_loss: 1.8446\n",
      "Epoch 162/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9023 - val_loss: 1.8567\n",
      "Epoch 163/200\n",
      "1960/1960 [==============================] - 0s 18us/step - loss: 1.8372 - val_loss: 1.8374\n",
      "Epoch 164/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8491 - val_loss: 1.8168\n",
      "Epoch 165/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8284 - val_loss: 1.8109\n",
      "Epoch 166/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8601 - val_loss: 1.8902\n",
      "Epoch 167/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8482 - val_loss: 1.7821\n",
      "Epoch 168/200\n",
      "1960/1960 [==============================] - 0s 18us/step - loss: 1.8582 - val_loss: 1.7876\n",
      "Epoch 169/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8457 - val_loss: 1.9451\n",
      "Epoch 170/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9512 - val_loss: 1.8687\n",
      "Epoch 171/200\n",
      "1960/1960 [==============================] - 0s 18us/step - loss: 1.9222 - val_loss: 1.8298\n",
      "Epoch 172/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8901 - val_loss: 1.9777\n",
      "Epoch 173/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9367 - val_loss: 1.9167\n",
      "Epoch 174/200\n",
      "1960/1960 [==============================] - 0s 18us/step - loss: 1.8980 - val_loss: 1.8192\n",
      "Epoch 175/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8992 - val_loss: 1.8802\n",
      "Epoch 176/200\n",
      "1960/1960 [==============================] - 0s 18us/step - loss: 1.8783 - val_loss: 1.8127\n",
      "Epoch 177/200\n",
      "1960/1960 [==============================] - 0s 18us/step - loss: 1.8485 - val_loss: 1.8469\n",
      "Epoch 178/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8350 - val_loss: 1.8637\n",
      "Epoch 179/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1960/1960 [==============================] - 0s 18us/step - loss: 1.9036 - val_loss: 1.8920\n",
      "Epoch 180/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8831 - val_loss: 1.8250\n",
      "Epoch 181/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8865 - val_loss: 1.7896\n",
      "Epoch 182/200\n",
      "1960/1960 [==============================] - 0s 18us/step - loss: 1.9446 - val_loss: 1.8183\n",
      "Epoch 183/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8340 - val_loss: 1.8354\n",
      "Epoch 184/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8492 - val_loss: 1.8069\n",
      "Epoch 185/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8797 - val_loss: 1.8030\n",
      "Epoch 186/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9347 - val_loss: 1.8687\n",
      "Epoch 187/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8851 - val_loss: 1.9606\n",
      "Epoch 188/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8416 - val_loss: 1.8399\n",
      "Epoch 189/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9036 - val_loss: 1.9442\n",
      "Epoch 190/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9553 - val_loss: 1.8403\n",
      "Epoch 191/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9353 - val_loss: 1.8679\n",
      "Epoch 192/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8947 - val_loss: 1.7880\n",
      "Epoch 193/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9332 - val_loss: 1.8156\n",
      "Epoch 194/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9158 - val_loss: 2.1866\n",
      "Epoch 195/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9810 - val_loss: 1.8132\n",
      "Epoch 196/200\n",
      "1960/1960 [==============================] - 0s 18us/step - loss: 1.8484 - val_loss: 1.8112\n",
      "Epoch 197/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8870 - val_loss: 1.7957\n",
      "Epoch 198/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8563 - val_loss: 1.9017\n",
      "Epoch 199/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.8695 - val_loss: 1.8209\n",
      "Epoch 200/200\n",
      "1960/1960 [==============================] - 0s 17us/step - loss: 1.9254 - val_loss: 2.0005\n"
     ]
    }
   ],
   "source": [
    "x = np.linspace(-4, 4, 100)\n",
    "mean, variance = ems.fit_pred(data_x, data_y, x[:, np.newaxis])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x576 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot(mean, variance, data_x, data_y, x, \"Ensemble\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fe5adb72dd8>]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x576 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(12, 8))\n",
    "plt.plot(x, variance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dpl_keras",
   "language": "python",
   "name": "dpl_keras"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
